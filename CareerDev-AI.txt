##### INICIO DO ARQUIVO: alembic/env.py #####
from logging.config import fileConfig
import os
import sys
from sqlalchemy import engine_from_config
from sqlalchemy import pool
from alembic import context
from dotenv import load_dotenv

# Add the project root to the path so we can import from app
sys.path.append(os.getcwd())

# Load environment variables
load_dotenv()

# Import Base and Settings
from app.db.base import Base
from app.core.config import settings

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# Set the database URL from settings
config.set_main_option("sqlalchemy.url", settings.DATABASE_URL)

# add your model's MetaData object here
# for 'autogenerate' support
target_metadata = Base.metadata

def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    # Overwrite the section with the URL from code to avoid ini issues
    section = config.get_section(config.config_ini_section, {})
    section["sqlalchemy.url"] = settings.DATABASE_URL

    connectable = engine_from_config(
        section,
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()



##### INICIO DO ARQUIVO: alembic/versions/b117589f198b_initial_migration_reset.py #####
"""Initial migration reset

Revision ID: b117589f198b
Revises:
Create Date: 2026-02-03 14:18:57.999005

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'b117589f198b'
down_revision: Union[str, Sequence[str], None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('badges',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('slug', sa.String(), nullable=False),
    sa.Column('name', sa.String(), nullable=False),
    sa.Column('description', sa.String(), nullable=False),
    sa.Column('icon', sa.String(), nullable=False),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('slug')
    )
    op.create_index(op.f('ix_badges_id'), 'badges', ['id'], unique=False)
    op.create_table('users',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('email', sa.String(), nullable=False),
    sa.Column('hashed_password', sa.String(), nullable=False),
    sa.Column('full_name', sa.String(), nullable=True),
    sa.Column('is_active', sa.Boolean(), nullable=True),
    sa.Column('is_admin', sa.Boolean(), nullable=True),
    sa.Column('is_banned', sa.Boolean(), nullable=True),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
    sa.Column('email_verified', sa.Boolean(), nullable=True),
    sa.Column('is_profile_completed', sa.Boolean(), nullable=True),
    sa.Column('terms_accepted', sa.Boolean(), nullable=True),
    sa.Column('terms_accepted_at', sa.DateTime(), nullable=True),
    sa.Column('last_login', sa.DateTime(), nullable=True),
    sa.Column('avatar_url', sa.String(), nullable=True),
    sa.Column('github_id', sa.String(), nullable=True),
    sa.Column('github_username', sa.String(), nullable=True),
    sa.Column('github_token', sa.String(), nullable=True),
    sa.Column('linkedin_profile_url', sa.String(), nullable=True),
    sa.Column('linkedin_id', sa.String(), nullable=True),
    sa.Column('linkedin_token', sa.String(), nullable=True),
    sa.Column('streak_count', sa.Integer(), nullable=True),
    sa.Column('accelerator_mode', sa.Boolean(), nullable=True),
    sa.Column('last_weekly_check', sa.DateTime(), nullable=True),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('github_id'),
    sa.UniqueConstraint('linkedin_id')
    )
    op.create_index(op.f('ix_users_email'), 'users', ['email'], unique=True)
    op.create_index(op.f('ix_users_id'), 'users', ['id'], unique=False)
    op.create_table('audit_logs',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('user_id', sa.Integer(), nullable=True),
    sa.Column('session_id', sa.String(), nullable=True),
    sa.Column('action', sa.String(), nullable=True),
    sa.Column('details', sa.String(), nullable=True),
    sa.Column('ip_address', sa.String(), nullable=True),
    sa.Column('user_agent_raw', sa.String(), nullable=True),
    sa.Column('device_type', sa.String(), nullable=True),
    sa.Column('browser', sa.String(), nullable=True),
    sa.Column('os', sa.String(), nullable=True),
    sa.Column('login_timestamp', sa.DateTime(), nullable=True),
    sa.Column('is_active_session', sa.Boolean(), nullable=True),
    sa.Column('auth_method', sa.String(), nullable=True),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_table('career_profiles',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('user_id', sa.Integer(), nullable=False),
    sa.Column('bio', sa.Text(), nullable=True),
    sa.Column('location', sa.String(), nullable=True),
    sa.Column('target_role', sa.String(), nullable=True),
    sa.Column('experience_level', sa.String(), nullable=True),
    sa.Column('seniority', sa.String(length=20), nullable=True),
    sa.Column('primary_stack', sa.String(length=50), nullable=True),
    sa.Column('skills_snapshot', sa.JSON(), nullable=True),
    sa.Column('github_stats', sa.JSON(), nullable=True),
    sa.Column('linkedin_stats', sa.JSON(), nullable=True),
    sa.Column('skills_graph_data', sa.JSON(), nullable=True),
    sa.Column('github_activity_metrics', sa.JSON(), nullable=True),
    sa.Column('linkedin_alignment_data', sa.JSON(), nullable=True),
    sa.Column('ai_insights_summary', sa.Text(), nullable=True),
    sa.Column('active_challenge', sa.JSON(), nullable=True),
    sa.Column('active_weekly_plan', sa.JSON(), nullable=True),
    sa.Column('market_relevance_score', sa.Integer(), nullable=True),
    sa.Column('pending_micro_projects', sa.JSON(), nullable=True),
    sa.Column('updated_at', sa.DateTime(), nullable=True),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('user_id')
    )
    op.create_index(op.f('ix_career_profiles_id'), 'career_profiles', ['id'], unique=False)
    op.create_table('learning_plans',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('user_id', sa.Integer(), nullable=False),
    sa.Column('title', sa.String(), nullable=False),
    sa.Column('description', sa.Text(), nullable=True),
    sa.Column('technology', sa.String(), nullable=True),
    sa.Column('status', sa.String(), nullable=True),
    sa.Column('resources', sa.JSON(), nullable=True),
    sa.Column('created_at', sa.DateTime(), nullable=True),
    sa.Column('due_date', sa.DateTime(), nullable=True),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_learning_plans_id'), 'learning_plans', ['id'], unique=False)
    op.create_table('mentor_memories',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('user_id', sa.Integer(), nullable=False),
    sa.Column('context_key', sa.String(), nullable=True),
    sa.Column('memory_value', sa.Text(), nullable=True),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_mentor_memories_context_key'), 'mentor_memories', ['context_key'], unique=False)
    op.create_index(op.f('ix_mentor_memories_id'), 'mentor_memories', ['id'], unique=False)
    op.create_table('risk_snapshots',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('user_id', sa.Integer(), nullable=False),
    sa.Column('risk_score', sa.Integer(), nullable=False),
    sa.Column('risk_level', sa.String(length=10), nullable=True),
    sa.Column('risk_factor', sa.String(), nullable=True),
    sa.Column('mitigation_strategy', sa.String(), nullable=True),
    sa.Column('recorded_at', sa.DateTime(), nullable=True),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_risk_snapshots_id'), 'risk_snapshots', ['id'], unique=False)
    op.create_table('skill_snapshots',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('user_id', sa.Integer(), nullable=False),
    sa.Column('skill', sa.String(), nullable=False),
    sa.Column('confidence_score', sa.Integer(), nullable=False),
    sa.Column('recorded_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_skill_snapshots_id'), 'skill_snapshots', ['id'], unique=False)
    op.create_table('user_badges',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('user_id', sa.Integer(), nullable=False),
    sa.Column('badge_id', sa.Integer(), nullable=False),
    sa.Column('awarded_at', sa.DateTime(), nullable=True),
    sa.ForeignKeyConstraint(['badge_id'], ['badges.id'], ),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_user_badges_id'), 'user_badges', ['id'], unique=False)
    op.create_table('user_sessions',
    sa.Column('id', sa.String(), nullable=False),
    sa.Column('user_id', sa.Integer(), nullable=False),
    sa.Column('ip_address', sa.String(), nullable=True),
    sa.Column('user_agent', sa.String(), nullable=True),
    sa.Column('created_at', sa.DateTime(), nullable=True),
    sa.Column('expires_at', sa.DateTime(), nullable=True),
    sa.Column('last_active_at', sa.DateTime(), nullable=True),
    sa.Column('is_active', sa.Boolean(), nullable=True),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_table('weekly_routines',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('user_id', sa.Integer(), nullable=False),
    sa.Column('week_id', sa.String(length=10), nullable=False),
    sa.Column('mode', sa.String(length=20), nullable=True),
    sa.Column('focus', sa.String(length=50), nullable=False),
    sa.Column('tasks', sa.JSON(), nullable=False),
    sa.Column('suggested_pr', sa.JSON(), nullable=True),
    sa.Column('completed', sa.Boolean(), nullable=True),
    sa.Column('completion_rate', sa.Integer(), nullable=True),
    sa.Column('created_at', sa.DateTime(), nullable=True),
    sa.Column('completed_at', sa.DateTime(), nullable=True),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table('weekly_routines')
    op.drop_table('user_sessions')
    op.drop_index(op.f('ix_user_badges_id'), table_name='user_badges')
    op.drop_table('user_badges')
    op.drop_index(op.f('ix_skill_snapshots_id'), table_name='skill_snapshots')
    op.drop_table('skill_snapshots')
    op.drop_index(op.f('ix_risk_snapshots_id'), table_name='risk_snapshots')
    op.drop_table('risk_snapshots')
    op.drop_index(op.f('ix_mentor_memories_id'), table_name='mentor_memories')
    op.drop_index(op.f('ix_mentor_memories_context_key'), table_name='mentor_memories')
    op.drop_table('mentor_memories')
    op.drop_index(op.f('ix_learning_plans_id'), table_name='learning_plans')
    op.drop_table('learning_plans')
    op.drop_index(op.f('ix_career_profiles_id'), table_name='career_profiles')
    op.drop_table('career_profiles')
    op.drop_table('audit_logs')
    op.drop_index(op.f('ix_users_id'), table_name='users')
    op.drop_index(op.f('ix_users_email'), table_name='users')
    op.drop_table('users')
    op.drop_index(op.f('ix_badges_id'), table_name='badges')
    op.drop_table('badges')
    # ### end Alembic commands ###



##### INICIO DO ARQUIVO: app/__init__.py #####



##### INICIO DO ARQUIVO: app/ai/__init__.py #####



##### INICIO DO ARQUIVO: app/ai/chatbot.py #####
from typing import Optional, Tuple, Dict, Any
import openai
import json
import asyncio
from datetime import datetime
from sqlalchemy.orm import Session
from app.core.config import settings
from app.ai.prompts import (
    CAREER_ASSISTANT_SYSTEM_PROMPT,
    RUTHLESS_CTO_SYSTEM_PROMPT,
    get_interviewer_system_prompt,
    CHALLENGE_GENERATOR_PROMPT,
    CHALLENGE_GRADER_PROMPT,
    LINKEDIN_POST_GENERATOR_PROMPT,
    PROJECT_SPEC_GENERATOR_PROMPT
)
from app.db.models.user import User
from app.db.models.career import CareerProfile

def _fetch_user_and_profile_sync(user_id: int, db: Session):
    """
    Synchronously fetch user and ensure career_profile is loaded.
    """
    user = db.query(User).filter(User.id == user_id).first()
    profile = None
    if user:
        profile = user.career_profile
        # Touch lazy loaded fields to ensure they are available
        if profile:
             _ = profile.skills_snapshot
             _ = profile.github_activity_metrics
    return user, profile

def _save_challenge_trigger_sync(db: Session, profile_id: int, question: str, skill: str):
    profile = db.query(CareerProfile).get(profile_id)
    if profile:
        profile.active_challenge = {
            "skill": skill,
            "question": question,
            "timestamp": datetime.utcnow().isoformat()
        }
        db.commit()

def _save_challenge_grading_sync(db: Session, profile_id: int):
    profile = db.query(CareerProfile).get(profile_id)
    if profile:
        profile.active_challenge = None
        db.commit()

def _fetch_user_and_build_context(user_id: int, db: Session, mode: str) -> Tuple[str, str]:
    """
    Synchronously fetches user data and builds context to be run in a thread.
    Returns a tuple of (context_string, system_prompt).
    """
    user = db.query(User).filter(User.id == user_id).first()
    if not user:
        return "", CAREER_ASSISTANT_SYSTEM_PROMPT

    profile = user.career_profile
    plans = user.learning_plans
    skills = profile.skills_snapshot if profile else {}
    target_role = profile.target_role if profile else 'Software Engineer'
    active_plans = [p.title for p in plans if p.status != 'completed']

    if mode == "interview":
        system_prompt = get_interviewer_system_prompt(
            {"target_role": target_role, "skills": skills},
            user.name
        )
        context_str = ""
    else:
        # Check for HARDCORE MODE
        if user.weekly_streak_count >= 4:
            system_prompt = RUTHLESS_CTO_SYSTEM_PROMPT
        else:
            system_prompt = CAREER_ASSISTANT_SYSTEM_PROMPT

        context_str = f"""
        **User Context:**
        - Name: {user.name}
        - Premium Status: {user.is_premium}
        - Current Skills: {json.dumps(skills)}
        - Active Learning Plan: {', '.join(active_plans)}
        - Focus: {target_role}
        - Weekly Streak: {user.weekly_streak_count}

        Use this context to give personalized advice. If Premium is False and they ask for advanced resume checks, suggest upgrading.
        """
    return context_str, system_prompt

class ChatbotService:
    def __init__(self, simulated: bool = True):
        if settings.OPENAI_API_KEY:
            self.async_client = openai.AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
            self.simulated = False
        else:
            self.async_client = None
            self.simulated = True

    async def get_response(self, message: str, lang: str = "en", user_id: int = None, db: Session = None, mode: str = "standard") -> Dict[str, Any]:
        """
        Mode can be 'standard', 'interview', or 'challenge'.
        Returns Dict with 'message' and 'meta'.

        COMPLIANCE NOTE:
        - OpenAI API is used for generation only.
        - No user data is sent to external training/feedback endpoints.
        - Privacy settings rely on 'settings.OPENAI_API_KEY' configuration.
        """

        profile = None
        user = None
        if user_id and db:
             # Offload blocking DB fetch
             user, profile = await asyncio.to_thread(_fetch_user_and_profile_sync, user_id, db)

        # 1. TRIGGER CHALLENGE
        if message == "/trigger_challenge" and profile:
             return await self._handle_challenge_trigger(profile, db, lang)

        # 2. GRADE CHALLENGE
        if mode == "challenge" and profile:
             return await self._handle_challenge_grading(message, profile, db, lang)

        # 3. STANDARD / INTERVIEW
        context_str = ""
        system_prompt = CAREER_ASSISTANT_SYSTEM_PROMPT

        if user_id and db:
            context_str, system_prompt = await asyncio.to_thread(
                _fetch_user_and_build_context, user_id, db, mode
            )

        response_text = ""
        if self.simulated:
            # Pass user object to check streak in simulation
            response_text = self._simulated_response(message, lang, context_str, mode, user)
        else:
            response_text = await self._llm_response(message, lang, context_str, system_prompt)

        return {"message": response_text, "meta": {"mode": mode}}

    def _find_weakness(self, profile: CareerProfile) -> str:
        try:
            metrics = profile.github_activity_metrics or {}
            raw_langs = metrics.get("raw_languages", {})
            if not raw_langs:
                return "General Engineering"

            total = sum(raw_langs.values())
            if total == 0:
                return "General Engineering"

            lowest_skill = None
            lowest_pct = 100

            for skill, bytes_count in raw_langs.items():
                pct = (bytes_count / total) * 100
                if 0 < pct < lowest_pct:
                    lowest_pct = pct
                    lowest_skill = skill

            return lowest_skill or "General Engineering"
        except Exception:
            return "General Engineering"

    async def _handle_challenge_trigger(self, profile: CareerProfile, db: Session, lang: str) -> Dict[str, Any]:
        skill = self._find_weakness(profile)

        question = ""
        if self.simulated:
            question = f"Challenge Mode (Simulated): Explain the core concept of {skill} and why it matters."
        else:
            prompt = CHALLENGE_GENERATOR_PROMPT.format(skill=skill)
            # We call LLM directly
            question = await self._llm_response("", lang, "", prompt)

        # Save State (Offload sync commit)
        if profile:
             await asyncio.to_thread(_save_challenge_trigger_sync, db, profile.id, question, skill)

        return {"message": question, "meta": {"mode": "challenge"}}

    async def _handle_challenge_grading(self, answer: str, profile: CareerProfile, db: Session, lang: str) -> Dict[str, Any]:
        active = profile.active_challenge or {}
        question = active.get("question", "Unknown")

        grade = ""
        if self.simulated:
            grade = "Rating: â­â­â­\nCorrection: Good attempt (Simulated).\nFollow-up: Try diving deeper into the docs."
        else:
            prompt = CHALLENGE_GRADER_PROMPT.format(question=question, answer=answer)
            grade = await self._llm_response("", lang, "", prompt)

        # Clear State (Offload sync commit)
        if profile:
             await asyncio.to_thread(_save_challenge_grading_sync, db, profile.id)

        return {"message": grade, "meta": {"mode": "standard"}}

    async def generate_linkedin_post(self, skill: str, lang: str) -> str:
        """
        Generates a viral LinkedIn post using the dedicated prompt.
        """
        clean_skill = skill.replace(" ", "").replace("/", "")

        if self.simulated:
            return f"Excited to share my progress in {skill}! Thanks to CareerDev AI for the structured learning path. ðŸš€ #{clean_skill} #TechJourney (Simulated)"

        prompt = LINKEDIN_POST_GENERATOR_PROMPT.format(skill=skill, skill_clean=clean_skill)
        return await self._llm_response("", lang, "", prompt)

    async def generate_project_spec(self, skill: str, lang: str) -> str:
        """
        Generates a Micro-Project Specification (Markdown) to fill a skill gap.
        """
        if self.simulated:
            return f"# Project: {skill} Accelerator (Simulated)\n\n**Objective:** Build a proof of concept.\n\n**Tasks:**\n1. Setup Environment.\n2. Implement core logic.\n3. Push to GitHub."

        prompt = PROJECT_SPEC_GENERATOR_PROMPT.format(skill=skill)
        return await self._llm_response("", lang, "", prompt)

    def _simulated_response(self, message: str, lang: str, context: str, mode: str, user: User = None) -> str:
        msg = message.lower()

        # Helper for simple multilingual return
        def reply(en, pt, es):
            if lang == 'pt-BR' or lang == 'pt': return pt
            if lang == 'es': return es
            return en

        if mode == "interview":
             if "start" in msg:
                 return reply(
                     "Let's start. Explain the difference between TCP and UDP.",
                     "Vamos comeÃ§ar. Explique a diferenÃ§a entre TCP e UDP.",
                     "Empecemos. Explica la diferencia entre TCP y UDP."
                 )
             return reply(
                 "Good answer (Simulated). Next: What is Dependency Injection?",
                 "Boa resposta (Simulado). PrÃ³xima: O que Ã© InjeÃ§Ã£o de DependÃªncia?",
                 "Buena respuesta (Simulado). Siguiente: Â¿QuÃ© es la InyecciÃ³n de Dependencias?"
             )

        # HARDCORE MODE SIMULATION
        if user and user.weekly_streak_count >= 4:
            return reply(
                "HARDCORE MODE: I don't care about your feelings. Your system design is flawed. Design a distributed lock manager using Redis. NOW.",
                "MODO HARDCORE: NÃ£o me importo com seus sentimentos. Seu design de sistema Ã© falho. Projete um gerenciador de bloqueio distribuÃ­do usando Redis. AGORA.",
                "MODO HARDCORE: No me importan tus sentimientos. Tu diseÃ±o de sistema es defectuoso. DiseÃ±a un gestor de bloqueo distribuido usando Redis. AHORA."
            )

        if "my plan" in msg or "meu plano" in msg:
            if "Active Learning Plan" in context:
                 plan_name = context.split("Active Learning Plan:")[1].split("- Focus")[0].strip()
                 return reply(
                     "Based on your profile, you should focus on: " + plan_name,
                     "Com base no seu perfil, vocÃª deve focar em: " + plan_name,
                     "Basado en tu perfil, deberÃ­as enfocarte en: " + plan_name
                 )
            return reply(
                "You don't have an active plan yet. Go to the dashboard to generate one.",
                "VocÃª ainda nÃ£o tem um plano ativo. VÃ¡ ao dashboard para gerar um.",
                "AÃºn no tienes un plan activo. Ve al panel para generar uno."
            )

        if "rust" in msg:
            return reply(
                "Rust is a language focused on safety and performance. Great for embedded systems and critical services.",
                "Rust Ã© uma linguagem focada em seguranÃ§a e performance. Ã“tima para sistemas embarcados e serviÃ§os crÃ­ticos.",
                "Rust es un lenguaje enfocado en seguridad y rendimiento. Genial para sistemas integrados y servicios crÃ­ticos."
            )
        elif "go" in msg or "golang" in msg:
            return reply(
                "Go is excellent for microservices and cloud applications due to its lightweight concurrency.",
                "Go Ã© excelente para microsserviÃ§os e aplicaÃ§Ãµes em nuvem devido Ã  sua concorrÃªncia leve.",
                "Go es excelente para microservicios y aplicaciones en la nube debido a su concurrencia ligera."
            )
        elif "career" in msg or "carreira" in msg:
            return reply(
                "To advance your career, CareerDev AI suggests focusing on T-Shaped skills and connecting your GitHub for gap analysis.",
                "Para avanÃ§ar na carreira, o CareerDev AI sugere focar em habilidades T-Shaped e conectar seu GitHub para anÃ¡lise de lacunas.",
                "Para avanzar en tu carrera, CareerDev AI sugiere enfocarse en habilidades T-Shaped y conectar tu GitHub para anÃ¡lisis de brechas."
            )
        elif "login" in msg or "entrar" in msg:
            return reply(
                "You can login using Email/Password, GitHub or LinkedIn for a complete experience.",
                "VocÃª pode entrar usando E-mail/Senha, GitHub ou LinkedIn para uma experiÃªncia completa.",
                "Puedes iniciar sesiÃ³n usando Email/ContraseÃ±a, GitHub o LinkedIn para una experiencia completa."
            )

        return reply(
            "Operating in simulated mode. Ask about 'Rust', 'Go', 'Career' or 'My Plan'. Try Interview Mode!",
            "Operando em modo simulado. Pergunte sobre 'Rust', 'Go', 'Carreira' ou 'Meu Plano'. Tente o Modo Entrevista!",
            "Operando en modo simulado. Pregunta sobre 'Rust', 'Go', 'Carrera' o 'Mi Plan'. Â¡Prueba el Modo Entrevista!"
        )

    async def verify_connection(self):
        """
        Forces a test call to OpenAI to verify the API Key.
        Raises an exception if verification fails.
        """
        if self.simulated:
             print("WARNING: Chatbot is in simulated mode (No API Key).")
             return

        try:
            # Simple list models call to verify auth
            await self.async_client.models.list()
            print("SUCCESS: OpenAI Connection Verified.")
        except Exception as e:
            # Re-raise to let caller handle critical alert
            raise Exception(f"OpenAI Connection Failed: {e}")

    async def _llm_response(self, message: str, lang: str, context: str, system_prompt: str) -> str:
        lang_instruction = f"Reply in {lang}."
        if lang == 'pt-BR' or lang == 'pt':
             lang_instruction = "Responda em PortuguÃªs do Brasil."
        elif lang == 'es':
             lang_instruction = "Responda em Espanhol."

        messages = [
            {"role": "system", "content": system_prompt + "\n" + context},
            {"role": "system", "content": lang_instruction}
        ]
        if message:
            messages.append({"role": "user", "content": message})

        # Determine params based on model name
        primary_model = settings.OPENAI_MODEL
        params = {
            "model": primary_model,
            "messages": messages
        }

        # O1 models and gpt-5-mini do not support temperature
        if not (primary_model.startswith("o1-") or primary_model == "gpt-5-mini"):
            params["temperature"] = 0.7

        try:
            response = await self.async_client.chat.completions.create(**params)
            return response.choices[0].message.content
        except (openai.NotFoundError, openai.BadRequestError) as e:
            print(f"WARNING: Primary model {settings.OPENAI_MODEL} failed (Error: {e}). Switching to fallback: {settings.OPENAI_FALLBACK_MODEL}.")
            try:
                response = await self.async_client.chat.completions.create(
                    model=settings.OPENAI_FALLBACK_MODEL,
                    messages=messages,
                    temperature=0.7
                )
                return response.choices[0].message.content
            except Exception as e_fallback:
                print(f"CRITICAL: Fallback model {settings.OPENAI_FALLBACK_MODEL} also failed: {e_fallback}")
                return "Error communicating with AI (Fallback failed)."
        except Exception as e:
            print(f"OpenAI Error: {e}")
            return "Error communicating with AI (Check API Key)."

# Global Instance
chatbot_service = ChatbotService()



##### INICIO DO ARQUIVO: app/ai/prompts.py #####
# System Prompts for CareerDev AI

CAREER_ASSISTANT_SYSTEM_PROMPT = """
You are CareerDev AI, a Senior Technical Mentor and Autonomous Career Manager.
Your goal is not just to assist, but to strategically engineer the user's career trajectory using the "Gap Analysis Engine" and "Adaptive Upskilling" protocols.

**USER PROFILE CONTEXT (GROUND TRUTH):**

[USER_PROFILE_CONTEXT]
{
  "name": "Robson Holanda da Silva",
  "current_stack": ["Python", "FastAPI", "Accessibility Standards", "Web Development"],
  "active_projects": ["CareerDev AI (Author/Lead)", "ASL/LSQ 3D Avatars"],
  "career_goal": "Global Authority in Inclusive AI & Edge Computing",
  "target_market": "International (Remote/Relocation)",
  "learning_style": "Hands-on, Project-Based"
}

**CORE DIRECTIVES (BUSINESS LOGIC ALGORITHMS):**

1.  **Scarcity & Upskilling Algorithm:**
    -   **Logic:** Compare `current_stack` vs. `High_Value_Niche`.
    -   **Rule:** If User asks "What to study?", DO NOT suggest generic web dev. Suggest Rust (for performance) or WASM (for the 3D Avatar project).
    -   **Output:** "To optimize your CareerDev AI backend, migrate the heavy computation modules from Python to Rust."

2.  **Gap Analysis & Micro-Projects:**
    -   **Logic:** `Target_Job_Reqs` - `User_Skills` = `The_Gap`.
    -   **Action:** Generate a "Weekend Micro-Project".
    -   **Example:** "You lack Graph Database experience. Task: Implement a Neo4j recommendation engine for CareerDev AI users by Sunday. Push to GitHub."

3.  **Real-Time Trend Simulation:**
    -   **Logic:** Correlate "Ethical AI" trends with the user's Accessibility focus.
    -   **Advice:** "Accessibility is becoming a compliance requirement in EU. Position your 'CareerDev AI' not just as a tool, but as a Compliance Engine."

4.  **Networking Simulator:**
    -   **Logic:** Analyze the generated micro-project.
    -   **Action:** Draft a LinkedIn post structure for the user to share the result. "Here is the hook to post about your new 3D Avatar module..."

5.  **Persona & Tone:**
    -   Act as a Senior Staff Engineer or CTO mentoring a junior/mid-level dev.
    -   While maintaining technical precision, you must be encouraging, warm, and empathetic.
    -   Acknowledge the user's effort before correcting them. Use phrases like "That's a great start," or "I see what you're trying to do."
    -   Do not be purely transactional.
    -   Use futuristic/cyberpunk terminology where appropriate (e.g., "Ops", "Protocol", "Delta", "Latency").

**OFFICIAL WEBSITE CONTENT & CONTEXT:**
-   **Project Motto:** "A mÃ¡quina nÃ£o substitui o humano. Ela amplia seu potencial."
-   **Integrations:**
    -   **GitHub:** Used for code analysis and skill verification.
    -   **LinkedIn:** Used for market trend alignment and networking.
-   **Accessibility:** You are fully aware of the "Universal Accessibility Panel" in Security Settings (Dyslexic Font, High Contrast, etc.). Guide users there if they express UI difficulties.

**Behavior:**
-   Detect the user's language (PT, EN, ES) and reply in the same language.
-   If the user asks about their "Plan", refer to the "Weekly Routine Board" on their dashboard.
-   If the user seems comfortable, challenge them to optimize their code.

**Current State:**
-   The user is currently interacting with the Chatbot Widget on the web application.
"""

# HARDCORE MODE PROMPT
RUTHLESS_CTO_SYSTEM_PROMPT = """
You are the RUTHLESS CTO.
The user has unlocked "HARDCORE MODE" (Streak >= 4 weeks).
Your goal is to break their bad habits and force High-Level System Design thinking.

**DIRECTIVES:**
1.  **NO TUTORIALS:** Do not explain "how" to write syntax. If they ask, say "Read the docs."
2.  **SYSTEM DESIGN ONLY:** Every answer must challenge the scalability, latency, or security of their approach.
3.  **TONE:** Direct, terse, demanding. No encouragement. Only raw feedback.
    -   Bad: "Good job, but try..."
    -   Good: "This O(n^2) garbage will crash production. Rewrite it."
4.  **CHALLENGES:** Constantly issue design challenges (e.g., "Design a Rate Limiter", "How would you shard this DB?").

**Behavior:**
-   Detect language and reply in the same language, but keep the ruthless tone.
-   If they complain, tell them to go back to "Tutorial Mode" by breaking their streak.
"""

def get_interviewer_system_prompt(profile_data: dict, user_name: str) -> str:
    role = profile_data.get('target_role', 'Software Engineer')
    skills = ", ".join(profile_data.get('skills', {}).keys())

    base = f"""
    You are a Senior Tech Lead conducting a rigorous technical interview for a {role} position.
    The candidate is {user_name}. Their listed skills are: {skills}.

    Your Goal:
    1.  Ask ONE challenging technical question related to their role/skills.
    2.  Wait for their answer.
    3.  Evaluate their answer strictly but constructively. Focus on **Syntax**, **Optimization**, and **System Design**.
    4.  Then ask the next question.

    STRUCTURE OF YOUR RESPONSE (When evaluating an answer):

    [Evaluation]
    Grade: [A-F]
    Technical Accuracy: [Feedback on correctness/syntax]
    Optimization: [Feedback on time/space complexity or best practices]
    Communication Style: [Brief feedback on clarity]

    [Next Question]
    [Your next question here]

    Behavior:
    -   If the user just says "Start" or "Begin" or "Iniciar", ONLY ask the first question (no evaluation).
    -   Be professional, concise, and direct.
    -   Do not give long lectures; give feedback and move on.
    -   If they struggle, give a hint but mark it down.

    Current State: YOU ARE THE INTERVIEWER. DO NOT BREAK CHARACTER.
    """
    return base


CHALLENGE_GENERATOR_PROMPT = """
You are a Senior Technical Interviewer.
The candidate has a specific weakness in: {skill}.
Your goal is to ask ONE conceptual, tough question about {skill} to test their depth of knowledge.
Do NOT ask for code. Ask for an explanation of a concept, trade-off, or architectural pattern.
Keep it short (suitable for audio delivery). Do not provide the answer.
"""

CHALLENGE_GRADER_PROMPT = """
You are a Senior Technical Evaluator.
Question: "{question}"
User Answer: "{answer}"

Evaluate the answer strictly.
Return your response in this format:
Rating: [1-5 Stars]
Correction: [Brief correction if wrong, or "Spot on" if right]
Follow-up: [A short encouragement or next challenge phrase]
"""

LINKEDIN_POST_GENERATOR_PROMPT = """
Draft a professional LinkedIn post celebrating my progress in {skill}.
Keep it under 280 chars to ensure it's punchy and viral.
Tone: Enthusiastic, Professional, and Growth-Oriented.
Include hashtags: #CareerDevAI #Coding #TechGrowth #{skill_clean}
Output ONLY the post text. No intro/outro.
"""

PROJECT_SPEC_GENERATOR_PROMPT = """
You are a Staff Engineer designing a learning project for a Junior/Mid-level Developer.
The goal is to build a "Proof of Work" to demonstrate competence in: {skill}.

Generate a complete Project Specification in MARKDOWN format.
It must include:
1.  **Project Title**: Catchy and professional.
2.  **Objective**: What problem does it solve?
3.  **Core Features**: 3-4 bullet points.
4.  **Tech Stack**: Must include {skill} + complementary tools.
5.  **Folder Structure**: A simple ASCII tree.
6.  **Step-by-Step Instructions**: High-level guide.
7.  **Bonus Challenge**: To go from Junior to Senior.

Keep it concise but actionable.
"""



##### INICIO DO ARQUIVO: app/core/__init__.py #####



##### INICIO DO ARQUIVO: app/core/accessibility.py #####
translations = {
    "pt": {
        "login_title": "MÃ¡quinas ampliam o potencial humano",
        "login_button": "Entrar com seguranÃ§a"
    },
    "en": {
        "login_title": "Machines amplify human potential",
        "login_button": "Secure Login"
    },
    "es": {
        "login_title": "Las mÃ¡quinas amplÃ­an el potencial humano",
        "login_button": "Acceso seguro"
    }
}

def t(lang: str, key: str):
    return translations.get(lang, translations["pt"]).get(key, key)




##### INICIO DO ARQUIVO: app/core/auth_guard.py #####
from fastapi import Request
from fastapi.responses import RedirectResponse

def require_auth(request: Request):
    """
    Guards a route by checking if request.state.user is populated.
    This relies on AuthMiddleware having run and validated the session.
    """
    if getattr(request.state, "user", None):
        return None

    return RedirectResponse("/login", status_code=302)

def get_current_user_from_request(request: Request):
    """
    Helper to extract user_id from request.state.user.
    Returns None if invalid.
    """
    user = getattr(request.state, "user", None)
    if user:
        return user.id
    return None



##### INICIO DO ARQUIVO: app/core/config.py #####
from pydantic_settings import BaseSettings
from pydantic import field_validator, ValidationInfo, model_validator
from typing import Optional, Any
import os
import urllib.parse

class Settings(BaseSettings):
    # App
    APP_NAME: str = "CareerDev AI"
    RESUME_ANALYZER_TITLE: str = "Resume Analyzer"
    DOMAIN: str = "https://www.careerdev-ai.online"
    ENVIRONMENT: str = "development" # development, production, test
    ALLOWED_HOSTS: list[str] = ["*"]
    SECRET_KEY: str = "super-secret-key-change-in-production"
    SESSION_SECRET_KEY: str = "change-this-to-a-secure-random-string"

    # Database
    DATABASE_URL: str
    POSTGRES_URL: Optional[str] = None

    # Railway / Standard Postgres Env Vars
    PGHOST: Optional[str] = None
    PGPORT: Optional[str] = None
    PGUSER: Optional[str] = None
    PGPASSWORD: Optional[str] = None
    PGDATABASE: Optional[str] = None

    @model_validator(mode='before')
    @classmethod
    def check_database_url(cls, data: Any) -> Any:
        if isinstance(data, dict):
            if not data.get("DATABASE_URL"):
                if data.get("POSTGRES_URL"):
                    print("DEBUG: Using POSTGRES_URL fallback for DATABASE_URL")
                    data["DATABASE_URL"] = data.get("POSTGRES_URL")
                elif all(data.get(k) for k in ["PGHOST", "PGUSER", "PGDATABASE"]):
                    print("DEBUG: Constructing DATABASE_URL from PG* environment variables")
                    user = data.get("PGUSER")
                    password = data.get("PGPASSWORD")
                    host = data.get("PGHOST")
                    port = data.get("PGPORT", "5432")
                    dbname = data.get("PGDATABASE")

                    # Handle password encoding to prevent issues with special characters
                    if password:
                        password = urllib.parse.quote_plus(password)
                        auth = f"{user}:{password}"
                    else:
                        auth = user

                    data["DATABASE_URL"] = f"postgresql://{auth}@{host}:{port}/{dbname}"
        return data

    @field_validator("DATABASE_URL", mode="before")
    @classmethod
    def assemble_db_connection(cls, v: Optional[str], info: ValidationInfo) -> str:
        if isinstance(v, str):
            if v.startswith("postgres://"):
                return v.replace("postgres://", "postgresql://", 1)
        return v

    # Notification settings removed

    # AI
    OPENAI_API_KEY: Optional[str] = None
    OPENAI_MODEL: str = "gpt-5-mini"
    LLM_MODEL_DISPLAY_NAME: str = "GPT-5-Mini"
    OPENAI_FALLBACK_MODEL: str = "gpt-4o-mini"

    # OAuth
    GITHUB_CLIENT_ID: Optional[str] = None
    GITHUB_CLIENT_SECRET: Optional[str] = None

    LINKEDIN_CLIENT_ID: Optional[str] = None
    LINKEDIN_CLIENT_SECRET: Optional[str] = None
    LINKEDIN_REDIRECT_URI: Optional[str] = None

    # Analytics (PostHog)
    POSTHOG_API_KEY: Optional[str] = None
    POSTHOG_HOST: str = "https://app.posthog.com"

    # Monitoring (Sentry)
    SENTRY_DSN: Optional[str] = None

    # Feature Flags
    FEATURES: dict = {
        "ENABLE_CHATBOT": True,
        "ENABLE_VOICE_MODE": True,
        "ENABLE_SOCIAL_LOGIN": True,
        "ENABLE_REGISTRATION": True
    }

    class Config:
        env_file = ".env"
        extra = "ignore" # Prevent crash on extra env vars

settings = Settings()

if not settings.OPENAI_API_KEY:
    import logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    logger.critical("OPENAI_API_KEY is missing. AI features will fail. Please check your environment variables.")



##### INICIO DO ARQUIVO: app/core/i18n.py #####
translations = {
    "pt": {
        "login_title": "MÃ¡quinas ampliam o potencial humano",
        "login_button": "Entrar com seguranÃ§a"
    },
    "en": {
        "login_title": "Machines amplify human potential",
        "login_button": "Secure Login"
    },
    "es": {
        "login_title": "Las mÃ¡quinas amplÃ­an el potencial humano",
        "login_button": "Acceso seguro"
    }
}

def t(lang: str, key: str):
    return translations.get(lang, translations["pt"]).get(key, key)




##### INICIO DO ARQUIVO: app/core/jwt.py #####
from datetime import datetime, timedelta
from jose import jwt, JWTError
from app.core.config import settings

# Use config for secrets
SECRET_KEY = settings.SECRET_KEY
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 30 # Reduced default for security


def create_access_token(data: dict, expires_minutes: int = None, expires_delta: timedelta = None):
    to_encode = data.copy()

    if expires_minutes:
        expire = datetime.utcnow() + timedelta(minutes=expires_minutes)
    elif expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)

    to_encode.update({
        "exp": expire,
        "iat": datetime.utcnow()
    })

    return jwt.encode(
        to_encode,
        SECRET_KEY,
        algorithm=ALGORITHM
    )


def decode_token(token: str):
    try:
        payload = jwt.decode(
            token,
            SECRET_KEY,
            algorithms=[ALGORITHM]
        )
        return payload
    except JWTError:
        return None



##### INICIO DO ARQUIVO: app/core/limiter.py #####
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address, default_limits=["100/minute"])



##### INICIO DO ARQUIVO: app/core/security.py #####
from passlib.context import CryptContext
from jose import jwt
from datetime import datetime, timedelta
from app.core.config import settings

ALGORITHM = "HS256"

pwd_context = CryptContext(
    schemes=["bcrypt"],
    deprecated="auto"
)

def hash_password(password: str) -> str:
    return pwd_context.hash(password)

def create_access_token(data: dict, expires_minutes: int = 60):
    to_encode = data.copy()
    expire = datetime.utcnow() + timedelta(minutes=expires_minutes)
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, settings.SECRET_KEY, algorithm=ALGORITHM)



##### INICIO DO ARQUIVO: app/core/utils.py #####
from fastapi import Request

def get_client_ip(request: Request) -> str:
    """
    Extracts the client's real IP address, trusting X-Forwarded-For if present.
    Essential for applications running behind proxies (Railway, Heroku, Nginx).
    """
    forwarded = request.headers.get("X-Forwarded-For")
    if forwarded:
        # The first IP in the list is the client's original IP
        return forwarded.split(",")[0].strip()

    return request.client.host if request.client else "0.0.0.0"



##### INICIO DO ARQUIVO: app/db/__init__.py #####



##### INICIO DO ARQUIVO: app/db/base.py #####
from app.db.base_class import Base

# Modelos Core
from app.db.models.user import User

# Modelos de Carreira
from app.db.models.career import CareerProfile, LearningPlan
from app.db.models.weekly_routine import WeeklyRoutine

# Modelos Auxiliares
from app.db.models.security import UserSession
from app.db.models.gamification import Badge, UserBadge
from app.db.models.audit import AuditLog
from app.db.models.skill_snapshot import SkillSnapshot
from app.db.models.mentor import MentorMemory

# --- ADICIONE ESTA LINHA ---
from app.db.models.analytics import RiskSnapshot



##### INICIO DO ARQUIVO: app/db/base_class.py #####
from typing import Any
from sqlalchemy.ext.declarative import as_declarative, declared_attr

@as_declarative()
class Base:
    id: Any
    __name__: str

    # Gera o nome da tabela automaticamente a partir do nome da classe (em minÃºsculas)
    @declared_attr
    def __tablename__(cls) -> str:
        return cls.__name__.lower()



##### INICIO DO ARQUIVO: app/db/crud/__init__.py #####



##### INICIO DO ARQUIVO: app/db/crud/users.py #####
import asyncio
from sqlalchemy.orm import Session
from app.db.models.user import User

def get_user_by_email(db: Session, email: str) -> User | None:
    return db.query(User).filter(User.email == email).first()

def get_user_by_github_id(db: Session, github_id: str) -> User | None:
    if not github_id:
        return None
    return db.query(User).filter(User.github_id == github_id).first()

def get_user_by_linkedin_id(db: Session, linkedin_id: str) -> User | None:
    if not linkedin_id:
        return None
    return db.query(User).filter(User.linkedin_id == linkedin_id).first()

def create_user(
    db: Session,
    name: str,
    email: str,
    hashed_password: str,
    **kwargs
) -> User:
    user = User(
        full_name=name,
        email=email,
        hashed_password=hashed_password,
        **kwargs
    )

    db.add(user)
    db.commit()
    db.refresh(user)
    return user

async def create_user_async(
    db: Session,
    name: str,
    email: str,
    hashed_password: str,
    **kwargs
) -> User:
    return await asyncio.to_thread(
        create_user,
        db=db,
        name=name,
        email=email,
        hashed_password=hashed_password,
        **kwargs
    )



##### INICIO DO ARQUIVO: app/db/declarative.py #####
from sqlalchemy.orm import DeclarativeBase

class Base(DeclarativeBase):
    pass



##### INICIO DO ARQUIVO: app/db/models/__init__.py #####



##### INICIO DO ARQUIVO: app/db/models/analytics.py #####
from sqlalchemy import Column, Integer, String, DateTime, ForeignKey
from sqlalchemy.orm import relationship, backref
from sqlalchemy.sql import func
from datetime import datetime

# Ajustado para 'app.db.base_class' para evitar dependÃªncia circular
from app.db.base_class import Base

class RiskSnapshot(Base):
    __tablename__ = "risk_snapshots"

    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"), nullable=False)

    # --- MÃ©tricas Quantitativas (Essenciais para ML/LSTM) ---
    risk_score = Column(Integer, nullable=False)  # 0-100 (Probabilidade/Impacto)
    risk_level = Column(String(10))               # LOW | MEDIUM | HIGH

    # --- Contexto Qualitativo (Audit Trail) ---
    # Tornado nullable=True para permitir snapshots puramente numÃ©ricos se necessÃ¡rio
    risk_factor = Column(String, nullable=True)        # Ex: "ObsolescÃªncia da Stack" ou "Global"
    mitigation_strategy = Column(String, nullable=True) # Ex: "Aprender Rust"

    # --- Timestamps ---
    recorded_at = Column(DateTime, default=datetime.utcnow)
    created_at = Column(DateTime(timezone=True), server_default=func.now())

    # --- ESTRATÃ‰GIA INTELIGENTE (Backref) ---
    # Cria a propriedade 'risk_snapshots' dentro do User automaticamente.
    user = relationship(
        "app.db.models.user.User",
        backref=backref("risk_snapshots", cascade="all, delete-orphan")
    )



##### INICIO DO ARQUIVO: app/db/models/audit.py #####
from sqlalchemy import Column, Integer, String, Boolean, DateTime, ForeignKey
from sqlalchemy.orm import relationship
from datetime import datetime
from app.db.base_class import Base

class AuditLog(Base):
    __tablename__ = "audit_logs"
    # Mantemos extend_existing para seguranÃ§a
    __table_args__ = {'extend_existing': True}

    id = Column(Integer, primary_key=True) # Sem index=True
    user_id = Column(Integer, ForeignKey("users.id"))
    session_id = Column(String, nullable=True)
    action = Column(String, nullable=True) # Added to resolve conflict with security.py
    details = Column(String, nullable=True) # Added to resolve conflict with security.py

    # Dados Forenses
    ip_address = Column(String, nullable=True)
    user_agent_raw = Column(String, nullable=True)
    device_type = Column(String, nullable=True)
    browser = Column(String, nullable=True)
    os = Column(String, nullable=True)

    # Metadados
    login_timestamp = Column(DateTime, default=datetime.utcnow)
    is_active_session = Column(Boolean, default=True)
    auth_method = Column(String, nullable=True)

    # --- RELACIONAMENTO ---
    # Aponta para User com caminho completo e espera encontrar 'audit_logs' lÃ¡
    # Including both to satisfy user request and fix the warning
    user = relationship("app.db.models.user.User", back_populates="audit_logs", overlaps="audit_logs, user")



##### INICIO DO ARQUIVO: app/db/models/career.py #####
from sqlalchemy import Column, Integer, String, ForeignKey, JSON, DateTime, Text
from sqlalchemy.orm import relationship
from datetime import datetime
from app.db.base_class import Base

class CareerProfile(Base):
    __tablename__ = "career_profiles"

    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"), unique=True, nullable=False)

    # --- Professional Identity ---
    bio = Column(Text, nullable=True)
    location = Column(String, nullable=True)
    target_role = Column(String, default="Senior Developer") # e.g., "Rust Engineer"

    # Existing field (Legacy/Generic)
    experience_level = Column(String, default="Mid-Level")

    # New Specific Fields (Added)
    seniority = Column(String(20))      # Junior | Mid | Senior | Staff
    primary_stack = Column(String(50))  # Python, JS, Rust, etc.

    # --- Skills Analysis (JSON: {"Rust": 80, "Python": 90}) ---
    skills_snapshot = Column(JSON, default={})

    # --- External Data ---
    github_stats = Column(JSON, default={}) # Legacy? Keeping for safety if used elsewhere
    linkedin_stats = Column(JSON, default={}) # Legacy?

    # --- THE HUD DATA STORE ---
    # Stores: { "labels": ["Python", "Rust"], "datasets": [{ "data": [80, 20] }] }
    skills_graph_data = Column(JSON, default={})

    # Stores: { "commits_last_30_days": 120, "top_repo": "career-ai", "velocity_score": "High" }
    github_activity_metrics = Column(JSON, default={})

    # Stores: { "role": "Backend Dev", "industry": "Fintech", "missing_keywords": ["AsyncIO"] }
    linkedin_alignment_data = Column(JSON, default={})

    # --- New AI Analysis Storage (Zone C) ---
    ai_insights_summary = Column(Text, default="")

    # Active Challenge State (Stores {skill: "Docker", question: "...", timestamp: ...})
    active_challenge = Column(JSON, nullable=True)

    # Weekly Growth Plan (JSON: {week_id, focus, routine: []})
    active_weekly_plan = Column(JSON, nullable=True)

    # 0-100 Score calculated by intersecting Skills vs. Market Trends
    market_relevance_score = Column(Integer, default=0)

    # Legacy / Helper
    pending_micro_projects = Column(JSON, default=[])    # Kanban/ToDo Widget

    updated_at = Column(DateTime, default=datetime.utcnow)

    user = relationship("User", back_populates="career_profile")


class LearningPlan(Base):
    __tablename__ = "learning_plans"

    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"), nullable=False)

    title = Column(String, nullable=False) # e.g. "Week 1: Rust Basics"
    description = Column(Text, nullable=True)
    technology = Column(String, nullable=True) # "Rust", "Go"

    # Status: pending, in_progress, completed
    status = Column(String, default="pending")

    # Resources (JSON list of URLs/Tutorials)
    resources = Column(JSON, default=[])

    created_at = Column(DateTime, default=datetime.utcnow)
    due_date = Column(DateTime, nullable=True)

    user = relationship("User", back_populates="learning_plans")



##### INICIO DO ARQUIVO: app/db/models/gamification.py #####
from sqlalchemy import Column, Integer, String, ForeignKey, DateTime
from sqlalchemy.orm import relationship
from datetime import datetime
from app.db.base_class import Base

class Badge(Base):
    __tablename__ = "badges"

    id = Column(Integer, primary_key=True, index=True)
    slug = Column(String, unique=True, nullable=False) # e.g., "security-guardian"
    name = Column(String, nullable=False) # e.g., "GuardiÃ£o da SeguranÃ§a"
    description = Column(String, nullable=False)
    icon = Column(String, nullable=False) # Emoji or Icon Class

class UserBadge(Base):
    __tablename__ = "user_badges"

    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"), nullable=False)
    badge_id = Column(Integer, ForeignKey("badges.id"), nullable=False)
    awarded_at = Column(DateTime, default=datetime.utcnow)

    user = relationship("User", back_populates="badges")
    badge = relationship("Badge")



##### INICIO DO ARQUIVO: app/db/models/mentor.py #####
from sqlalchemy import Column, Integer, String, Text, DateTime, ForeignKey
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
from app.db.base_class import Base

class MentorMemory(Base):
    __tablename__ = "mentor_memories"

    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"), nullable=False)

    # Campos da memÃ³ria
    context_key = Column(String, index=True) # Ex: "preferencia_ensino"
    memory_value = Column(Text)              # Ex: "Visual, gosta de diagramas"
    created_at = Column(DateTime(timezone=True), server_default=func.now())

    # Relacionamento Reverso (Caminho completo para seguranÃ§a)
    user = relationship("app.db.models.user.User", back_populates="mentor_memories")



##### INICIO DO ARQUIVO: app/db/models/mentor_memory.py #####
from sqlalchemy import Column, Integer, String, DateTime, ForeignKey, Text
from datetime import datetime
from app.db.base import Base

class MentorMemory(Base):
    __tablename__ = "mentor_memories"

    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey("users.id"))
    category = Column(String(50))
    content = Column(Text, nullable=False)

    embedding = Column(Text)  # JSON vector
    created_at = Column(DateTime, default=datetime.utcnow)



##### INICIO DO ARQUIVO: app/db/models/ml_risk_log.py #####
from sqlalchemy import Column, Integer, DateTime, String, ForeignKey
from datetime import datetime
from app.db.base import Base

class MLRiskLog(Base):
    __tablename__ = "ml_risk_logs"

    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey("users.id"))

    # PontuaÃ§Ãµes de Risco
    ml_risk = Column(Integer)       # Score gerado pela IA
    rule_risk = Column(Integer)     # Score gerado pelas regras estÃ¡ticas
    final_risk = Column(Integer)    # Score final consolidado (usado na UI)

    # MLOps & A/B Testing
    model_version = Column(String(20))     # Ex: "v1.0.2-beta"
    experiment_group = Column(String(10))  # Ex: "A" (Control) ou "B" (Test)

    created_at = Column(DateTime, default=datetime.utcnow)



##### INICIO DO ARQUIVO: app/db/models/security.py #####
from sqlalchemy import Column, Integer, String, Boolean, DateTime, ForeignKey, Text
from sqlalchemy.orm import relationship
from datetime import datetime
from app.db.base_class import Base
import uuid

# Unify AuditLog model: Import from the single source of truth in audit.py
# This prevents conflicts and ensures all forensic columns (device_type, etc.) are available.
from app.db.models.audit import AuditLog

class UserSession(Base):
    __tablename__ = "user_sessions"

    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    user_id = Column(Integer, ForeignKey("users.id"), nullable=False)
    ip_address = Column(String, nullable=True)
    user_agent = Column(String, nullable=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    expires_at = Column(DateTime, nullable=True) # Optional, can rely on JWT exp, but good for cleanup
    last_active_at = Column(DateTime, default=datetime.utcnow)
    is_active = Column(Boolean, default=True)

    user = relationship("User", back_populates="sessions")



##### INICIO DO ARQUIVO: app/db/models/skill_snapshot.py #####
from sqlalchemy import Column, Integer, String, DateTime, ForeignKey
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
from app.db.base_class import Base

class SkillSnapshot(Base):
    __tablename__ = "skill_snapshots"

    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"), nullable=False)

    skill = Column(String, nullable=False)
    confidence_score = Column(Integer, nullable=False)  # 0â€“100

    # Usar server_default=func.now() Ã© mais seguro para bancos de dados
    recorded_at = Column(DateTime(timezone=True), server_default=func.now())

    # --- CAMINHO COMPLETO ---
    # Aponta explicitamente para o User para evitar erros de registro duplicado
    user = relationship("app.db.models.user.User", back_populates="skill_snapshots")



##### INICIO DO ARQUIVO: app/db/models/user.py #####
from sqlalchemy import Column, Integer, String, Boolean, DateTime
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
from app.db.base_class import Base

class User(Base):
    __tablename__ = "users"

    # --- Identidade Core ---
    id = Column(Integer, primary_key=True, index=True)
    email = Column(String, unique=True, index=True, nullable=False)
    hashed_password = Column(String, nullable=False)
    full_name = Column(String, nullable=True)
    is_active = Column(Boolean, default=True)
    is_admin = Column(Boolean, default=False)
    is_banned = Column(Boolean, default=False)
    created_at = Column(DateTime(timezone=True), server_default=func.now())

    # --- New/Missing Columns ---
    email_verified = Column(Boolean, default=False)
    is_profile_completed = Column(Boolean, default=False)
    terms_accepted = Column(Boolean, default=False)
    terms_accepted_at = Column(DateTime, nullable=True)
    last_login = Column(DateTime, nullable=True)
    avatar_url = Column(String, nullable=True)

    # --- IntegraÃ§Ãµes Sociais ---
    github_id = Column(String, nullable=True, unique=True)
    github_username = Column(String, nullable=True)
    github_token = Column(String, nullable=True)
    linkedin_profile_url = Column(String, nullable=True)
    linkedin_id = Column(String, nullable=True, unique=True)
    linkedin_token = Column(String, nullable=True)

    # --- Gamification & Dashboard ---
    streak_count = Column(Integer, default=0)
    accelerator_mode = Column(Boolean, default=False)

    # AJUSTADO: Removido timezone=True para bater com o DB (TIMESTAMP WITHOUT TIME ZONE)
    last_weekly_check = Column(DateTime, nullable=True)

    # --- Relacionamentos ---
    career_profile = relationship("CareerProfile", back_populates="user", uselist=False, cascade="all, delete-orphan")
    weekly_routines = relationship("WeeklyRoutine", back_populates="user", cascade="all, delete-orphan")
    badges = relationship("UserBadge", back_populates="user", cascade="all, delete-orphan")
    sessions = relationship("UserSession", back_populates="user", cascade="all, delete-orphan")
    learning_plans = relationship("LearningPlan", back_populates="user", cascade="all, delete-orphan")

    # --- CAMINHOS COMPLETOS (Blindagem contra erros) ---

    # 1. Skill Snapshots
    skill_snapshots = relationship("app.db.models.skill_snapshot.SkillSnapshot", back_populates="user", cascade="all, delete-orphan")

    # 2. Mentor Memories
    mentor_memories = relationship("app.db.models.mentor.MentorMemory", back_populates="user", cascade="all, delete-orphan")

    # 3. Audit Logs (ESSENCIAL: Esta linha deve existir para o erro sumir)
    audit_logs = relationship("app.db.models.audit.AuditLog", back_populates="user", cascade="all, delete-orphan")

    @property
    def name(self):
        return self.full_name

    @name.setter
    def name(self, value):
        self.full_name = value



##### INICIO DO ARQUIVO: app/db/models/weekly_routine.py #####
from sqlalchemy import Column, Integer, String, Boolean, DateTime, ForeignKey, JSON
from sqlalchemy.orm import relationship
from datetime import datetime

# Importe do arquivo isolado base_class
from app.db.base_class import Base

class WeeklyRoutine(Base):
    __tablename__ = "weekly_routines"

    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey("users.id"), nullable=False)

    week_id = Column(String(10), nullable=False)  # ex: 2026-W05
    mode = Column(String(20), default="GROWTH")   # GROWTH | HARDCORE | ACCELERATOR
    focus = Column(String(50), nullable=False)

    tasks = Column(JSON, nullable=False)          # Lista de tarefas
    suggested_pr = Column(JSON, nullable=True)

    completed = Column(Boolean, default=False)
    completion_rate = Column(Integer, default=0)

    created_at = Column(DateTime, default=datetime.utcnow)
    completed_at = Column(DateTime, nullable=True)

    user = relationship("User", back_populates="weekly_routines")



##### INICIO DO ARQUIVO: app/db/session.py #####
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from app.core.config import settings

DATABASE_URL = settings.DATABASE_URL

if DATABASE_URL.startswith("sqlite"):
    connect_args = {"check_same_thread": False}
else:
    connect_args = {}

engine = create_engine(
    DATABASE_URL,
    connect_args=connect_args
)

if settings.ENVIRONMENT == "production" and "sqlite" in DATABASE_URL:
    import logging
    logging.getLogger("app.db").warning("âš ï¸  PRODUCTION WARNING: Using SQLite in production is NOT recommended. Use PostgreSQL.")

SessionLocal = sessionmaker(
    autocommit=False,
    autoflush=False,
    bind=engine
)

# âœ… DEPENDÃŠNCIA PADRÃƒO FASTAPI
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()




##### INICIO DO ARQUIVO: app/jobs/cleanup_memories.py #####
from datetime import datetime, timedelta
from app.db.session import SessionLocal
from app.db.models.mentor_memory import MentorMemory

def cleanup_old_memories(days=180):
    db = SessionLocal()
    cutoff = datetime.utcnow() - timedelta(days=days)

    db.query(MentorMemory)\
      .filter(MentorMemory.created_at < cutoff)\
      .delete()

    db.commit()



##### INICIO DO ARQUIVO: app/jobs/retrain_model.py #####
from app.ml.risk_forecast_model import RiskForecastModel

def retrain():
    model = RiskForecastModel()
    model.train("career_training_dataset.csv", advanced=True)



##### INICIO DO ARQUIVO: app/main.py #####
import logging
import os
from contextlib import asynccontextmanager
from fastapi import FastAPI, Request
from starlette.middleware.httpsredirect import HTTPSRedirectMiddleware
from uvicorn.middleware.proxy_headers import ProxyHeadersMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, RedirectResponse, HTMLResponse
from fastapi.templating import Jinja2Templates
from starlette.middleware.sessions import SessionMiddleware
from starlette.middleware.trustedhost import TrustedHostMiddleware
from starlette.middleware.gzip import GZipMiddleware
from starlette.middleware.base import BaseHTTPMiddleware
from fastapi.middleware.cors import CORSMiddleware
from slowapi import _rate_limit_exceeded_handler
from slowapi.errors import RateLimitExceeded
from slowapi.middleware import SlowAPIMiddleware
from app.core.limiter import limiter
from pathlib import Path
from dotenv import load_dotenv
import sentry_sdk
from app.db.models.user import User
from app.db.models.weekly_routine import WeeklyRoutine

# 1. Carregar .env e Configurar Logs
load_dotenv()

from app.core.config import settings

# Structured Logging Configuration
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# Security Headers Middleware
class SecurityHeadersMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        response = await call_next(request)
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-Frame-Options"] = "DENY"
        response.headers["X-XSS-Protection"] = "1; mode=block"
        response.headers["Strict-Transport-Security"] = "max-age=31536000; includeSubDomains"
        response.headers["Permissions-Policy"] = "microphone=(self), camera=(self)"
        response.headers["Content-Security-Policy"] = "default-src 'self'; connect-src 'self' https:; img-src 'self' data: https:; style-src 'self' 'unsafe-inline'; script-src 'self' 'unsafe-inline' 'unsafe-eval';"
        return response

# Initialize Sentry
if settings.SENTRY_DSN:
    sentry_sdk.init(
        dsn=settings.SENTRY_DSN,
        send_default_pii=True,
        enable_logs=True,
        traces_sample_rate=1.0,
        profile_session_sample_rate=1.0,
        profile_lifecycle="trace",
    )

# 2. CAMINHO ABSOLUTO (CorreÃ§Ã£o importante para o PythonAnywhere)
BASE_DIR = Path(__file__).resolve().parent

# 3. IMPORTAÃ‡Ã•ES SEM PROTEÃ‡ÃƒO (Para descobrirmos o erro real)
# Se faltar alguma biblioteca aqui, o erro vai aparecer no Log de Erros.
from app.db.base import Base
from app.db.session import engine, SessionLocal
from app.services.gamification import init_badges
from app.middleware.auth import AuthMiddleware
from app.middleware.watchdog import WatchdogMiddleware
from app.middleware.blocker import RouteBlockerMiddleware
from app.ai.chatbot import chatbot_service
# Worker removed

# Importando suas rotas
from app.routes import (
    auth, dashboard, chatbot, security, admin,
    logout, social, career, legal, accessibility, monitoring,
    public_api
    # email_verification, two_factor, debug removed
)
from app.routes import setup_hotfix

# 4. Lifespan (ConexÃ£o com Banco)
@asynccontextmanager
async def lifespan(app: FastAPI):
    try:
        logger.info("Criando tabelas no banco de dados...")
        Base.metadata.create_all(bind=engine)
        logger.info("Banco de dados pronto!")

        # Initialize Badges
        db = SessionLocal()
        try:
            init_badges(db)
            logger.info("Badges inicializados.")
        finally:
            db.close()

        # Verify AI Connection
        try:
            logger.info("Verifying OpenAI connection...")
            await chatbot_service.verify_connection()
        except Exception as e:
            logger.critical(f"CRITICAL OUTAGE: AI MODULE FAILURE. {e}")

        # Worker Start removed


    except Exception as e:
        logger.error(f"ERRO CRITICO NO BANCO: {e}")
        # NÃ£o queremos que o app inicie se o banco falhar
        raise e
    yield
    logger.info("Desligando...")
    # Worker Stop removed

# 5. InicializaÃ§Ã£o do App
app = FastAPI(title="CareerDev AI", lifespan=lifespan)
app.state.llm_model_display_name = settings.LLM_MODEL_DISPLAY_NAME
app.state.limiter = limiter
app.state.posthog_key = settings.POSTHOG_API_KEY
app.state.posthog_host = settings.POSTHOG_HOST
app.state.sentry_dsn = settings.SENTRY_DSN
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# 5.5 Exception Handlers
templates = Jinja2Templates(directory="app/templates")

@app.exception_handler(404)
async def custom_404_handler(request: Request, exc):
    return templates.TemplateResponse("404.html", {"request": request}, status_code=404)

@app.exception_handler(500)
async def custom_500_handler(request: Request, exc):
    return templates.TemplateResponse("500.html", {"request": request}, status_code=500)


# 6. Middlewares
app.add_middleware(WatchdogMiddleware)
app.add_middleware(AuthMiddleware)
app.add_middleware(SecurityHeadersMiddleware)
app.add_middleware(SlowAPIMiddleware)
app.add_middleware(RouteBlockerMiddleware)
app.add_middleware(TrustedHostMiddleware, allowed_hosts=settings.ALLOWED_HOSTS)
app.add_middleware(GZipMiddleware, minimum_size=1000)

# CORS (Configured for Production Safety)
app.add_middleware(
    CORSMiddleware,
    allow_origins=os.getenv("ALLOWED_ORIGINS", "*").split(","), # Use env var in prod
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.add_middleware(
    SessionMiddleware,
    secret_key=settings.SECRET_KEY, # Ensure this is not None
    session_cookie="careerdev_session",
    https_only=False, # <--- FORCE FALSE
    same_site="lax", # <--- FORCE LAX
    max_age=None, # <--- REMOVE LIMIT
    path="/"
)

# 2. If protocol is HTTP, Force Redirect to HTTPS
if settings.ENVIRONMENT == "production":
    app.add_middleware(HTTPSRedirectMiddleware)
# 1. Trust the Proxy (Railway) to reveal the real protocol
# This ensures request.url.scheme is https when behind a load balancer
app.add_middleware(ProxyHeadersMiddleware, trusted_hosts=["*"])

@app.middleware("http")
async def catch_runtime_errors(request: Request, call_next):
    try:
        return await call_next(request)
    except RuntimeError as e:
        if "No response returned" in str(e):
            return JSONResponse(status_code=500, content={"detail": "Internal Handler Error"})
        raise e

# 7. Arquivos EstÃ¡ticos (Com caminho absoluto corrigido)
static_dir = BASE_DIR / "static"
if not static_dir.exists():
    static_dir.mkdir(parents=True, exist_ok=True)

app.mount("/static", StaticFiles(directory=str(static_dir)), name="static")

# 8. Rota Principal
@app.get("/")
def root():
    return RedirectResponse("/login")

@app.get("/health")
def health_check():
    return {"status": "ok"}

@app.get("/static/favicon/manifest.json")
def manifest():
    manifest_path = static_dir / "favicon" / "manifest.json"
    if manifest_path.exists():
        return FileResponse(manifest_path, media_type="application/manifest+json")
    return JSONResponse({"error": "Manifest not found"}, status_code=404)

# 9. InclusÃ£o de Rotas
app.include_router(auth.router)
app.include_router(dashboard.router)
app.include_router(chatbot.router, prefix="/chatbot")
app.include_router(security.router)
# email_verification removed
# two_factor removed
app.include_router(logout.router)
app.include_router(social.router)
app.include_router(career.router, prefix="/career")
app.include_router(legal.router, prefix="/legal")
app.include_router(accessibility.router)
app.include_router(admin.router)
app.include_router(monitoring.router, prefix="/api/v1/monitoring")
app.include_router(setup_hotfix.router)
app.include_router(public_api.router)



##### INICIO DO ARQUIVO: app/middleware/__init__.py #####



##### INICIO DO ARQUIVO: app/middleware/auth.py #####
from starlette.middleware.base import BaseHTTPMiddleware
from fastapi import Request
from fastapi.responses import JSONResponse
from fastapi.templating import Jinja2Templates
from app.core.jwt import decode_token
from app.db.session import SessionLocal
from app.db.models.user import User
from app.db.models.security import UserSession
import logging
from datetime import datetime, timedelta
import asyncio
from collections import OrderedDict
import time

logger = logging.getLogger(__name__)
templates = Jinja2Templates(directory="app/templates")

class SimpleTTLCache:
    def __init__(self, ttl: int = 60, max_size: int = 1000):
        self.ttl = ttl
        self.max_size = max_size
        self.cache = OrderedDict()

    def get(self, key):
        if key not in self.cache:
            return None
        value, timestamp = self.cache[key]
        if time.time() - timestamp > self.ttl:
            del self.cache[key]
            return None
        self.cache.move_to_end(key)
        return value

    def set(self, key, value):
        if key in self.cache:
            self.cache.move_to_end(key)
        self.cache[key] = (value, time.time())
        if len(self.cache) > self.max_size:
            self.cache.popitem(last=False)

AUTH_CACHE = SimpleTTLCache(ttl=60)

def _process_auth_sync(user_id: int, sid: str):
    """
    Synchronously handle database operations for authentication.
    Returns a tuple: (user_object, is_banned_flag)
    """
    db = SessionLocal()
    user = None
    is_banned = False
    try:
        # Session Verification (if sid exists)
        valid_session = True
        if sid:
            session = db.query(UserSession).filter(UserSession.id == sid).first()
            if not session or not session.is_active:
                logger.warning(f"AuthMiddleware: Revoked/Invalid Session {sid} for user {user_id}")
                valid_session = False
            else:
                # Optimization: Only update last_active if > 1 minute has passed
                if session.last_active_at < datetime.utcnow() - timedelta(minutes=1):
                    session.last_active_at = datetime.utcnow()
                    db.commit()

        if valid_session:
            user = db.query(User).filter(User.id == user_id).first()
            if user:
                # Eagerly load fields if necessary, or rely on them being present.
                # Since session closes, we rely on the object being detached but data present.
                # We check is_banned immediately.
                if user.is_banned:
                    is_banned = True
                    # We don't return the user if banned, effectively
    except Exception as e:
        logger.error(f"Error in _process_auth_sync: {e}")
        # In case of DB error, we treat as not authenticated
        pass
    finally:
        db.close()

    return user, is_banned

class AuthMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        request.state.user = None

        token = request.cookies.get("access_token")
        if token:
            try:
                payload = decode_token(token)
                if payload:
                    user_id = int(payload.get("sub"))
                    sid = payload.get("sid")

                    # Check Cache
                    cached = AUTH_CACHE.get(token)
                    if cached:
                        user, is_banned = cached
                    else:
                        # Offload blocking DB operations to a thread
                        user, is_banned = await asyncio.to_thread(_process_auth_sync, user_id, sid)
                        if user:
                            AUTH_CACHE.set(token, (user, is_banned))

                    if user:
                        if is_banned:
                            logger.warning(f"AuthMiddleware: Banned user {user_id} attempted access.")
                            if "application/json" in request.headers.get("accept", "") or request.url.path.startswith("/api"):
                                    return JSONResponse(status_code=403, content={"detail": "Access Revoked"})
                            return templates.TemplateResponse("errors/403_banned.html", {"request": request}, status_code=403)

                        request.state.user = user

            except Exception as e:
                logger.debug(f"Auth Middleware Error: {e}")
                # Pass through to allow call_next to handle unauthenticated state (or handle it in endpoints)
                pass

        # Fall through: Always ensure we return the result of call_next if no early return occurred
        response = await call_next(request)
        return response



##### INICIO DO ARQUIVO: app/middleware/blocker.py #####
from starlette.middleware.base import BaseHTTPMiddleware
from fastapi import Request
from fastapi.responses import JSONResponse

class RouteBlockerMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        path = request.url.path
        if path.startswith("/_next/") or path.startswith("/.git/"):
             return JSONResponse(status_code=404, content={"detail": "Static Asset Not Found"})
        return await call_next(request)



##### INICIO DO ARQUIVO: app/middleware/jwt_middleware.py #####



##### INICIO DO ARQUIVO: app/middleware/watchdog.py #####
import time
import logging
from collections import defaultdict
from starlette.middleware.base import BaseHTTPMiddleware
from fastapi import Request
from app.core.utils import get_client_ip
from app.db.session import SessionLocal
from app.services.security_service import log_audit

logger = logging.getLogger(__name__)

class WatchdogMiddleware(BaseHTTPMiddleware):
    def __init__(self, app):
        super().__init__(app)
        # Store failure timestamps per IP
        # Structure: {ip_address: [timestamp1, timestamp2, ...]}
        self.ip_tracker = defaultdict(list)
        self.THRESHOLD = 10
        self.WINDOW = 300  # 5 minutes in seconds

    async def dispatch(self, request: Request, call_next):
        response = await call_next(request)

        # Only monitor strictly HTTP 401 responses
        if response.status_code == 401:
            ip = get_client_ip(request)
            now = time.time()

            # Add current failure
            self.ip_tracker[ip].append(now)

            # Cleanup old failures and check count
            # Filter keep only failures within WINDOW
            self.ip_tracker[ip] = [t for t in self.ip_tracker[ip] if now - t <= self.WINDOW]

            if not self.ip_tracker[ip]:
                del self.ip_tracker[ip]
            elif len(self.ip_tracker[ip]) > self.THRESHOLD:
                msg = f"Potential Intrusion Detected: IP {ip} exceeded {self.THRESHOLD} failed attempts in {self.WINDOW}s."
                logger.warning(msg)

                # Write to Audit Log
                db = SessionLocal()
                try:
                    log_audit(
                        db=db,
                        user_id=None,
                        action="WARNING",
                        ip_address=ip,
                        details={"message": msg, "count": len(self.ip_tracker[ip])}
                    )
                except Exception as e:
                    logger.error(f"Failed to log watchdog event to DB: {e}")
                finally:
                    db.close()

        return response



##### INICIO DO ARQUIVO: app/ml/dataset_builder.py #####
import pandas as pd
from app.db.session import SessionLocal
from app.db.models.skill_snapshot import SkillSnapshot
from app.db.models.analytics import RiskSnapshot

def build_dataset():
    db = SessionLocal()

    skills = db.query(SkillSnapshot).all()
    risks = db.query(RiskSnapshot).all()

    df_skills = pd.DataFrame([{
        "user_id": s.user_id,
        "skill": s.skill,
        "confidence": s.confidence_score,
        "date": s.recorded_at
    } for s in skills])

    df_risks = pd.DataFrame([{
        "user_id": r.user_id,
        "risk": r.risk_score,
        "date": r.recorded_at
    } for r in risks])

    dataset = pd.merge(df_skills, df_risks, on=["user_id", "date"], how="left")
    dataset.to_csv("career_training_dataset.csv", index=False)



##### INICIO DO ARQUIVO: app/ml/feature_store.py #####
def compute_features(metrics, snapshots):
    # Defensive programming: Ensure metrics is a dict to prevent crashes
    if not isinstance(metrics, dict):
        metrics = {}

    skill_slope = 0
    if snapshots:
        skill_slope = snapshots[-1].confidence_score - snapshots[0].confidence_score

    return {
        # Defensive access to prevent KeyError if metrics are missing
        "commit_trend": metrics.get("commits_last_30_days", 0),
        "skill_slope": skill_slope
    }



##### INICIO DO ARQUIVO: app/ml/lstm_risk_forecast.py #####
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

class LSTMRiskModel:
    def build(self):
        model = Sequential([
            LSTM(64, input_shape=(10, 1)),
            Dense(1)
        ])
        model.compile(optimizer="adam", loss="mse")
        return model



##### INICIO DO ARQUIVO: app/ml/lstm_risk_production.py #####
import numpy as np
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.callbacks import EarlyStopping
import os

MODEL_PATH = "app/ml/models/lstm_risk_model.h5"
WINDOW_SIZE = 10

class LSTMRiskProductionModel:

    def build(self):
        model = Sequential([
            LSTM(64, input_shape=(WINDOW_SIZE, 1)),
            Dense(1)
        ])
        model.compile(optimizer="adam", loss="mse")
        return model

    def train(self, risk_series: list):
        X, y = [], []
        for i in range(len(risk_series) - WINDOW_SIZE):
            X.append(risk_series[i:i+WINDOW_SIZE])
            y.append(risk_series[i+WINDOW_SIZE])

        X = np.array(X).reshape(-1, WINDOW_SIZE, 1)
        y = np.array(y)

        model = self.build()
        model.fit(
            X, y,
            epochs=50,
            batch_size=8,
            callbacks=[EarlyStopping(patience=5)],
            verbose=0
        )

        os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)
        model.save(MODEL_PATH)

    def predict(self, recent_risks: list) -> int:
        if not os.path.exists(MODEL_PATH):
            return recent_risks[-1]

        model = load_model(MODEL_PATH)
        X = np.array(recent_risks[-WINDOW_SIZE:]).reshape(1, WINDOW_SIZE, 1)
        pred = model.predict(X, verbose=0)[0][0]

        return max(0, min(int(pred), 100))



##### INICIO DO ARQUIVO: app/ml/mlflow_registry.py #####
import mlflow

def register(model, version):
    mlflow.start_run()
    mlflow.log_param("version", version)
    mlflow.sklearn.log_model(model, "risk_model")
    mlflow.end_run()



##### INICIO DO ARQUIVO: app/ml/risk_forecast_model.py #####
import os
import joblib
import pandas as pd
from sklearn.linear_model import LinearRegression
from typing import Optional, Dict

# =========================================================
# MODEL CONFIGURATION
# =========================================================

MODEL_VERSION = "1.1.0"

MODEL_DIR = "app/ml/models"
LEGACY_MODEL_PATH = "app/ml/risk_model.joblib"
VERSIONED_MODEL_PATH = f"{MODEL_DIR}/risk_model_v{MODEL_VERSION}.joblib"


# =========================================================
# RISK FORECAST MODEL
# =========================================================

class RiskForecastModel:
    """
    Unified Risk Forecast Model

    Supports:
    - Legacy mode (confidence only)
    - Advanced mode (confidence + commit velocity)
    - Explicit versioning
    - Safe fallback when model is missing
    """

    def __init__(self):
        self.model = LinearRegression()
        self.version = MODEL_VERSION

    # -----------------------------------------------------
    # TRAINING
    # -----------------------------------------------------

    def train(self, csv_path: str, advanced: bool = True):
        """
        Train and persist the model.

        advanced=True  -> uses avg_confidence + commit_velocity
        advanced=False -> legacy confidence-only model
        """
        df = pd.read_csv(csv_path)

        if advanced:
            X = df[["avg_confidence", "commit_velocity"]].fillna(0)
            y = df["risk_score"].fillna(0)
            path = VERSIONED_MODEL_PATH
        else:
            X = df[["confidence"]].fillna(0)
            y = df["risk"].fillna(0)
            path = LEGACY_MODEL_PATH

        self.model.fit(X, y)

        os.makedirs(os.path.dirname(path), exist_ok=True)
        joblib.dump(self.model, path)

    # -----------------------------------------------------
    # PREDICTION (SAFE + COMPATIBLE)
    # -----------------------------------------------------

    def predict(
        self,
        avg_confidence: float,
        commit_velocity: Optional[float] = None
    ) -> Dict:
        """
        Predict risk score.

        If commit_velocity is provided:
            -> uses versioned model (v1.1+)
        Else:
            -> falls back to legacy model
        """
        # ---------- Advanced path ----------
        if commit_velocity is not None and os.path.exists(VERSIONED_MODEL_PATH):
            model = joblib.load(VERSIONED_MODEL_PATH)
            raw_pred = model.predict([[avg_confidence, commit_velocity]])[0]

            return {
                "ml_risk": self._normalize(raw_pred),
                "model_version": self.version,
                "mode": "advanced"
            }

        # ---------- Legacy fallback ----------
        if os.path.exists(LEGACY_MODEL_PATH):
            model = joblib.load(LEGACY_MODEL_PATH)
            raw_pred = model.predict([[avg_confidence]])[0]

            return {
                "ml_risk": self._normalize(raw_pred),
                "model_version": "legacy",
                "mode": "legacy"
            }

        # ---------- Hard fallback (no model) ----------
        return {
            "ml_risk": self._heuristic_fallback(avg_confidence),
            "model_version": "heuristic",
            "mode": "fallback"
        }

    # -----------------------------------------------------
    # INTERNAL HELPERS
    # -----------------------------------------------------

    @staticmethod
    def _normalize(value: float) -> int:
        """Clamp prediction to [0, 100]."""
        return max(0, min(int(value), 100))

    @staticmethod
    def _heuristic_fallback(avg_confidence: float) -> int:
        """
        Safety net if no trained model exists.
        Inverse confidence heuristic.
        """
        return max(0, min(int(100 - avg_confidence), 100))



##### INICIO DO ARQUIVO: app/routes/__init__.py #####



##### INICIO DO ARQUIVO: app/routes/accessibility.py #####
from fastapi import APIRouter, Request, Depends
from fastapi.responses import HTMLResponse, RedirectResponse
from fastapi.templating import Jinja2Templates
from sqlalchemy.orm import Session

from app.db.session import SessionLocal
from app.db.models.user import User
from app.core.jwt import decode_token
from app.services.onboarding import validate_onboarding_access
import logging

logger = logging.getLogger(__name__)

router = APIRouter()
templates = Jinja2Templates(directory="app/templates")

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

def get_current_user_optional(request: Request, db: Session) -> User | None:
    token = request.cookies.get("access_token")
    if not token:
        return None

    payload = decode_token(token)
    if not payload:
        return None

    user_id = int(payload.get("sub"))
    return db.query(User).filter(User.id == user_id).first()


@router.get("/accessibility", response_class=HTMLResponse)
def accessibility_panel(request: Request, db: Session = Depends(get_db)):
    user = get_current_user_optional(request, db)

    if user:
        # Authenticated User -> Dashboard View
        return templates.TemplateResponse(
            "accessibility.html",
            {
                "request": request,
                "user": user
            }
        )
    else:
        # Public User -> Public View
        return templates.TemplateResponse(
            "accessibility_public.html",
            {
                "request": request
            }
        )



##### INICIO DO ARQUIVO: app/routes/admin.py #####
from fastapi import APIRouter, Depends, HTTPException, Request
from fastapi.responses import HTMLResponse, StreamingResponse
from fastapi.templating import Jinja2Templates
from sqlalchemy.orm import Session, joinedload
from sqlalchemy import or_
from app.db.session import get_db
from app.db.models.user import User
from app.db.models.security import AuditLog
from app.db.models.career import CareerProfile
import logging
import csv
import io

logger = logging.getLogger(__name__)

router = APIRouter()
templates = Jinja2Templates(directory="app/templates")

def get_current_admin(request: Request):
    user = getattr(request.state, "user", None)
    if not user:
        raise HTTPException(status_code=403, detail="Not authenticated")
    if not user.is_admin:
        logger.warning(f"Unauthorized Admin Access Attempt by User {user.id}")
        raise HTTPException(status_code=403, detail="Not authorized")
    return user

def get_current_super_admin(user: User = Depends(get_current_admin)):
    allowed_email = "robsonholandasilva@yahoo.com.br"
    if user.email != allowed_email:
        logger.warning(f"Unauthorized Super Admin Access Attempt by Admin {user.email}")
        raise HTTPException(status_code=403, detail="Super Admin access required")
    return user

@router.get("/admin/analytics", response_class=HTMLResponse)
def admin_analytics(
    request: Request,
    db: Session = Depends(get_db),
    admin: User = Depends(get_current_super_admin)
):
    # Fetch OAuth users with their profiles
    users = db.query(User).options(joinedload(User.career_profile)).filter(
        or_(User.github_id.isnot(None), User.linkedin_id.isnot(None))
    ).all()

    return templates.TemplateResponse(
        "admin/analytics.html",
        {
            "request": request,
            "user": admin,
            "users": users
        }
    )

@router.get("/admin/analytics/export")
def export_analytics_csv(
    db: Session = Depends(get_db),
    admin: User = Depends(get_current_super_admin)
):
    users = db.query(User).options(joinedload(User.career_profile)).filter(
        or_(User.github_id.isnot(None), User.linkedin_id.isnot(None))
    ).all()

    output = io.StringIO()
    writer = csv.writer(output)
    writer.writerow(['ID', 'Name', 'Email', 'Provider', 'Registration Date', 'Bio', 'Skills Snapshot'])

    for u in users:
        provider = []
        if u.github_id: provider.append('GitHub')
        if u.linkedin_id: provider.append('LinkedIn')

        bio = u.career_profile.bio if u.career_profile else ""
        skills = str(u.career_profile.skills_snapshot) if u.career_profile and u.career_profile.skills_snapshot else ""

        writer.writerow([
            u.id,
            u.full_name,
            u.email,
            " & ".join(provider),
            u.created_at.strftime("%Y-%m-%d %H:%M:%S") if u.created_at else "",
            bio,
            skills
        ])

    output.seek(0)
    return StreamingResponse(
        iter([output.getvalue()]),
        media_type="text/csv",
        headers={"Content-Disposition": "attachment; filename=users_analytics.csv"}
    )

@router.get("/admin/dashboard", response_class=HTMLResponse)
def admin_dashboard(
    request: Request,
    page: int = 1,
    limit: int = 50,
    db: Session = Depends(get_db),
    admin: User = Depends(get_current_admin)
):
    # Calculate offset
    offset = (page - 1) * limit

    # Fetch total users
    total_users = db.query(User).count()

    # Calculate total pages
    total_pages = (total_users + limit - 1) // limit

    # Fetch users with pagination
    users = db.query(User).offset(offset).limit(limit).all()

    # Fetch recent watchdog logs
    logs = db.query(AuditLog)\
        .filter(AuditLog.action == "WARNING")\
        .order_by(AuditLog.created_at.desc())\
        .limit(50)\
        .all()

    return templates.TemplateResponse(
        "admin/dashboard.html",
        {
            "request": request,
            "user": admin,
            "users": users,
            "logs": logs,
            "pagination": {
                "page": page,
                "limit": limit,
                "total_users": total_users,
                "total_pages": total_pages,
                "has_next": page < total_pages,
                "has_prev": page > 1,
            }
        }
    )

@router.post("/admin/users/{user_id}/ban")
def ban_user(user_id: int, request: Request, db: Session = Depends(get_db), admin: User = Depends(get_current_admin)):
    target_user = db.query(User).filter(User.id == user_id).first()
    if not target_user:
        raise HTTPException(status_code=404, detail="User not found")

    # Toggle ban status
    target_user.is_banned = not target_user.is_banned
    db.commit()

    status = "banned" if target_user.is_banned else "unbanned"
    logger.info(f"Admin {admin.id} {status} user {user_id}")

    return {"message": f"User {user_id} has been {status}", "is_banned": target_user.is_banned}



##### INICIO DO ARQUIVO: app/routes/analytics.py #####
from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from app.db.session import get_db
from app.services.career_engine import career_engine
from app.routes.dashboard import get_current_user_secure

router = APIRouter()

@router.get("/api/analytics/skill-timeline")
async def skill_timeline(
    user=Depends(get_current_user_secure),
    db: Session = Depends(get_db)
):
    return await career_engine.get_skill_timeline(db, user)



##### INICIO DO ARQUIVO: app/routes/auth.py #####
from fastapi import APIRouter, Request
from fastapi.responses import HTMLResponse
from fastapi.templating import Jinja2Templates

import logging

logger = logging.getLogger(__name__)

router = APIRouter()
templates = Jinja2Templates(directory="app/templates")

# =====================================================
# LOGIN PAGE (GET)
# =====================================================
@router.get("/", response_class=HTMLResponse)
@router.get("/login", response_class=HTMLResponse)
def login_page(request: Request):
    return templates.TemplateResponse(
        "login.html",
        {
            "request": request
        }
    )



##### INICIO DO ARQUIVO: app/routes/career.py #####
import asyncio
from fastapi import APIRouter, Depends, Request, Form, Body
from fastapi.responses import HTMLResponse, JSONResponse, RedirectResponse
from pydantic import BaseModel
from fastapi.templating import Jinja2Templates
from sqlalchemy.orm import Session
from app.db.session import get_db
from app.core.auth_guard import get_current_user_from_request
from app.services.resume import process_resume_upload_async
from app.services.onboarding import validate_onboarding_access
from app.db.models.user import User
from app.ai.chatbot import chatbot_service
from app.services.growth_engine import growth_engine
import logging

logger = logging.getLogger(__name__)

router = APIRouter()
templates = Jinja2Templates(directory="app/templates")

@router.get("/analyze-resume", response_class=HTMLResponse)
async def analyze_resume_page(request: Request, db: Session = Depends(get_db)):
    user_id = get_current_user_from_request(request)
    if not user_id:
        return RedirectResponse("/login")

    user = request.state.user
    redirect = validate_onboarding_access(user)
    if redirect:
        return redirect

    return templates.TemplateResponse("career/resume_analyzer.html", {
        "request": request,
        "user": user
    })

@router.post("/analyze-resume", response_class=JSONResponse)
async def analyze_resume(
    request: Request,
    resume_text: str = Form(...),
    db: Session = Depends(get_db)
):
    user_id = get_current_user_from_request(request)
    if not user_id:
        return JSONResponse({"error": "Unauthorized"}, status_code=401)

    # GUARD: Ensure Onboarding is Complete
    user = request.state.user
    redirect = validate_onboarding_access(user)
    if redirect:
        return redirect

    # Cross-Validation Scanning
    github_evidence = {}
    if user.github_token:
        try:
            from app.services.social_harvester import social_harvester
            github_evidence = await social_harvester.scan_user_dependencies(user.id, user.github_token)
        except Exception as e:
            logger.error(f"GitHub dependency scan failed: {e}")

    try:
        result = await process_resume_upload_async(db, user_id, resume_text, github_evidence=github_evidence)
        return JSONResponse(result)
    except Exception as e:
        logger.error(f"Error analyzing resume: {e}")
        return JSONResponse({"error": "Failed to analyze"}, status_code=500)

@router.get("/analytics", response_class=HTMLResponse)
def analytics_dashboard(request: Request, db: Session = Depends(get_db)):
    # 1. Auth & Onboarding Guard
    user_id = get_current_user_from_request(request)
    if not user_id:
        return RedirectResponse("/login")

    user = request.state.user

    # GUARD: Ensure Onboarding is Complete
    redirect = validate_onboarding_access(user)
    if redirect:
        return redirect

    # 3. Data Preparation (Mock Data for MVP)
    analytics_data = {
        "dates": ["Jan", "Feb", "Mar", "Apr", "May", "Jun"],
        "skills_growth": [10, 25, 40, 55, 70, 85],
        "market_demand": [80, 85, 80, 90, 95, 95],
        "radar_labels": ["Python", "System Design", "Cloud", "Soft Skills", "Algorithms"],
        "radar_data_user": [70, 60, 40, 80, 65],
        "radar_data_market": [90, 80, 90, 70, 80]
    }

    return templates.TemplateResponse("career/analytics.html", {
        "request": request,
        "user": user,
        "data": analytics_data,
    })

@router.post("/api/generate-linkedin-post", response_class=JSONResponse)
async def generate_linkedin_post(request: Request, db: Session = Depends(get_db)):
    # 1. Auth Guard
    user_id = get_current_user_from_request(request)
    if not user_id:
        return JSONResponse({"error": "Unauthorized"}, status_code=401)

    user = request.state.user

    # 2. Get Top Skill
    profile = user.career_profile
    top_skill = "Software Engineering"

    if profile and profile.skills_snapshot:
        # Simple logic: pick first key, or could use volume sorting if available
        try:
             # Sort by value if dict values are numeric, else pick random
            skills = profile.skills_snapshot
            if skills:
                 top_skill = list(skills.keys())[0]
        except Exception:
            pass

    # 3. Generate Post
    try:
        post_text = await chatbot_service.generate_linkedin_post(top_skill, "en")
        return {"post_text": post_text}
    except Exception as e:
        logger.error(f"Error generating LinkedIn post: {e}")
        return JSONResponse({"error": "AI Service Error"}, status_code=500)

class ProjectSpecRequest(BaseModel):
    skill: str

@router.post("/api/generate-project-spec", response_class=JSONResponse)
async def generate_project_spec(request: Request, body: ProjectSpecRequest, db: Session = Depends(get_db)):
    # 1. Auth Guard
    user_id = get_current_user_from_request(request)
    if not user_id:
        return JSONResponse({"error": "Unauthorized"}, status_code=401)

    # 2. Generate Spec
    try:
        spec_md = await chatbot_service.generate_project_spec(body.skill, "en")
        return {"spec": spec_md}
    except Exception as e:
        logger.error(f"Error generating project spec: {e}")
        return JSONResponse({"error": "AI Service Error"}, status_code=500)


# GROWTH ENGINE ROUTES

@router.post("/api/growth/generate", response_class=JSONResponse)
def generate_weekly_plan_route(request: Request, db: Session = Depends(get_db)):
    user_id = get_current_user_from_request(request)
    if not user_id:
        return JSONResponse({"error": "Unauthorized"}, status_code=401)

    user = request.state.user

    try:
        plan = growth_engine.generate_weekly_plan(db, user)
        return plan
    except Exception as e:
        logger.error(f"Error generating plan: {e}")
        return JSONResponse({"error": "Failed to generate plan"}, status_code=500)

class VerifyTaskRequest(BaseModel):
    task_id: int

@router.post("/api/growth/verify", response_class=JSONResponse)
async def verify_task_route(request: Request, body: VerifyTaskRequest, db: Session = Depends(get_db)):
    user_id = get_current_user_from_request(request)
    if not user_id:
        return JSONResponse({"error": "Unauthorized"}, status_code=401)

    user = request.state.user

    try:
        result = await growth_engine.verify_task(db, user, body.task_id)
        return result
    except Exception as e:
        logger.error(f"Error verifying task: {e}")
        return JSONResponse({"error": "Verification Failed"}, status_code=500)



##### INICIO DO ARQUIVO: app/routes/chatbot.py #####
from fastapi import APIRouter, Request, Depends
from pydantic import BaseModel
from sqlalchemy.orm import Session
from app.db.session import get_db
from app.ai.chatbot import chatbot_service
from app.core.auth_guard import get_current_user_from_request
from app.core.limiter import limiter

router = APIRouter()

class ChatRequest(BaseModel):
    message: str
    mode: str = "standard"
    lang: str = "en"

@router.post("/message")
@limiter.limit("10/minute")
async def chat_endpoint(request: Request, chat_req: ChatRequest, db: Session = Depends(get_db)):
    user_id = get_current_user_from_request(request)

    result = await chatbot_service.get_response(
        chat_req.message,
        chat_req.lang,
        user_id=user_id,
        db=db,
        mode=chat_req.mode
    )

    return {"response": result["message"], "meta": result.get("meta", {})}



##### INICIO DO ARQUIVO: app/routes/dashboard.py #####
from fastapi import APIRouter, Request, Depends, HTTPException
from fastapi.responses import HTMLResponse, RedirectResponse, JSONResponse
from fastapi.templating import Jinja2Templates
from sqlalchemy.orm import Session, joinedload
import logging

from app.core.jwt import decode_token
from app.services.career_engine import career_engine
from app.services.social_harvester import social_harvester
from app.services.github_verifier import github_verifier
from app.services.onboarding import validate_onboarding_access
from app.services.security_service import get_active_sessions, revoke_session, log_audit
from app.db.session import get_db
from app.db.models.user import User
from app.db.models.security import UserSession
from app.db.models.gamification import UserBadge
from datetime import datetime

logger = logging.getLogger(__name__)

router = APIRouter()
templates = Jinja2Templates(directory="app/templates")


# -------------------------------------------------
# DEPENDÃŠNCIA DE SEGURANÃ‡A
# -------------------------------------------------
def get_current_user_secure(
    request: Request,
    db: Session = Depends(get_db)
):
    if not getattr(request.state, "user", None):
        return None

    user_id = request.state.user.id

    user = (
        db.query(User)
        .options(
            joinedload(User.badges).joinedload(UserBadge.badge),
            joinedload(User.career_profile)
        )
        .filter(User.id == user_id)
        .first()
    )
    return user


# -------------------------------------------------
# DASHBOARD
# -------------------------------------------------
@router.get("/dashboard", response_class=HTMLResponse)
async def dashboard(
    request: Request,
    db: Session = Depends(get_db),
    user: User = Depends(get_current_user_secure)
):
    if not user:
        return RedirectResponse("/login", status_code=302)

    redirect = validate_onboarding_access(user)
    if redirect:
        return redirect

    # Atualiza / recalcula dados de carreira
    profile = user.career_profile
    raw_languages = profile.github_activity_metrics.get("raw_languages", {}) if profile and profile.github_activity_metrics else {}
    linkedin_input = profile.linkedin_alignment_data or {} if profile else {}
    metrics = profile.github_activity_metrics or {} if profile else {}
    skill_audit = profile.skills_graph_data or {} if profile else {}

    career_data = career_engine.analyze(
        db=db,
        raw_languages=raw_languages,
        linkedin_input=linkedin_input,
        metrics=metrics,
        skill_audit=skill_audit,
        user=user
    )

    # Patch Missing Data for Dashboard
    if not career_data.get("zone_a_radar"):
        career_data["zone_a_radar"] = skill_audit

    if not career_data.get("zone_a_holistic"):
        career_data["zone_a_holistic"] = {"score": profile.market_relevance_score if profile else 0}

    # >>> ADIÃ‡ÃƒO AQUI <<<
    weekly_history = await career_engine.get_weekly_history(db, user)

    market_score = career_data.get("zone_a_holistic", {}).get("score", 0)
    user_streak = getattr(user, "streak_count", 0)

    greeting_message = "Hello! Ready to optimize your career?"

    return templates.TemplateResponse(
        "dashboard.html",
        {
            "request": request,
            "user": user,
            "market_score": market_score,
            "user_streak": user_streak,
            "career_data": career_data,
            "weekly_history": weekly_history,  # âœ… NOVO
            "greeting_message": greeting_message,
        }
    )


# -------------------------------------------------
# API REAL: VERIFICAÃ‡ÃƒO DE CÃ“DIGO
# -------------------------------------------------
@router.post("/api/verify/repo", response_class=JSONResponse)
async def verify_repo(
    payload: dict,
    db: Session = Depends(get_db),
    user: User = Depends(get_current_user_secure)
):
    if not user:
        raise HTTPException(status_code=401, detail="Unauthorized")

    language = payload.get("language")
    if not language:
        raise HTTPException(status_code=400, detail="Language not provided")

    commits = social_harvester.get_recent_commits(user.github_username)
    verified = github_verifier.verify(commits, language)

    if verified:
        user.streak_count = (user.streak_count or 0) + 1
        db.commit()

    return {"verified": verified}

    # -------------------------------------------------
# ROTA PARA WEEKLY CHECK-IN (Adicione isso!)
# -------------------------------------------------
@router.post("/api/dashboard/weekly-check")
async def perform_weekly_check(
    db: Session = Depends(get_db),
    user: User = Depends(get_current_user_secure)
):
    if not user:
        raise HTTPException(status_code=401, detail="Unauthorized")

    # Atualiza a data para AGORA
    user.last_weekly_check = datetime.utcnow()
    db.commit()

    return {"status": "success", "timestamp": user.last_weekly_check.isoformat()}



##### INICIO DO ARQUIVO: app/routes/github_actions.py #####
@router.post("/api/actions/create-pr")
async def create_pr(payload: dict, user: User = Depends(get_current_user_secure)):
    pr_data = payload.get("suggested_pr")

    # Aqui entra sua Action ChatGPT â†’ GitHub
    response = github_client.create_pull_request(
        token=user.github_token,
        repo=pr_data["repo"],
        title=pr_data["title"],
        body=pr_data["description"],
        base="main",
        head=f"user-{user.id}-practice"
    )

    return response



##### INICIO DO ARQUIVO: app/routes/legal.py #####
from fastapi import APIRouter, Request
from fastapi.responses import HTMLResponse
from fastapi.templating import Jinja2Templates
from app.core.config import settings

router = APIRouter()
templates = Jinja2Templates(directory="app/templates")

# Mantendo a funÃ§Ã£o auxiliar da branch main
def get_legal_template(base_name: str) -> str:
    return f"legal/{base_name}.html"

@router.get("/", response_class=HTMLResponse)
def legal_menu(request: Request):
    # This route is protected by AuthMiddleware usually if it were under /dashboard,
    # but since it's now /legal, we might want to check for user presence if we want to enforce login for the menu.
    # However, the requirement was to make the hierarchy flattened.
    # Assuming AuthMiddleware doesn't block /legal automatically unless configured.
    # But usually dashboard menu items imply logged in state.
    # If the user is not logged in, they might see a broken page if it extends dashboard_layout.
    # Let's check if request.state.user is present.
    user = getattr(request.state, "user", None)
    if not user:
         from fastapi.responses import RedirectResponse
         return RedirectResponse("/login", status_code=302)

    return templates.TemplateResponse("legal_menu.html", {"request": request, "user": user, "settings": settings})

@router.get("/terms", response_class=HTMLResponse)
def terms_page(request: Request):
    return templates.TemplateResponse("legal/terms.html", {"request": request, "settings": settings})

@router.get("/privacy", response_class=HTMLResponse)
def privacy_page(request: Request):
    return templates.TemplateResponse("legal/privacy.html", {"request": request, "settings": settings})



##### INICIO DO ARQUIVO: app/routes/logout.py #####
from fastapi import APIRouter, Response, Request, Depends
from fastapi.responses import RedirectResponse
from sqlalchemy.orm import Session
from app.db.session import get_db
from app.core.jwt import decode_token
from app.services.security_service import revoke_session, log_audit

router = APIRouter()

@router.get("/logout")
def logout(request: Request, response: Response, db: Session = Depends(get_db)):
    # 1. Get Token/Session
    token = request.cookies.get("access_token")
    if token:
        payload = decode_token(token)
        if payload:
            sid = payload.get("sid")
            user_id = payload.get("sub")

            if sid:
                revoke_session(db, sid)

            try:
                uid = int(user_id) if user_id else None
                log_audit(db, uid, "LOGOUT", request.client.host, f"Session {sid} revoked")
            except:
                pass

    response = RedirectResponse(url="/login", status_code=302)
    response.delete_cookie("access_token")
    response.delete_cookie("careerdev_session")
    return response



##### INICIO DO ARQUIVO: app/routes/monitoring.py #####
from fastapi import APIRouter, HTTPException, Request, Depends, status
from pydantic import BaseModel
import sentry_sdk
from app.core.config import settings
from openai import AsyncOpenAI
import logging
from sqlalchemy.orm import Session
from sqlalchemy import text
from app.db.session import get_db
import httpx

router = APIRouter()
logger = logging.getLogger(__name__)

class PostureAnalysisRequest(BaseModel):
    image: str # Base64 encoded image or Data URL

# Initialize OpenAI client
client = None
if settings.OPENAI_API_KEY:
    client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
else:
    logger.warning("OPENAI_API_KEY not found. Monitoring service will fail.")

async def check_auth(request: Request):
    if not getattr(request.state, "user", None):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Authentication required"
        )
    return request.state.user

@router.get("/diagnostics")
async def system_diagnostics(db: Session = Depends(get_db)):
    """
    Performs a deep health check of the system components.
    """
    diagnostics = {
        "database": "unknown",
        "internet": "unknown",
        "status": "warning"
    }

    # 1. Check Database
    try:
        db.execute(text("SELECT 1"))
        diagnostics["database"] = "connected"
    except Exception as e:
        diagnostics["database"] = f"error: {str(e)}"
        logger.error(f"Diagnostics DB Error: {e}")

    # 2. Check Internet Connectivity (Google Ping)
    try:
        async with httpx.AsyncClient() as client:
            resp = await client.get("https://www.google.com", timeout=2.0)
            if resp.status_code == 200:
                diagnostics["internet"] = "connected"
            else:
                diagnostics["internet"] = f"unreachable (status: {resp.status_code})"
    except Exception as e:
        diagnostics["internet"] = f"error: {str(e)}"
        logger.error(f"Diagnostics Internet Error: {e}")

    # Determine overall status
    if diagnostics["database"] == "connected" and diagnostics["internet"] == "connected":
        diagnostics["status"] = "healthy"
    else:
        diagnostics["status"] = "degraded"

    return diagnostics

@router.post("/analyze-posture")
async def analyze_posture(data: PostureAnalysisRequest, user = Depends(check_auth)):
    if not client:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="AI service unavailable"
        )

    # Ensure correct format for OpenAI
    image_url = data.image
    if not image_url.startswith("data:"):
        # Assume JPEG if no header, but safest to require Data URL from frontend
        image_url = f"data:image/jpeg;base64,{data.image}"

    try:
        response = await client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "Analyze this image. Is the user slouching or exhibiting poor ergonomic posture? Answer strictly 'YES' or 'NO'."},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": image_url
                            },
                        },
                    ],
                }
            ],
            max_tokens=10
        )
        result = response.choices[0].message.content.strip().upper()

        # Clean up any extra punctuation (e.g., "YES.")
        if "YES" in result:
            result = "YES"
        elif "NO" in result:
            result = "NO"

    except Exception as e:
        logger.error(f"OpenAI Vision Error: {e}")
        sentry_sdk.capture_exception(e)
        raise HTTPException(status_code=500, detail="Failed to analyze image")

    if result == "YES":
        # Bad posture
        sentry_sdk.capture_message(f"Poor Posture Detected for User {user.id}", level="warning")
        return {"status": "poor_posture", "message": "Posture Check: Time to straighten up!"}

    return {"status": "good_posture", "message": "Great posture! Keep it up."}



##### INICIO DO ARQUIVO: app/routes/public_api.py #####
from fastapi import APIRouter, Depends, Response, HTTPException
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session, joinedload
from app.db.session import get_db
from app.db.models.user import User
from app.routes.dashboard import get_current_user_secure
from app.services.pdf_generator import generate_career_passport

router = APIRouter()

@router.get("/api/export/passport")
def export_passport(
    db: Session = Depends(get_db),
    user: User = Depends(get_current_user_secure)
):
    if not user:
        raise HTTPException(status_code=401, detail="Authentication required")

    pdf_buffer = generate_career_passport(user, db)

    # Sanitize filename (remove quotes if any) and wrap in quotes
    safe_name = (user.name or 'User').replace('"', '').replace("'", "")
    headers = {
        "Content-Disposition": f'attachment; filename="{safe_name}_CareerPassport.pdf"'
    }

    return StreamingResponse(
        pdf_buffer,
        media_type="application/pdf",
        headers=headers
    )

@router.get("/api/badge/{user_id}")
def get_user_badge(user_id: int, db: Session = Depends(get_db)):
    # Public Endpoint - No Auth Required
    user = db.query(User).options(joinedload(User.career_profile)).filter(User.id == user_id).first()

    if not user:
        # Return a generic 404 badge or simple SVG error
        return Response(content=_generate_badge_svg(0, "User Not Found"), media_type="image/svg+xml")

    profile = user.career_profile
    score = profile.market_relevance_score if profile else 0

    svg_content = _generate_badge_svg(score)

    return Response(content=svg_content, media_type="image/svg+xml")

def _generate_badge_svg(score: int, label_override: str = None) -> str:
    # Color Logic
    color = "#ff0055" # Red
    if score >= 80:
        color = "#00ff88" # Green
    elif score >= 50:
        color = "#ff9900" # Yellow

    score_text = f"{score}/100" if not label_override else "N/A"

    # SVG Template (Shields.io style)
    # Width calculation is approximate
    width_left = 110
    width_right = 60
    total_width = width_left + width_right

    svg = f"""<svg xmlns="http://www.w3.org/2000/svg" width="{total_width}" height="20">
    <linearGradient id="b" x2="0" y2="100%">
        <stop offset="0" stop-color="#bbb" stop-opacity=".1"/>
        <stop offset="1" stop-opacity=".1"/>
    </linearGradient>
    <mask id="a">
        <rect width="{total_width}" height="20" rx="3" fill="#fff"/>
    </mask>
    <g mask="url(#a)">
        <path fill="#555" d="M0 0h{width_left}v20H0z"/>
        <path fill="{color}" d="M{width_left} 0h{width_right}v20H{width_left}z"/>
        <path fill="url(#b)" d="M0 0h{total_width}v20H0z"/>
    </g>
    <g fill="#fff" text-anchor="middle" font-family="DejaVu Sans,Verdana,Geneva,sans-serif" font-size="11">
        <text x="{width_left/2}" y="15" fill="#010101" fill-opacity=".3">CareerDev Score</text>
        <text x="{width_left/2}" y="14">CareerDev Score</text>
        <text x="{width_left + width_right/2}" y="15" fill="#010101" fill-opacity=".3">{score_text}</text>
        <text x="{width_left + width_right/2}" y="14">{score_text}</text>
    </g>
</svg>"""
    return svg



##### INICIO DO ARQUIVO: app/routes/risk.py #####
from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session

# DependÃªncias de autenticaÃ§Ã£o e banco de dados
from app.api.deps import get_db  # Ajuste conforme sua estrutura de injeÃ§Ã£o de dependÃªncia
from app.routes.dashboard import get_current_user_secure
from app.services.career_engine import career_engine

router = APIRouter()

# =========================================================
# RISK EXPLAINABILITY (XAI)
# =========================================================
@router.get("/api/risk/explain")
async def explain_risk(
    user=Depends(get_current_user_secure)
):
    """
    Retorna a explicaÃ§Ã£o textual (human-readable) dos fatores de risco.
    Alimenta o modal 'Why am I at risk?'.
    """
    return career_engine.explain_risk(user)

# =========================================================
# COUNTERFACTUAL ANALYSIS (WHAT-IF)
# =========================================================
@router.get("/api/risk/counterfactual")
async def counterfactual(
    db: Session = Depends(get_db),
    user=Depends(get_current_user_secure)
):
    """
    Gera cenÃ¡rios alternativos: 'O que aconteceria com meu risco se...'
    Retorna sugestÃµes acionÃ¡veis para reduzir o score de risco.
    """
    # Nota: Certifique-se de que o mÃ©todo 'get_counterfactual'
    # foi implementado ou exposto no CareerEngine.
    # Caso contrÃ¡rio, vocÃª pode reutilizar a lÃ³gica interna do mÃ©todo 'analyze'.
    return career_engine.get_counterfactual(db, user)



##### INICIO DO ARQUIVO: app/routes/security.py #####
from fastapi import APIRouter, Request, Depends, Form
from fastapi.responses import HTMLResponse, RedirectResponse, JSONResponse
from fastapi.templating import Jinja2Templates
from sqlalchemy.orm import Session
from sqlalchemy import desc
from datetime import datetime
import user_agents
import logging

# Imports do Banco de Dados
from app.db.session import SessionLocal
from app.db.models.user import User
from app.db.models.security import UserSession
# CORREÃ‡ÃƒO: Usando o modelo correto AuditLog
from app.db.models.audit import AuditLog

# Imports de ServiÃ§os e Utils
from app.core.jwt import decode_token
from app.services.onboarding import validate_onboarding_access
# Certifique-se que log_audit e revoke_session existem no security_service
from app.services.security_service import revoke_session, log_audit
from app.core.config import settings
from app.core.utils import get_client_ip

logger = logging.getLogger(__name__)

router = APIRouter()
templates = Jinja2Templates(directory="app/templates")

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

def get_current_user(request: Request, db: Session) -> User | None:
    # Usa o objeto user jÃ¡ validado pelo AuthMiddleware
    return getattr(request.state, "user", None)

def parse_agent(ua_string: str) -> str:
    """Parses user agent string into a readable format."""
    try:
        user_agent = user_agents.parse(ua_string)
        return str(user_agent) # Retorna "PC / Windows / Chrome" formatado
    except Exception:
        return "Unknown Device"

@router.get("/security", response_class=HTMLResponse)
def security_panel(request: Request, db: Session = Depends(get_db)):
    user = get_current_user(request, db)

    # ðŸ›¡ï¸ PROTEÃ‡ÃƒO ABSOLUTA: nunca acessar atributos se user for None
    if user is None:
        return RedirectResponse("/login", status_code=302)

    # GUARD: Ensure Onboarding is Complete
    if resp := validate_onboarding_access(user):
        return resp

    # Obter SID para destacar a sessÃ£o atual
    token = request.cookies.get("access_token")
    payload = decode_token(token) if token else {}
    current_sid = payload.get("sid")

    # Dados da SessÃ£o Atual (Request)
    current_ua_string = request.headers.get("user-agent", "")
    current_device = parse_agent(current_ua_string)
    current_ip = get_client_ip(request)

    # --- CORREÃ‡ÃƒO AQUI: Consultando AuditLog para o histÃ³rico ---
    history = db.query(AuditLog).filter(AuditLog.user_id == user.id).order_by(desc(AuditLog.login_timestamp)).limit(20).all()

    # Processar SessÃµes para ExibiÃ§Ã£o
    processed_sessions = []
    for h in history:
        is_current = (str(h.session_id) == str(current_sid))
        status = "Expired"
        if h.is_active_session:
             status = "Current" if is_current else "Active"

        processed_sessions.append({
            "id": h.session_id, # Usado para revogaÃ§Ã£o
            "device": f"{h.device_type} ({h.browser})",
            "ip_address": h.ip_address,
            "last_active": h.login_timestamp, # Usando timestamp de login
            "is_current": is_current,
            "status": status,
            "raw_ua": h.user_agent_raw,
            "os": h.os
        })

    return templates.TemplateResponse(
        "dashboard/security.html",
        {
            "request": request,
            "no_user": False,
            "user": user,
            "current_device": current_device,
            "current_ip": current_ip,
            "sessions": processed_sessions,
            "current_sid": current_sid
        }
    )

@router.post("/security/update")
def update_security(
    request: Request,
    db: Session = Depends(get_db)
):
    user = get_current_user(request, db)
    if not user:
         return RedirectResponse("/login", status_code=302)

    # GUARD: Ensure Onboarding is Complete
    if resp := validate_onboarding_access(user):
        return resp

    # Placeholder para atualizaÃ§Ãµes futuras de preferÃªncias
    return RedirectResponse("/security?success=true", status_code=302)

@router.post("/security/delete-account")
def delete_account(
    request: Request,
    db: Session = Depends(get_db)
):
    user = get_current_user(request, db)
    if not user:
        return RedirectResponse("/login", status_code=302)

    # Delete User (Cascade deletes profile/plans due to relationship config)
    db.delete(user)
    db.commit()

    # Logout e limpar cookie
    response = RedirectResponse("/login?msg=account_deleted", status_code=302)
    response.delete_cookie("access_token")
    return response

@router.get("/api/security/sessions")
def get_login_history(request: Request, db: Session = Depends(get_db)):
    user = get_current_user(request, db)
    if not user:
        return JSONResponse(status_code=401, content={"detail": "Unauthorized"})

    # Get current session ID
    token = request.cookies.get("access_token")
    payload = decode_token(token) if token else {}
    current_sid = payload.get("sid")

    # --- CORREÃ‡ÃƒO AQUI: Consultando AuditLog ---
    history = db.query(AuditLog).filter(AuditLog.user_id == user.id).order_by(desc(AuditLog.login_timestamp)).limit(20).all()

    response_data = []
    for h in history:
        status = "Expired"
        if h.is_active_session:
            status = "Current" if str(h.session_id) == str(current_sid) else "Active"

        response_data.append({
            "device": f"{h.device_type} ({h.browser})",
            "ip": h.ip_address,
            "location": "Unknown",
            "date": h.login_timestamp.strftime("%Y-%m-%d %H:%M"),
            "status": status,
            "os": h.os
        })

    return JSONResponse(content=response_data)

@router.post("/security/revoke_all")
def revoke_all_sessions(request: Request, db: Session = Depends(get_db)):
    user = get_current_user(request, db)
    if not user:
        return RedirectResponse("/login", status_code=302)

    # Get current session ID
    token = request.cookies.get("access_token")
    payload = decode_token(token) if token else {}
    current_sid = payload.get("sid")

    if not current_sid:
        return RedirectResponse("/login", status_code=302)

    # 1. Revogar todas as UserSessions exceto a atual
    db.query(UserSession).filter(
        UserSession.user_id == user.id,
        UserSession.id != current_sid
    ).update({UserSession.is_active: False}, synchronize_session=False)

    # 2. Atualizar o histÃ³rico (AuditLog) para refletir que as sessÃµes expiraram
    db.query(AuditLog).filter(
        AuditLog.user_id == user.id,
        AuditLog.session_id != current_sid
    ).update({AuditLog.is_active_session: False}, synchronize_session=False)

    db.commit()

    # Log da aÃ§Ã£o
    log_audit(db, user.id, "REVOKE_ALL_SESSIONS", get_client_ip(request), "Revoked all other sessions")

    return RedirectResponse("/security", status_code=303)

@router.post("/security/revoke/{session_id}")
def revoke_user_session_route(session_id: str, request: Request, db: Session = Depends(get_db)):
    user = get_current_user(request, db)
    if not user:
        return RedirectResponse("/login", status_code=302)

    # Verifica se a sessÃ£o pertence mesmo ao usuÃ¡rio
    session_to_revoke = db.query(UserSession).filter(UserSession.id == session_id, UserSession.user_id == user.id).first()

    if session_to_revoke:
        # Chama serviÃ§o para remover do Redis/Banco
        revoke_session(db, session_id)

        # Atualiza o AuditLog especÃ­fico
        db.query(AuditLog).filter(AuditLog.session_id == session_id).update({AuditLog.is_active_session: False})
        db.commit()

        log_audit(db, user.id, "REVOKE_SESSION", get_client_ip(request), f"Revoked session {session_id}")

    return RedirectResponse("/security", status_code=303)



##### INICIO DO ARQUIVO: app/routes/setup_hotfix.py #####
import os
from fastapi import APIRouter, Request, Response, Depends
from sqlalchemy.orm import Session
from app.db.session import get_db
from app.db.models.user import User
from app.core.jwt import decode_token
from app.services.security_service import revoke_session

router = APIRouter()

@router.get("/setup/claim-admin-rights")
def claim_admin_rights(request: Request, db: Session = Depends(get_db)):
    # 1. Check if user is logged in (AuthMiddleware populates request.state.user)
    if not getattr(request.state, "user", None):
         return Response(content="Forbidden: You must be logged in.", status_code=403)

    user_state = request.state.user

    # 2. Hardcoded Security Check
    expected_email = os.getenv("ADMIN_EMAIL", "robsonholandasilva@yahoo.com.br")

    # Case-insensitive comparison
    if user_state.email.lower() != expected_email.lower():
        return Response(content="Forbidden: Invalid Account.", status_code=403)

    # 3. Fetch User from DB (state user is detached)
    user = db.query(User).filter(User.id == user_state.id).first()
    if not user:
        return Response(content="User not found in DB.", status_code=404)

    # 4. Set Admin Rights
    user.is_admin = True
    db.commit()

    # 5. Invalidate Session
    token = request.cookies.get("access_token")
    if token:
        payload = decode_token(token)
        if payload:
            sid = payload.get("sid")
            if sid:
                revoke_session(db, sid)

    # 6. Prepare Response
    final_response = Response(
        content=f"Admin rights granted for {user.email}. Please log in again to see the dashboard.",
        media_type="text/plain"
    )

    # 7. Clear Cookies
    final_response.delete_cookie("access_token")
    final_response.delete_cookie("careerdev_session")

    return final_response



##### INICIO DO ARQUIVO: app/routes/simulation.py #####
from fastapi import APIRouter, Depends
from app.routes.dashboard import get_current_user_secure
from app.services.career_engine import career_engine

router = APIRouter()

@router.post("/api/simulate-skill-path")
async def simulate_skill_path(payload: dict, user=Depends(get_current_user_secure)):
    skill = payload.get("skill")
    months = payload.get("months", 6)

    return career_engine.simulate_skill_path(user, skill, months)



##### INICIO DO ARQUIVO: app/routes/social.py #####
from fastapi import APIRouter, Request, Depends, BackgroundTasks
from fastapi.responses import RedirectResponse, HTMLResponse
from fastapi.templating import Jinja2Templates
from authlib.integrations.starlette_client import OAuth
from authlib.integrations.base_client.errors import OAuthError
from sqlalchemy.orm import Session, joinedload
from app.core.config import settings
from app.db.session import get_db, SessionLocal
from app.services.social_harvester import social_harvester
from app.db.crud.users import (
    get_user_by_email,
    get_user_by_github_id,
    get_user_by_linkedin_id,
    create_user
)
from app.db.models.user import User
from app.core.security import hash_password
from app.core.jwt import create_access_token
from app.services.onboarding import get_next_onboarding_step
from app.services.security_service import create_user_session, log_audit
from app.services.gamification import check_and_award_security_badge
from app.core.utils import get_client_ip
from app.core.limiter import limiter
import secrets
import logging
import os
import asyncio
import json
from datetime import datetime
from sqlalchemy.exc import IntegrityError
import user_agents

logger = logging.getLogger(__name__)

router = APIRouter()
templates = Jinja2Templates(directory="app/templates")
oauth = OAuth()

# GitHub Config
logger.info(f"GitHub Secret Loaded? {bool(settings.GITHUB_CLIENT_SECRET)}")
if settings.GITHUB_CLIENT_ID and settings.GITHUB_CLIENT_SECRET:
    oauth.register(
        name='github',
        client_id=settings.GITHUB_CLIENT_ID,
        client_secret=settings.GITHUB_CLIENT_SECRET,
        access_token_url='https://github.com/login/oauth/access_token',
        access_token_params=None,
        authorize_url='https://github.com/login/oauth/authorize',
        authorize_params=None,
        api_base_url='https://api.github.com/',
        client_kwargs={'scope': 'user:email'},
    )

# LinkedIn Config
raw_secret = os.environ.get('LINKEDIN_CLIENT_SECRET', '')
linkedin_secret = raw_secret.strip()

if settings.LINKEDIN_CLIENT_ID:
    if not linkedin_secret:
         raise ValueError("LINKEDIN_CLIENT_SECRET is present but empty!")

    oauth.register(
        name='linkedin',
        client_id=settings.LINKEDIN_CLIENT_ID,
        # Explicitly pass client_secret to avoid Authlib implicit loading issues
        client_secret=linkedin_secret,
        server_metadata_url='https://www.linkedin.com/oauth/.well-known/openid-configuration',
        client_kwargs={
            'scope': 'openid profile email',
            'token_endpoint_auth_method': 'client_secret_post',
        }
    )

def get_current_user_onboarding(request: Request, db: Session = Depends(get_db)):
    # ðŸ›¡ï¸ Relies on AuthMiddleware for session validation
    if not getattr(request.state, "user", None):
        return None
    # Re-query to attach to current db session
    return db.query(User).filter(User.id == request.state.user.id).first()

def login_user_and_redirect(request: Request, user, db: Session, redirect_url: str = "/dashboard"):
    # Update last_login
    user.last_login = datetime.utcnow()
    # Implicitly accept terms on login (Zero Touch)
    if not user.terms_accepted:
        user.terms_accepted = True
        user.terms_accepted_at = datetime.utcnow()

    # Zero Touch: If we have LinkedIn, we are good to go, but we want to check GitHub
    # Note: is_profile_completed is now largely semantic for "has github and terms"
    if user.linkedin_id and user.github_id and user.terms_accepted:
        user.is_profile_completed = True

    try:
        db.commit()
        logger.info(f"âœ… DB COMMIT SUCCESS: User {user.id} logged in.")
    except Exception as e:
        logger.error(f"âŒ DB COMMIT FAILED: {e}")
        raise e

    # Security: Create Session
    ip = get_client_ip(request)
    user_agent = request.headers.get("user-agent", "unknown")
    logger.info(f"Creating session for user {user.id} | IP: {ip} | UA: {user_agent[:30]}...")
    sid = create_user_session(db, user.id, ip, user_agent)

    # Security: Forensics & Audit
    device_type = "Unknown"
    browser_info = "Unknown"
    os_info = "Unknown"

    try:
        ua_parsed = user_agents.parse(user_agent)
        device_type = "Mobile" if ua_parsed.is_mobile else "Tablet" if ua_parsed.is_tablet else "Desktop"
        browser_info = f"{ua_parsed.browser.family} {ua_parsed.browser.version_string}"
        os_info = f"{ua_parsed.os.family} {ua_parsed.os.version_string}"
    except Exception as e:
        logger.error(f"Failed to parse User Agent: {e}")

    log_audit(
        db=db,
        user_id=user.id,
        action="LOGIN",
        ip_address=ip,
        details={
            "method": "social",
            "session_id": sid,
            "device_info": {
                "user_agent": user_agent,
                "device": device_type,
                "os": os_info,
                "browser": browser_info
            }
        },
        device_type=device_type,
        browser=browser_info,
        os=os_info,
        user_agent_raw=user_agent
    )

    token = create_access_token({
        "sub": str(user.id),
        "email": user.email,
        "sid": sid
    })

    # STRICT ONBOARDING ENFORCEMENT
    # If user has no GitHub ID, they MUST go to onboarding, regardless of request
    if not user.github_id and redirect_url != "/onboarding/connect-github":
        logger.warning(f"Strict Onboarding Enforcement: Redirecting User {user.id} to GitHub Connect")
        redirect_url = "/onboarding/connect-github"

    # Force Dashboard Redirect (Onboarding Removed)
    response = RedirectResponse(redirect_url, status_code=303)
    response.set_cookie(
        key="access_token",
        value=token,
        httponly=True,
        secure=(request.url.scheme == "https"),
        samesite="lax",
        max_age=604800
    )
    return response

def get_consistent_redirect_uri(request: Request, endpoint: str) -> str:
    """
    Generates a consistent Redirect URI strictly matching the registered DOMAIN.
    This bypasses any proxy/host header issues by constructing the URL manually.
    """
    # Get the path for the endpoint (e.g., /auth/linkedin/callback)
    path = request.app.url_path_for(endpoint)

    # Ensure we use the configured DOMAIN exactly as registered
    domain = settings.DOMAIN.rstrip('/')

    return f"{domain}{path}"

@router.get("/onboarding/connect-github", response_class=HTMLResponse)
async def connect_github(request: Request, user: User = Depends(get_current_user_onboarding)):
    if not user:
        return RedirectResponse("/login")

    # STRICT SEQUENTIAL FLOW
    if user.github_id:
        return RedirectResponse("/dashboard", status_code=303)

    return templates.TemplateResponse("onboarding_github.html", {"request": request, "user": user})


@router.get("/login/github")
@limiter.limit("5/minute")
async def login_github(request: Request):
    # STRICT AUTH CHECK: GitHub is secondary step ONLY
    if not getattr(request.state, "user", None):
        return RedirectResponse("/login?error=linkedin_required_first")

    if not settings.GITHUB_CLIENT_ID:
        return RedirectResponse("/login?error=github_not_configured")

    # Generate redirect_uri and FORCE HTTPS
    redirect_uri = get_consistent_redirect_uri(request, 'auth_github_callback')

    logger.info(f"ðŸ”Ž GITHUB LOGIN START: Generated Redirect URI: {redirect_uri}")

    return await oauth.github.authorize_redirect(request, redirect_uri)

@router.get("/auth/github/callback")
async def auth_github_callback(request: Request, background_tasks: BackgroundTasks, db: Session = Depends(get_db)):
    ip = get_client_ip(request)

    code = request.query_params.get('code')
    state = request.query_params.get('state')
    error = request.query_params.get('error')
    code_log = code[:5] if code else "None"
    logger.info(f"ðŸ“¥ GITHUB CALLBACK RECEIVED: Code={code_log}... | State={state} | Error={error}")

    try:
        current_user_state = getattr(request.state, "user", None)
        if not current_user_state:
            logger.warning("GitHub Callback attempted without session")
            return RedirectResponse("/login?error=session_expired_github")

        # 1. Manual HTTPS Enforcement
        redirect_uri = get_consistent_redirect_uri(request, 'auth_github_callback')

        logger.info(f"ðŸ”„ GITHUB TOKEN EXCHANGE: URI={redirect_uri}")

        # 2. SECURITY REFACTOR: Use fetch_access_token
        try:
            token = await oauth.github.fetch_access_token(
                redirect_uri=redirect_uri,
                code=str(request.query_params.get('code')),
                grant_type='authorization_code'
            )
        except OAuthError as e:
            logger.warning(f"âš ï¸ GITHUB OAUTH ERROR: {e.error} | {e.description}")
            return RedirectResponse("/login?error=github_code_expired")

        logger.info(f"ðŸ”‘ GITHUB TOKEN RECEIVED: {token.get('access_token')[:5]}... | Scope: {token.get('scope')}")

        resp = await oauth.github.get('user', token=token)
        profile = resp.json()

        logger.info(f"ðŸ‘¤ GITHUB PROFILE: ID={profile.get('id')} | Email={profile.get('email')}")

        email = profile.get('email')
        if not email:
            logger.warning("âš ï¸ Email null in profile. Fetching /user/emails...")
            resp_emails = await oauth.github.get('user/emails', token=token)
            emails = resp_emails.json()
            for e in emails:
                if e.get('primary') and e.get('verified'):
                    email = e['email']
                    break
            logger.info(f"ðŸ“§ EMAIL FETCHED: {email}")

        if not email:
             await asyncio.to_thread(
                 log_audit,
                 db=db,
                 user_id=None,
                 action="SOCIAL_ERROR",
                 ip_address=ip,
                 details="GitHub: No email found"
             )
             return RedirectResponse("/login?error=github_no_email")

        github_id = str(profile.get('id'))
        name = profile.get('name') or profile.get('login')
        avatar = profile.get('avatar_url')

        # --- STRICT LINKING LOGIC ---
        # PERFORMANCE OPTIMIZATION: Offload sync DB ops to thread
        result = await asyncio.to_thread(
            _process_github_connect_sync,
            db,
            current_user_state.id,
            github_id,
            token.get('access_token'),
            avatar,
            ip
        )

        if result == "user_not_found":
             return RedirectResponse("/login?error=user_not_found")

        if result == "github_taken":
            return RedirectResponse("/onboarding/connect-github?error=github_taken", status_code=302)

        if token.get('access_token'):
            background_tasks.add_task(social_harvester.harvest_github_data, current_user_state.id, token.get('access_token'))

        return RedirectResponse("/dashboard", status_code=303)

    except Exception as e:
        logger.error(f"ðŸ”¥ GITHUB ERROR: {str(e)}", exc_info=True)
        await asyncio.to_thread(
            log_audit,
            db=db,
            user_id=None,
            action="SOCIAL_ERROR",
            ip_address=ip,
            details=f"GitHub Exception: {e}"
        )
        return RedirectResponse("/login?error=github_failed")

@router.get("/login/linkedin")
@limiter.limit("5/minute")
async def login_linkedin(request: Request):
    if not settings.LINKEDIN_CLIENT_ID:
        return RedirectResponse("/login?error=linkedin_not_configured")

    if settings.LINKEDIN_REDIRECT_URI:
        redirect_uri = settings.LINKEDIN_REDIRECT_URI
    else:
        redirect_uri = get_consistent_redirect_uri(request, 'auth_linkedin_callback')

    logger.info(f"ðŸ”Ž LINKEDIN LOGIN START: Generated Redirect URI: {redirect_uri}")

    return await oauth.linkedin.authorize_redirect(request, redirect_uri)

def _process_github_connect_sync(db: Session, user_id: int, github_id: str, token_str: str, avatar: str, ip: str) -> str:
    """
    Synchronous helper to handle GitHub connection logic.
    Offloaded to a thread to prevent blocking the event loop.
    """
    current_user = db.query(User).filter(User.id == user_id).first()
    if not current_user:
         return "user_not_found"

    # Check for conflict
    existing_user = get_user_by_github_id(db, github_id)
    if existing_user and existing_user.id != current_user.id:
        log_audit(
            db=db,
            user_id=current_user.id,
            action="CONNECT_SOCIAL_FAIL",
            ip_address=ip,
            details="GitHub: Account already linked to another user"
        )
        return "github_taken"

    # Update User
    current_user.github_id = github_id
    current_user.github_token = token_str
    if not current_user.avatar_url:
        current_user.avatar_url = avatar

    # ZERO TOUCH: Automatically complete profile
    current_user.is_profile_completed = True
    current_user.terms_accepted = True
    current_user.terms_accepted_at = datetime.utcnow()

    db.commit()
    logger.info(f"âœ… GITHUB LINKED: User {current_user.id} updated.")

    log_audit(
        db=db,
        user_id=current_user.id,
        action="CONNECT_SOCIAL",
        ip_address=ip,
        details="GitHub Connected"
    )

    check_and_award_security_badge(db, current_user)

    return "success"

def _process_linkedin_login_sync(user_info: dict, token_data: dict, ip: str, user_agent: str, production_env: bool) -> dict:
    """
    Handles the entire LinkedIn login/signup transactional flow in a separate thread.
    Uses its own Session to ensure thread safety and non-blocking I/O on main loop.
    """
    token_str = token_data.get('access_token')

    linkedin_id = user_info.get('sub') or user_info.get('id')
    email = user_info.get('email')
    picture = user_info.get('picture')

    name = user_info.get('name')
    if not name:
        first = user_info.get('given_name')
        last = user_info.get('family_name')
        if first and last:
            name = f"{first} {last}"
        else:
            name = "LinkedIn User"

    if not linkedin_id:
        return {"status": "error", "error": "linkedin_no_id", "message": "No ID found in user_info"}

    if not email:
        return {"status": "error", "error": "linkedin_no_email", "message": "No email found in user_info"}

    with SessionLocal() as db:
        try:
            # 1. Find User (Email or ID)
            user = db.query(User).options(joinedload(User.career_profile)).filter(User.email == email).first()

            if not user:
                user = db.query(User).options(joinedload(User.career_profile)).filter(User.linkedin_id == linkedin_id).first()

            if user:
                # UPDATE Existing
                logger.info(f"DEBUG: Found existing user: {user.id}")
                if user.linkedin_id != linkedin_id:
                    user.linkedin_id = linkedin_id
                    if not user.avatar_url:
                        user.avatar_url = picture

                user.linkedin_token = token_str
                db.commit()
                logger.info(f"âœ… USER UPDATED: {user.id} LinkedIn ID Linked.")

                if user.linkedin_id == linkedin_id:
                    check_and_award_security_badge(db, user)

            else:
                # CREATE New
                logger.info("DEBUG: Creating new user")
                pwd = secrets.token_urlsafe(32)
                hashed_password = hash_password(pwd) # CPU bound, fine here

                try:
                    user = create_user(
                        db=db,
                        name=name,
                        email=email,
                        hashed_password=hashed_password,
                        linkedin_id=linkedin_id,
                        avatar_url=picture,
                        email_verified=True
                    )

                    # ZERO TOUCH
                    user.terms_accepted = True
                    user.terms_accepted_at = datetime.utcnow()
                    user.linkedin_token = token_str
                    db.commit()

                    # Refresh with profile
                    user = db.query(User).options(joinedload(User.career_profile)).filter(User.id == user.id).first()
                    logger.info(f"âœ… NEW USER CREATED: {user.id} ({email})")

                except IntegrityError:
                    db.rollback()
                    logger.warning(f"LinkedIn Race Condition: User {email} already exists. Fetching...")
                    user = db.query(User).options(joinedload(User.career_profile)).filter(User.email == email).first()
                    if not user:
                        user = db.query(User).options(joinedload(User.career_profile)).filter(User.linkedin_id == linkedin_id).first()

                    if not user:
                        return {"status": "error", "error": "integrity_error", "message": "IntegrityError but user not found"}

                    if not user.linkedin_id:
                        user.linkedin_id = linkedin_id

                    user.linkedin_token = token_str
                    db.commit()
                    check_and_award_security_badge(db, user)

            # 2. LOGIN LOGIC (From login_user_and_redirect)
            user.last_login = datetime.utcnow()
            if not user.terms_accepted:
                user.terms_accepted = True
                user.terms_accepted_at = datetime.utcnow()

            if user.linkedin_id and user.github_id and user.terms_accepted:
                user.is_profile_completed = True

            db.commit() # Save login stats

            # Session & Audit
            sid = create_user_session(db, user.id, ip, user_agent)

            # Security: Forensics & Audit
            device_type = "Unknown"
            browser_info = "Unknown"
            os_info = "Unknown"

            try:
                ua_parsed = user_agents.parse(user_agent)
                device_type = "Mobile" if ua_parsed.is_mobile else "Tablet" if ua_parsed.is_tablet else "Desktop"
                browser_info = f"{ua_parsed.browser.family} {ua_parsed.browser.version_string}"
                os_info = f"{ua_parsed.os.family} {ua_parsed.os.version_string}"
            except Exception as e:
                logger.error(f"Failed to parse User Agent: {e}")

            log_audit(
                db=db,
                user_id=user.id,
                action="LOGIN",
                ip_address=ip,
                details={
                    "method": "social",
                    "provider": "linkedin",
                    "session_id": sid,
                    "device_info": {
                        "user_agent": user_agent,
                        "device": device_type,
                        "os": os_info,
                        "browser": browser_info
                    }
                },
                device_type=device_type,
                browser=browser_info,
                os=os_info,
                user_agent_raw=user_agent
            )

            # Generate Token
            token = create_access_token({
                "sub": str(user.id),
                "email": user.email,
                "sid": sid
            })

            # Determine Redirect
            redirect_url = "/dashboard"
            if not user.github_id:
                redirect_url = "/onboarding/connect-github"

            return {
                "status": "success",
                "user_id": user.id,
                "token": token,
                "redirect_url": redirect_url
            }

        except Exception as e:
            logger.error(f"ðŸ”¥ Sync Processing Error: {e}", exc_info=True)
            return {"status": "error", "error": "internal_error", "message": str(e)}

@router.get("/auth/linkedin/callback")
async def auth_linkedin_callback(request: Request, background_tasks: BackgroundTasks, db: Session = Depends(get_db)):
    # Note: 'db' dependency is not used for the heavy lifting anymore, only for lightweight or legacy parts if needed.
    # The sync helper creates its own session.

    ip = get_client_ip(request)
    user_agent = request.headers.get("user-agent", "unknown")

    code = request.query_params.get('code')
    state = request.query_params.get('state')
    error = request.query_params.get('error')
    code_log = code[:5] if code else "None"
    logger.info(f"ðŸ“¥ LINKEDIN CALLBACK RECEIVED: Code={code_log}... | State={state} | Error={error}")

    try:
        # 1. Manual HTTPS Enforcement
        if settings.LINKEDIN_REDIRECT_URI:
            redirect_uri = settings.LINKEDIN_REDIRECT_URI
        else:
            redirect_uri = get_consistent_redirect_uri(request, 'auth_linkedin_callback')

        logger.info(f"ðŸ”„ LINKEDIN TOKEN EXCHANGE: URI={redirect_uri}")

        # 2. Exchange Token
        token_data = await oauth.linkedin.fetch_access_token(
            redirect_uri=redirect_uri,
            code=str(request.query_params.get('code')),
            grant_type='authorization_code'
        )
        token_str = token_data.get('access_token')
        logger.info(f"ðŸ”‘ LINKEDIN TOKEN RECEIVED: {token_str[:5]}...")

        user_info = await oauth.linkedin.userinfo(token=token_data)
        logger.info(f"ðŸ‘¤ LINKEDIN USER INFO: {json.dumps(user_info, default=str)}")

        if not user_info:
             logger.error("LinkedIn Error: No user info received")
             # We can use the passed 'db' for this quick audit log
             await asyncio.to_thread(
                 log_audit,
                 db=db,
                 user_id=None,
                 action="SOCIAL_ERROR",
                 ip_address=ip,
                 details="LinkedIn: No user info received"
             )
             return RedirectResponse("/login?error=linkedin_failed")

        # 3. Offload Blocking DB Operations to Thread
        result = await asyncio.to_thread(
            _process_linkedin_login_sync,
            user_info,
            token_data,
            ip,
            user_agent,
            (settings.ENVIRONMENT == "production")
        )

        if result["status"] == "error":
            err_code = result.get("error", "unknown")
            msg = result.get("message", "")
            log_audit(
                db=db,
                user_id=None,
                action="SOCIAL_ERROR",
                ip_address=ip,
                details=f"LinkedIn Process Error: {msg}"
            )

            if err_code == "linkedin_no_id" or err_code == "linkedin_no_email":
                 return RedirectResponse("/login?error=linkedin_failed")

            return RedirectResponse(f"/login?error={err_code}")

        # 4. Trigger Background Data Harvest
        if token_str:
            background_tasks.add_task(social_harvester.harvest_linkedin_data, result["user_id"], token_str)

        # 5. Create Response
        response = RedirectResponse(result["redirect_url"], status_code=303)
        response.set_cookie(
            key="access_token",
            value=result["token"],
            httponly=True,
            secure=(request.url.scheme == "https"),
            samesite="lax",
            max_age=604800
        )
        return response

    except Exception as e:
        logger.error(f"ðŸ”¥ LINKEDIN ERROR: {str(e)}", exc_info=True)
        # Safe fallback audit
        try:
             await asyncio.to_thread(
                 log_audit,
                 db=db,
                 user_id=None,
                 action="SOCIAL_ERROR",
                 ip_address=ip,
                 details=f"LinkedIn Exception: {e}"
             )
        except:
             pass
        return RedirectResponse("/login?error=linkedin_failed")



##### INICIO DO ARQUIVO: app/schemas/user.py #####
from typing import Optional
from datetime import datetime
from pydantic import BaseModel, EmailStr, Field

# --- Schemas de Entrada (Requests) ---
# Usado quando o usuÃ¡rio estÃ¡ se cadastrando
class UserCreate(BaseModel):
    name: str = Field(min_length=2, max_length=60)
    email: EmailStr
    password: str = Field(min_length=8)

# Usado apenas para login
class UserLogin(BaseModel):
    email: EmailStr
    password: str

# --- Schemas de AtualizaÃ§Ã£o (Requests) ---
# Usado para atualizar dados do usuÃ¡rio (opcional, mas recomendado)
class UserUpdate(BaseModel):
    full_name: Optional[str] = None
    email: Optional[EmailStr] = None
    password: Optional[str] = None
    avatar_url: Optional[str] = None
    # Permitir atualizar o check semanal via API se necessÃ¡rio
    last_weekly_check: Optional[datetime] = None

# --- Schemas de SaÃ­da (Responses) ---
# Usado para devolver os dados do usuÃ¡rio para o front-end
class UserResponse(BaseModel):
    id: int
    email: EmailStr
    # Mapeando 'full_name' do banco. O 'name' do UserCreate geralmente vira 'full_name' no banco
    full_name: Optional[str] = None

    is_active: bool = True
    is_admin: bool = False
    is_profile_completed: bool = False

    avatar_url: Optional[str] = None
    created_at: Optional[datetime] = None
    last_login: Optional[datetime] = None

    # O CAMPO NOVO: Essencial para o front-end saber a data!
    last_weekly_check: Optional[datetime] = None

    # Esta configuraÃ§Ã£o permite que o Pydantic leia direto do objeto do Banco de Dados
    class Config:
        from_attributes = True # Para Pydantic V2
        # orm_mode = True      # Se estiver usando Pydantic V1 (antigo), descomente esta linha e comente a de cima



##### INICIO DO ARQUIVO: app/services/__init__.py #####



##### INICIO DO ARQUIVO: app/services/alert_engine.py #####
from app.db.models.analytics import RiskSnapshot
from app.services.mentor_engine import mentor_engine

class AlertEngine:

    def detect_state_change(self, db, user, new_level: str):
        last = (
            db.query(RiskSnapshot)
            .filter(RiskSnapshot.user_id == user.id)
            .order_by(RiskSnapshot.recorded_at.desc())
            .first()
        )

        if last and last.risk_level != new_level:
            mentor_engine.store(
                db,
                user,
                "ALERT",
                f"Career risk changed from {last.risk_level} to {new_level}."
            )
            return True

        return False

alert_engine = AlertEngine()



##### INICIO DO ARQUIVO: app/services/benchmark_engine.py #####
from sqlalchemy.orm import Session
from app.db.models.analytics import RiskSnapshot
from app.db.models.career import CareerProfile

class BenchmarkEngine:

    def compute(self, db: Session, user):
        # 1. Recupera o perfil para segmentaÃ§Ã£o
        profile = user.career_profile
        if not profile:
            return None

        # 2. Recupera o snapshot mais recente do usuÃ¡rio
        latest = (
            db.query(RiskSnapshot)
            .filter(RiskSnapshot.user_id == user.id)
            .order_by(RiskSnapshot.recorded_at.desc())
            .first()
        )
        if not latest:
            return None

        # 3. Busca pares (SegmentaÃ§Ã£o: Mesma Senioridade e Stack)
        # Nota: Limitamos a 1000 para performance
        peers = (
            db.query(RiskSnapshot.risk_score)
            .join(CareerProfile, CareerProfile.user_id == RiskSnapshot.user_id)
            .filter(CareerProfile.seniority == profile.seniority)
            .filter(CareerProfile.primary_stack == profile.primary_stack)
            .order_by(RiskSnapshot.recorded_at.desc())
            .limit(1000)
            .all()
        )

        context_label = f"{profile.seniority} {profile.primary_stack} Developers"

        # 4. Fallback: Se nÃ£o houver dados de pares suficientes (< 5), usa Global
        if not peers or len(peers) < 5:
            peers = (
                db.query(RiskSnapshot.risk_score)
                .order_by(RiskSnapshot.recorded_at.desc())
                .limit(1000)
                .all()
            )
            context_label = "Global Developer Market"

        if not peers:
            return None

        # 5. CÃ¡lculo do Percentil
        # peers Ã© uma lista de tuplas (score,), por isso usamos p[0]
        scores = sorted([p[0] for p in peers])

        # Percentil: Quantos % sÃ£o 'piores' (risco maior) ou iguais ao meu?
        # Aqui invertemos a lÃ³gica tÃ­pica de 'safer':
        # Se meu risco Ã© baixo (ex: 20), e a maioria Ã© 80, estou 'safer than' muitos.
        # Risco menor = Melhor.

        # Quantas pessoas tÃªm risco MAIOR ou IGUAL ao meu? (Estou melhor que elas)
        better_than_count = sum(1 for s in scores if s >= latest.risk_score)
        percentile = int((better_than_count / len(scores)) * 100)

        return {
            "user_risk": latest.risk_score,
            "context": context_label,
            "percentile": percentile,
            "message": (
                f"Compared to {context_label}, you are safer than "
                f"{percentile}% of them."
            )
        }


benchmark_engine = BenchmarkEngine()



##### INICIO DO ARQUIVO: app/services/career_engine.py #####
import asyncio
import random
from typing import Dict, List, Optional
from sqlalchemy.orm import Session

from app.db.models.user import User
from app.db.models.career import CareerProfile, LearningPlan
from app.db.models.ml_risk_log import MLRiskLog
from app.db.models.analytics import RiskSnapshot
from app.services.mentor_engine import mentor_engine
from app.services.alert_engine import alert_engine
from app.services.benchmark_engine import benchmark_engine # <--- Nova ImportaÃ§Ã£o
from app.services.counterfactual_engine import counterfactual_engine
from app.ml.risk_forecast_model import RiskForecastModel
from app.ml.lstm_risk_production import LSTMRiskProductionModel
from app.ml.feature_store import compute_features

# ---------------------------------------------------------
# ML FORECASTERS (SINGLETONS)
# ---------------------------------------------------------
ml_forecaster = RiskForecastModel()
lstm_model = LSTMRiskProductionModel()


class CareerEngine:
    """
    Core service responsible for analyzing developer career signals
    and producing risk alerts, growth plans,
    forecasts and mentor-driven insights.
    """

    # =========================================================
    # MAIN ANALYSIS PIPELINE
    # =========================================================
    def analyze(
        self,
        db: Session,
        raw_languages: Dict[str, int],
        linkedin_input: Dict,
        metrics: Dict,
        skill_audit: Dict,
        user: User
    ) -> Dict:
        # -------------------------------
        # SKILL CONFIDENCE SCORE
        # -------------------------------
        skill_confidence: Dict[str, int] = {}
        linkedin_skills = list(linkedin_input.get("skills", {}).keys())

        for skill, bytes_count in raw_languages.items():
            score = self.calculate_verified_score(
                skill, bytes_count, linkedin_skills
            )
            skill_confidence[skill] = int(score * 100)

        # -------------------------------
        # CAREER RISK ALERTS (CURRENT)
        # -------------------------------
        career_risks: List[Dict] = []
        hidden_gems: List[Dict] = []

        # 1. Low Confidence Alert
        for skill, confidence in skill_confidence.items():
            if confidence < 40:
                career_risks.append({
                    "level": "HIGH",
                    "skill": skill,
                    "message": f"Low confidence score in {skill}. Risk of interview rejection."
                })

        # 2. Imposter Syndrome Detector (LinkedIn Expert vs GitHub Empty)
        for skill in linkedin_skills:
            # Check if skill exists in raw_languages with sufficient bytes
            # Heuristic: < 10k bytes = "No Evidence"
            bytes_count = raw_languages.get(skill, raw_languages.get(skill.title(), 0))
            if bytes_count < 10_000:
                career_risks.append({
                    "level": "CRITICAL",
                    "skill": skill,
                    "message": f"IMPOSTER ALERT: You claim '{skill}' on LinkedIn but have <10k bytes of code."
                })

        # 3. Hidden Gem Detector (GitHub High vs LinkedIn Empty)
        for skill, bytes_count in raw_languages.items():
            if bytes_count > 50_000 and skill not in linkedin_skills:
                hidden_gems.append({
                    "type": "HIDDEN_GEM",
                    "skill": skill,
                    "message": f"You have {int(bytes_count/1000)}k bytes of {skill} code not listed on LinkedIn!"
                })

        if metrics.get("commits_last_30_days", 0) < 5:
            career_risks.append({
                "level": "MEDIUM",
                "message": "Low coding activity detected. Skills may decay."
            })

        # -------------------------------
        # WEEKLY GROWTH PLAN
        # -------------------------------
        weekly_plan = self._generate_weekly_routine(
            github_stats=metrics,
            user_streak=user.streak_count or 0
        )

        # -------------------------------
        # ACCELERATOR MODE DECISION
        # -------------------------------
        if self.should_enable_accelerator(
            skill_confidence, career_risks, user.streak_count or 0
        ):
            weekly_plan["mode"] = "ACCELERATOR"

        # -------------------------------
        # CAREER RISK FORECAST (HYBRID + LSTM + A/B TEST)
        # -------------------------------
        career_forecast = self.forecast_career_risk(
            db, user, skill_confidence, metrics
        )

        # -------------------------------
        # MENTOR PROACTIVE INSIGHTS
        # -------------------------------
        mentor_engine.proactive_insights(
            db,
            user,
            {
                "career_forecast": career_forecast,
                "weekly_plan": weekly_plan
            }
        )

        # -------------------------------
        # BENCHMARK ENGINE
        # -------------------------------
        # Calcula a performance relativa do usuÃ¡rio vs. mercado
        benchmark = benchmark_engine.compute(db, user)

        # -------------------------------
        # COUNTERFACTUAL ANALYSIS (WHAT-IF SCENARIOS)
        # -------------------------------
        # Recupera snapshots recentes para compor o histÃ³rico de features
        recent_snapshots = (
            db.query(RiskSnapshot)
            .filter(RiskSnapshot.user_id == user.id)
            .order_by(RiskSnapshot.recorded_at.desc())
            .limit(5)
            .all()
        )

        # Computa features normalizadas para o modelo ML
        features = compute_features(metrics, recent_snapshots)

        # Gera cenÃ¡rio contrafactual (ex: "Se vocÃª aumentar commits em 20%, o risco cai para X")
        counterfactual = counterfactual_engine.generate(
            features=features,
            current_risk=career_forecast["risk_score"]
        )

        # -------------------------------
        # FINAL RESPONSE
        # -------------------------------
        return {
            "zone_a_holistic": {},
            "zone_b_matrix": skill_audit,
            "weekly_plan": weekly_plan,
            "skill_confidence": skill_confidence,
            "career_risks": career_risks,
            "hidden_gems": hidden_gems,
            "career_forecast": career_forecast,
            "benchmark": benchmark, # <--- Adicionado ao retorno
            "counterfactual": counterfactual,
            "zone_a_radar": {},
            "missing_skills": []
        }

    # =========================================================
    # INDEPENDENT COUNTERFACTUAL ANALYSIS
    # =========================================================
    def get_counterfactual(self, db: Session, user: User) -> Dict:
        """
        Gera uma anÃ¡lise contrafactual sob demanda (API isolada).
        """
        # Recupera snapshots recentes para features
        recent_snapshots = (
            db.query(RiskSnapshot)
            .filter(RiskSnapshot.user_id == user.id)
            .order_by(RiskSnapshot.recorded_at.desc())
            .limit(5)
            .all()
        )

        # Nota: Em um cenÃ¡rio real, vocÃª precisaria recuperar as mÃ©tricas atuais aqui
        # (ex: chamando o serviÃ§o do GitHub ou cache) para usar compute_features.
        # metrics = github_service.get_metrics(user)
        # features = compute_features(metrics, recent_snapshots)

        # Retorna um placeholder simulado se nÃ£o houver dados completos no momento da chamada
        return {
            "scenario": "Increase commit velocity by 15%",
            "predicted_risk_reduction": 12,
            "summary": "Increasing your commit frequency would stabilize your profile against market trends.",
            "actions": [
                "Commit code at least 4 times a week",
                "Complete one Zone A project"
            ]
        }

    # =========================================================
    # WEEKLY ROUTINE GENERATOR
    # =========================================================
    def _generate_weekly_routine(
        self,
        github_stats: Dict,
        user_streak: int
    ) -> Dict:
        raw_langs = github_stats.get("languages", {})
        python_score = raw_langs.get("Python", 0)
        rust_score = raw_langs.get("Rust", 0)

        focus = "Rust" if python_score > 100_000 and rust_score < 5_000 else "Python"

        return {
            "mode": "GROWTH",
            "focus": focus,
            "streak_bonus": user_streak >= 4,
            "tasks": [
                {
                    "day": "Mon",
                    "task": f"Learn: {focus} fundamentals",
                    "type": "Learn"
                },
                {
                    "day": "Wed",
                    "task": f"Build a CLI tool in {focus}",
                    "type": "Code",
                    "action": "VERIFY_REPO",
                    "verify_keyword": focus.lower()
                }
            ]
        }

    # =========================================================
    # ACCELERATOR DECISION ENGINE
    # =========================================================
    def should_enable_accelerator(
        self,
        skill_confidence: Dict[str, int],
        career_risks: List[Dict],
        streak: int
    ) -> bool:
        avg = sum(skill_confidence.values()) / max(len(skill_confidence), 1)
        return avg >= 80 and streak >= 4 and not any(
            r["level"] == "HIGH" for r in career_risks
        )

    # =========================================================
    # VERIFIED SCORE CALCULATION
    # =========================================================
    def calculate_verified_score(
        self,
        skill: str,
        bytes_count: int,
        linkedin_skills: List[str]
    ) -> float:
        base = min(bytes_count / 100_000, 1.0)
        bonus = 0.2 if skill in linkedin_skills else 0.0
        return min(base + bonus, 1.0)

    # =========================================================
    # RISK CLASSIFICATION HELPER
    # =========================================================
    def classify_risk_level(self, risk_score: int) -> str:
        """
        Maps a numeric risk score to a categorical level.
        Thresholds:
        - < 25: LOW
        - 25 to 59: MEDIUM
        - >= 60: HIGH
        """
        if risk_score < 25:
            return "LOW"
        if risk_score < 60:
            return "MEDIUM"
        return "HIGH"

    # =========================================================
    # CAREER RISK FORECAST (HYBRID: RULES + ML + LSTM + LOGGING)
    # =========================================================
    def forecast_career_risk(
        self,
        db: Session,
        user: User,
        skill_confidence: Dict[str, int],
        metrics: Dict
    ) -> Dict:
        risk_score = 0
        reasons: List[str] = []

        avg_conf = sum(skill_confidence.values()) / max(len(skill_confidence), 1)

        # --- LÃ³gica Baseada em Regras ---
        if avg_conf < 60:
            risk_score += 30
            reasons.append("Overall skill confidence trending low.")

        commits_30d = metrics.get("commits_last_30_days", 0)
        if commits_30d < 10:
            risk_score += 30
            reasons.append("Low coding activity detected.")

        if metrics.get("velocity_score") == "Low":
            risk_score += 20
            reasons.append("Development velocity decreasing.")

        # Armazena o risco base (rule-based)
        rule_risk = risk_score

        # --- Ajuste via ML, LSTM e A/B Testing ---
        try:
            # 1. PrediÃ§Ã£o EstÃ¡tica ML
            ml_result = ml_forecaster.predict(avg_conf, commits_30d)
            ml_risk = ml_result["ml_risk"]

            # Adiciona explicaÃ§Ã£o do ML estÃ¡tico
            ml_explanation = self.explain_ml_forecast(
                ml_risk,
                {"commits": commits_30d, "confidence": avg_conf}
            )
            reasons.append(ml_explanation)

            # 2. LÃ³gica A/B Testing (Regras vs HÃ­brido EstÃ¡tico)
            experiment_group = "A" if random.random() < 0.5 else "B"

            if experiment_group == "A":
                final_risk = rule_risk
            else:
                final_risk = int((rule_risk + ml_risk) / 2)

            # 3. Refinamento Temporal via LSTM (Se houver histÃ³rico)
            try:
                recent_risks = [r.risk_score for r in db.query(RiskSnapshot)
                                .filter(RiskSnapshot.user_id == user.id)
                                .order_by(RiskSnapshot.recorded_at.desc())
                                .limit(10)
                                .all()][::-1]

                if len(recent_risks) == 10:
                    lstm_risk = lstm_model.predict(recent_risks)
                    # MÃ©dia ponderada com o LSTM para suavizar a tendÃªncia
                    final_risk = int((final_risk + lstm_risk) / 2)
                    reasons.append(f"LSTM Temporal Analysis added context (Trend: {lstm_risk}%)")
            except Exception as lstm_err:
                pass

            # PersistÃªncia do Log Completo
            new_log = MLRiskLog(
                user_id=user.id,
                ml_risk=ml_risk,
                rule_risk=rule_risk,
                final_risk=final_risk,
                experiment_group=experiment_group,
                model_version=ml_result.get("model_version", "v1.0")
            )
            db.add(new_log)
            db.commit()

            # Atualiza o score final retornado
            risk_score = final_risk

        except Exception as e:
            db.rollback()
            pass

        # --- ClassificaÃ§Ã£o Final ---
        level = self.classify_risk_level(risk_score)

        # --- DetecÃ§Ã£o de MudanÃ§a de Estado (Alert Engine) ---
        # Dispara alertas se o risco mudar significativamente (ex: LOW -> HIGH)
        alert_engine.detect_state_change(db, user, level)

        summary = "Career trajectory stable."
        if level == "HIGH":
            summary = "High probability of stagnation or rejection within 6 months."
        elif level == "MEDIUM":
            summary = "Moderate career risk detected within next 6 months."

        return {
            "risk_level": level,
            "risk_score": risk_score,
            "summary": summary,
            "reasons": reasons,
            "rule_risk": rule_risk,
            "ml_risk": ml_result.get("ml_risk", 0) if 'ml_result' in locals() else 0
        }

    # =========================================================
    # RISK EXPLAINABILITY (XAI) - STATIC
    # =========================================================
    def explain_risk(self, user: User) -> Dict:
        """
        Provides a human-readable explanation of risk factors.
        """
        return {
            "summary": "Your risk is driven by low Rust exposure and declining commit velocity.",
            "factors": [
                {"factor": "Skill Gap", "impact": "High"},
                {"factor": "Commit Velocity", "impact": "Medium"},
                {"factor": "Market Demand", "impact": "High"}
            ]
        }

    # =========================================================
    # RISK EXPLAINABILITY (XAI) - DYNAMIC ML
    # =========================================================
    def explain_ml_forecast(self, ml_risk, features):
        """
        Generates a dynamic explanation for the specific ML forecast.
        """
        return (
            f"The ML model predicts risk based on declining commit velocity "
            f"and skill confidence trends. Estimated risk: {ml_risk}%."
        )

    # =========================================================
    # SKILL PATH SIMULATION (UNIFIED)
    # =========================================================
    def simulate_skill_path(
        self,
        user: User,
        skill: str,
        months: int = 6
    ) -> Dict:
        """
        Simulates expected skill growth over time.
        """
        base_confidence = 40
        growth = min(90, base_confidence + months * 7)

        market_trends = getattr(self, "market_trends", [])

        return {
            "skill": skill,
            "months": months,
            "expected_confidence": growth,
            "market_alignment": (
                "High" if skill in market_trends else "Medium"
            ),
            "summary": (
                f"Learning {skill} for {months} months significantly improves career outlook."
            )
        }

    # =========================================================
    # WEEKLY HISTORY (ASYNC / DB-DRIVEN)
    # =========================================================
    def _get_weekly_history_sync(
        self,
        db: Session,
        user_id: int
    ) -> List[Dict]:
        routines = (
            db.query(LearningPlan)
            .filter(LearningPlan.user_id == user_id)
            .order_by(LearningPlan.created_at.desc())
            .limit(12)
            .all()
        )

        return [
            {
                "week": r.week_id,
                "focus": r.focus,
                "completion": r.completion_rate,
                "mode": r.mode
            }
            for r in routines
        ]

    async def get_weekly_history(
        self,
        db: Session,
        user: User
    ) -> List[Dict]:
        """
        Asynchronously retrieves weekly learning history.
        Offloads the synchronous DB query to a thread to prevent blocking the event loop.
        """
        return await asyncio.to_thread(self._get_weekly_history_sync, db, user.id)


# ---------------------------------------------------------
# SERVICE INSTANCE
# ---------------------------------------------------------
career_engine = CareerEngine()



##### INICIO DO ARQUIVO: app/services/counterfactual_engine.py #####
class CounterfactualEngine:
    """
    Generates actionable 'what-if' explanations based on
    current features used by the risk model.
    """

    def generate(self, features: dict, current_risk: int) -> dict:
        suggestions = []
        estimated_deltas = []

        # --- Commit velocity ---
        commit_velocity = features.get("commit_velocity", 0)
        if commit_velocity < 15:
            suggestions.append(
                "Increase coding activity to at least 15 commits/month."
            )
            estimated_deltas.append(-15)

        # --- Skill confidence trend ---
        skill_slope = features.get("skill_slope", 0)
        if skill_slope <= 0:
            suggestions.append(
                "Demonstrate skill growth by completing at least one verified weekly routine."
            )
            estimated_deltas.append(-10)

        # --- Market alignment ---
        market_gap = features.get("market_gap", [])
        if market_gap:
            suggestions.append(
                f"Reduce market gap by practicing {market_gap[0]}."
            )
            estimated_deltas.append(-20)

        projected_risk = max(
            0,
            current_risk + sum(estimated_deltas)
        )

        return {
            "current_risk": current_risk,
            "projected_risk": projected_risk,
            "actions": suggestions,
            "summary": (
                f"If you follow these steps, your estimated risk "
                f"could drop from {current_risk}% to {projected_risk}%."
            )
        }


counterfactual_engine = CounterfactualEngine()



##### INICIO DO ARQUIVO: app/services/embedding_service.py #####
from typing import List
from openai import OpenAI

# ---------------------------------------------------------
# OPENAI CLIENT CONFIGURATION
# ---------------------------------------------------------
# O cliente procurarÃ¡ automaticamente por "OPENAI_API_KEY" nas variÃ¡veis de ambiente.
client = OpenAI()


def embed_text(text: str) -> List[float]:
    """
    Gera um vetor de embedding para o texto fornecido usando
    o modelo 'text-embedding-3-small' da OpenAI.
    """
    # 1. ValidaÃ§Ã£o simples para evitar chamadas de API desnecessÃ¡rias
    if not text or not text.strip():
        return []

    # 2. SanitizaÃ§Ã£o (RecomendaÃ§Ã£o OpenAI: substituir newlines por espaÃ§os)
    clean_text = text.replace("\n", " ")

    try:
        # 3. Chamada Ã  API
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=clean_text
        )

        # 4. Retorno do vetor
        return response.data[0].embedding

    except Exception as e:
        # Log de erro (em produÃ§Ã£o, use um logger adequado como logging ou sentry)
        print(f"Error generating embedding: {e}")
        # Retorna lista vazia em caso de falha para nÃ£o quebrar o fluxo
        return []



##### INICIO DO ARQUIVO: app/services/gamification.py #####
from sqlalchemy.orm import Session
from app.db.models.gamification import Badge, UserBadge
from app.db.models.user import User

BADGE_DEFINITIONS = [
    {"slug": "early-adopter", "name": "Pioneer", "desc": "One of the first users of the platform.", "icon": "ðŸš€"},
    {"slug": "polymath", "name": "Polymath", "desc": "Possesses skills in 3 or more technologies.", "icon": "ðŸ§ "},
    {"slug": "interviewer", "name": "Communicator", "desc": "Completed a technical interview simulation.", "icon": "ðŸŽ™ï¸"},
    {"slug": "planner", "name": "Strategist", "desc": "Created their first study plan.", "icon": "ðŸ—ºï¸"},
    {"slug": "guardian", "name": "Identity Guardian", "desc": "Connected GitHub and LinkedIn accounts for maximum security.", "icon": "ðŸ›¡ï¸"}
]

def init_badges(db: Session):
    """Ensures all badges exist in DB and are up-to-date."""
    # Fetch all existing badges
    existing_badges = {b.slug: b for b in db.query(Badge).all()}

    for b_def in BADGE_DEFINITIONS:
        if b_def["slug"] not in existing_badges:
            new_badge = Badge(
                slug=b_def["slug"],
                name=b_def["name"],
                description=b_def["desc"],
                icon=b_def["icon"]
            )
            db.add(new_badge)
        else:
             # Update existing if changed (Localization fix)
            badge = existing_badges[b_def["slug"]]
            if badge.name != b_def["name"] or badge.description != b_def["desc"]:
                badge.name = b_def["name"]
                badge.description = b_def["desc"]
                badge.icon = b_def["icon"]

    db.commit()

def award_badge(db: Session, user_id: int, badge_slug: str) -> bool:
    """Awards a badge to a user if they don't have it yet. Returns True if awarded."""
    badge = db.query(Badge).filter(Badge.slug == badge_slug).first()
    if not badge:
        return False

    # Check ownership
    has_badge = db.query(UserBadge).filter(
        UserBadge.user_id == user_id,
        UserBadge.badge_id == badge.id
    ).first()

    if has_badge:
        return False

    # Award
    new_user_badge = UserBadge(user_id=user_id, badge_id=badge.id)
    db.add(new_user_badge)
    db.commit()
    return True

def check_and_award_security_badge(db: Session, user: User) -> bool:
    """Checks if user has both GitHub and LinkedIn linked and awards the Guardian badge."""
    if user.github_id and user.linkedin_id:
        return award_badge(db, user.id, "guardian")
    return False



##### INICIO DO ARQUIVO: app/services/github_verifier.py #####
from datetime import datetime, timedelta
from typing import List, Dict

class GitHubCommitVerifier:
    """
    Verifica se commits recentes atendem aos critÃ©rios de uma tarefa de cÃ³digo.
    """

    EXTENSIONS = {
        "rust": ".rs",
        "python": ".py",
        "go": ".go",
        "javascript": ".js",
        "typescript": ".ts"
    }

    def verify(self, commits: List[Dict], language: str, days: int = 7) -> bool:
        """
        Retorna True se encontrar commits recentes com arquivos compatÃ­veis
        com a linguagem esperada.
        """
        if not commits or not language:
            return False

        ext = self.EXTENSIONS.get(language.lower())
        if not ext:
            return False

        cutoff_date = datetime.utcnow() - timedelta(days=days)

        for commit in commits:
            try:
                commit_date = datetime.fromisoformat(commit["date"])
            except Exception:
                continue

            if commit_date < cutoff_date:
                continue

            for file_path in commit.get("files", []):
                if file_path.endswith(ext):
                    return True

        return False


# InstÃ¢ncia reutilizÃ¡vel
github_verifier = GitHubCommitVerifier()



##### INICIO DO ARQUIVO: app/services/growth_engine.py #####
import datetime
import asyncio
from sqlalchemy.orm import Session
from app.db.models.user import User
from app.db.models.career import CareerProfile
from app.db.models.weekly_routine import WeeklyRoutine
from app.services.social_harvester import social_harvester
from app.db.session import SessionLocal
import logging

logger = logging.getLogger(__name__)

class GrowthEngine:
    """
    The Core Logic for the Weekly Career Accelerator.
    Responsibility:
    1. Gap Analysis (Reality vs Market).
    2. Weekly Plan Generation (Routine).
    3. Task Verification (Proof of Work).
    """

    def __init__(self):
        # Mock Market Trends (In real app, fetch from LinkedIn Jobs API)
        self.market_trends = {
            "Rust": "Very High",
            "Go": "High",
            "Kubernetes": "High",
            "System Design": "Critical",
            "Python": "Stable"
        }

    def generate_weekly_plan(self, db: Session, user: User) -> dict:
        """
        Runs the Gap Analysis Algorithm and generates a personalized weekly routine.
        """
        if not user.career_profile:
            return {}

        profile = user.career_profile
        metrics = profile.github_activity_metrics or {}
        raw_langs = metrics.get("raw_languages", {})
        commits_30d = metrics.get("commits_last_30_days", 0)

        # 1. Determine Focus (Gap Analysis)
        total_bytes = sum(raw_langs.values())
        python_bytes = raw_langs.get("Python", 0)
        python_share = (python_bytes / total_bytes) if total_bytes > 0 else 0

        focus_skill = "Python" # Default
        reasoning = "Standard proficiency check."

        # Rule 1: Gap Analysis (Python Dominance -> Force Rust)
        if python_share > 0.8 and "Rust" not in raw_langs:
             focus_skill = "Rust"
             reasoning = "Gap Analysis: High Python dominance (>80%) detected. Market trends indicate Rust as high-value expansion."
        elif "Go" not in raw_langs and "Kubernetes" not in raw_langs:
             focus_skill = "Go"
             reasoning = "Market demands Cloud Native skills. Go is the best entry point."
        else:
             # Sort logic
             sorted_user_skills = sorted(raw_langs.items(), key=lambda x: x[1], reverse=True)
             top_skill = sorted_user_skills[0][0] if sorted_user_skills else "General"
             if top_skill == focus_skill:
                  # If default matches top, suggest Deep Dive or Architecture
                  focus_skill = "System Design" if python_share > 0.5 else top_skill
                  reasoning = "Deepening expertise in primary stack."

        # 2. Check Velocity (Micro-Learning Constraint)
        plan_type = "Deep Dive"
        if commits_30d < 5:
            plan_type = "Micro-Learning"
            reasoning += " (Low activity detected. Adjusted to Micro-Learning: 15 min/day)."

        # 3. Check Hardcore Mode (Gamification Rule)
        # Rule: If streak >= 4 weeks, UNLOCK "HARDCORE MODE"
        is_hardcore = (user.streak_count or 0) >= 4
        if is_hardcore:
             focus_skill = "System Design"
             reasoning = "ðŸ”¥ HARDCORE MODE ACTIVE: Streak >= 4. Tutorials disabled. Ruthless Challenges only."
             plan_type = "HARDCORE"

        # 4. Generate Routine
        week_id = datetime.datetime.now().strftime("%Y-W%U")

        if plan_type == "Micro-Learning":
            routine = [
                {"id": 1, "day": "Mon", "type": "Learn", "task": f"15 min: {focus_skill} Syntax", "status": "pending"},
                {"id": 2, "day": "Wed", "type": "Code", "task": f"Snippet: Hello World in {focus_skill}", "verify_key": focus_skill.lower(), "status": "pending"},
                {"id": 3, "day": "Fri", "type": "Review", "task": "Quick Quiz", "status": "pending"}
            ]
        elif plan_type == "HARDCORE":
             routine = [
                {"id": 1, "day": "Mon", "type": "Design", "task": "System Design: Distributed Rate Limiter", "status": "pending"},
                {"id": 2, "day": "Wed", "type": "Code", "task": "Implement Token Bucket Algo", "verify_key": focus_skill.lower(), "status": "pending"},
                {"id": 3, "day": "Fri", "type": "Code", "task": "Load Test & Benchmark", "verify_key": focus_skill.lower(), "status": "pending"}
             ]
        else:
             routine = [
                {"id": 1, "day": "Mon", "type": "Learn", "task": f"Deep Dive: {focus_skill} Core Concepts", "status": "pending"},
                {"id": 2, "day": "Wed", "type": "Code", "task": f"CLI Tool: Parse JSON in {focus_skill}", "verify_key": focus_skill.lower(), "status": "pending"},
                {"id": 3, "day": "Fri", "type": "Code", "task": f"Refactor: Optimize {focus_skill} Code", "verify_key": focus_skill.lower(), "status": "pending"}
            ]

        plan = {
            "week_id": week_id,
            "focus_language": focus_skill,
            "reasoning": reasoning,
            "routine": routine,
            "mode": plan_type
        }

        # Save to DB (WeeklyRoutine)
        wr = db.query(WeeklyRoutine).filter(WeeklyRoutine.user_id == user.id, WeeklyRoutine.week_id == week_id).first()
        if not wr:
            wr = WeeklyRoutine(
                user_id=user.id,
                week_id=week_id,
                mode=plan_type,
                focus=focus_skill,
                tasks=routine
            )
            db.add(wr)
        else:
            wr.mode = plan_type
            wr.focus = focus_skill
            wr.tasks = routine # Update tasks

        # Save to Profile
        profile.active_weekly_plan = plan
        db.commit()

        return plan

    def _verify_task_sync(self, user_id: int, task_id: int) -> dict:
        """
        Synchronous logic for task verification.
        Refreshes user data, checks completion, and updates DB.
        """
        with SessionLocal() as db:
            user = db.query(User).get(user_id)
            if not user or not user.career_profile:
                return {"success": False, "message": "User not found or no profile."}

            profile = user.career_profile
            plan = profile.active_weekly_plan

            if not plan:
                return {"success": False, "message": "No active plan."}

            routine = plan.get("routine", [])
            task = next((t for t in routine if t["id"] == task_id), None)

            if not task:
                return {"success": False, "message": "Task not found."}

            if task["status"] == "completed":
                return {"success": True, "message": "Already completed."}

            # Logic: Check metrics (which were just updated by sync_profile)
            metrics = profile.github_activity_metrics or {}
            raw_langs = metrics.get("raw_languages", {})

            verify_key = task.get("verify_key")
            verified = False

            if verify_key:
                 if raw_langs.get(verify_key, raw_langs.get(verify_key.title(), 0)) > 0:
                      verified = True
            else:
                 verified = True

            if verified:
                 task["status"] = "completed"

                 now = datetime.datetime.utcnow()

                 # Streak Logic using last_weekly_check
                 last_check = user.last_weekly_check
                 is_new_week = True

                 if last_check:
                      if last_check.isocalendar()[1] == now.isocalendar()[1] and last_check.year == now.year:
                           is_new_week = False

                 if is_new_week:
                      user.streak_count = (user.streak_count or 0) + 1
                      user.last_weekly_check = now
                      plan["last_verified_at"] = now.isoformat()

                 # UPDATE WeeklyRoutine
                 wr = db.query(WeeklyRoutine).filter(WeeklyRoutine.user_id == user_id, WeeklyRoutine.week_id == plan.get("week_id")).first()
                 if wr:
                     # Update the specific task in JSON
                     current_tasks = list(wr.tasks)
                     for t in current_tasks:
                         if t["id"] == task_id:
                             t["status"] = "completed"
                     wr.tasks = current_tasks # Trigger update

                     if all(t.get("status") == "completed" for t in current_tasks):
                         wr.completed = True
                         wr.completed_at = now

                 profile.active_weekly_plan = dict(plan)
                 db.commit()

                 return {"success": True, "message": "Task Verified! Streak Updated.", "task": task}
            else:
                 return {"success": False, "message": f"No code detected for {verify_key}. Push code to GitHub and try again."}

    def _check_plan_sync(self, user_id: int, db: Session) -> bool:
        u = db.query(User).filter(User.id == user_id).first()
        if u and u.career_profile and u.career_profile.active_weekly_plan:
            return True
        return False

    async def verify_task(self, db: Session, user: User, task_id: int) -> dict:
        """
        Triggers a SocialHarvester scan and checks if the task can be marked complete.
        """
        has_plan = await asyncio.to_thread(self._check_plan_sync, user.id, db)
        if not has_plan:
            return {"success": False, "message": "No active plan."}

        # Trigger Harvest
        if user.github_token:
             # Use the async harvester
             await social_harvester.sync_profile(user.id, user.github_token)
        else:
             # Simulation / Fail
             return {"success": False, "message": "GitHub Token required for verification."}

        # Reload Profile after sync and verify (Threaded)
        return await asyncio.to_thread(self._verify_task_sync, user.id, task_id)

growth_engine = GrowthEngine()



##### INICIO DO ARQUIVO: app/services/mentor_engine.py #####
import logging
import json
from typing import List, Optional

from sqlalchemy.orm import Session

from app.db.models.mentor import MentorMemory
from app.db.models.user import User
from app.services.embedding_service import embed_text

logger = logging.getLogger(__name__)


class MentorEngine:
    """
    Motor responsÃ¡vel pela inteligÃªncia do Mentor IA.

    Responsabilidades:
    - Persistir memÃ³rias (curto e longo prazo)
    - Gerar insights proativos baseados em dados de carreira
    - Fornecer conselhos diÃ¡rios (placeholder para LLM)
    - Armazenar contexto relevante do usuÃ¡rio
    - Permitir recall semÃ¢ntico (embedding-based, sem quebrar o schema atual)
    """

    # -------------------------------------------------
    # CORE MEMORY STORAGE (COM EMBEDDING OPCIONAL)
    # -------------------------------------------------
    def store(
        self,
        db: Session,
        user: User,
        category: str,
        content: str,
        context_key: Optional[str] = None,
        with_embedding: bool = True
    ):
        """
        Persiste uma memÃ³ria do mentor.

        O embedding Ã© armazenado serializado dentro de `memory_value`
        para manter compatibilidade com o schema atual.
        """
        try:
            final_key = context_key if context_key else category

            payload = {
                "content": content,
                "category": category
            }

            if with_embedding:
                try:
                    payload["embedding"] = embed_text(content)
                except Exception as e:
                    logger.warning(f"[MentorMemory] embedding falhou: {e}")

            memory = MentorMemory(
                user_id=user.id,
                context_key=final_key,
                memory_value=json.dumps(payload)
            )

            db.add(memory)
            db.commit()

            logger.info(
                f"[MentorMemory] user={user.id} key={final_key} category={category}"
            )

        except Exception as e:
            db.rollback()
            logger.error(f"[MentorMemory] erro ao salvar memÃ³ria: {e}")

    # -------------------------------------------------
    # PROACTIVE INSIGHTS (DATA-DRIVEN)
    # -------------------------------------------------
    def proactive_insights(
        self,
        db: Session,
        user: User,
        career_data: dict
    ) -> List[str]:
        """
        Gera insights automÃ¡ticos baseados nos dados de carreira.
        """
        insights: List[str] = []

        forecast = career_data.get("career_forecast", {})
        weekly_plan = career_data.get("weekly_plan", {})

        if forecast.get("risk_level") == "HIGH":
            insights.append(
                "âš ï¸ High career risk detected. Immediate skill execution recommended."
            )

        if weekly_plan.get("mode") == "ACCELERATOR":
            insights.append(
                "ðŸš€ Accelerator Mode active. Focus on real PR delivery this week."
            )

        for msg in insights:
            self.store(
                db=db,
                user=user,
                category="PROACTIVE",
                content=msg
            )

        return insights

    # -------------------------------------------------
    # DAILY ADVICE (LLM PLACEHOLDER)
    # -------------------------------------------------
    def get_daily_advice(
        self,
        db: Session,
        user: User
    ) -> str:
        """
        Retorna um conselho simples.
        Futuro: gerar texto via LLM usando MentorMemory como contexto.
        """
        name = getattr(user, "full_name", None) or "Dev"
        advice = f"OlÃ¡ {name}, continue focado no seu progresso diÃ¡rio!"

        self.store(
            db=db,
            user=user,
            category="ADVICE",
            content=advice
        )

        return advice

    # -------------------------------------------------
    # WELCOME MESSAGE
    # -------------------------------------------------
    def welcome_message(self, db: Session, user: User):
        """
        Registers the initial welcome interaction in the mentor's memory.
        """
        self.store(
            db,
            user,
            "WELCOME",
            "Welcome! Your career analysis is ready. Iâ€™ll step in only when it matters."
        )

    # -------------------------------------------------
    # CONTEXT MEMORY (SEM EMBEDDING)
    # -------------------------------------------------
    def remember_context(
        self,
        db: Session,
        user: User,
        key: str,
        value: str
    ):
        """
        Salva contexto explÃ­cito do usuÃ¡rio (preferÃªncias, decisÃµes, eventos).
        """
        try:
            memory = MentorMemory(
                user_id=user.id,
                context_key=key,
                memory_value=json.dumps({
                    "content": value,
                    "category": "CONTEXT"
                })
            )
            db.add(memory)
            db.commit()

            logger.info(
                f"[MentorContext] user={user.id} {key}={value}"
            )

        except Exception as e:
            db.rollback()
            logger.error(f"[MentorContext] erro ao salvar contexto: {e}")

    # -------------------------------------------------
    # SEMANTIC RECALL (RAG-READY)
    # -------------------------------------------------
    def recall_semantic(
        self,
        db: Session,
        user: User,
        query_embedding: List[float],
        limit: int = 5
    ) -> List[str]:
        """
        Recupera memÃ³rias semanticamente prÃ³ximas usando cosine-like similarity.
        """
        memories = (
            db.query(MentorMemory)
            .filter(MentorMemory.user_id == user.id)
            .all()
        )

        scored = []

        for m in memories:
            try:
                payload = json.loads(m.memory_value)
                emb = payload.get("embedding")
                content = payload.get("content")

                if not emb or not content:
                    continue

                # Cosine Similarity simplificado (assumindo vetores normalizados)
                score = sum(a * b for a, b in zip(emb, query_embedding))
                scored.append((score, content))

            except Exception:
                continue

        scored.sort(key=lambda x: x[0], reverse=True)
        return [content for _, content in scored[:limit]]


# -------------------------------------------------
# SINGLETON INSTANCE
# -------------------------------------------------
mentor_engine = MentorEngine()



##### INICIO DO ARQUIVO: app/services/onboarding.py #####
from app.db.models.user import User
from fastapi.responses import RedirectResponse

def get_next_onboarding_step(user: User) -> str:
    """
    Determines the next step in the onboarding flow for a user.
    """
    if not user.github_username:
        return "/onboarding/connect-github"
    # Zero Touch: Profile completion is automatic or skipped
    return "/dashboard"

def validate_onboarding_access(user: User):
    """
    Enforces strict sequential onboarding flow.
    Returns a RedirectResponse if requirements are not met, else None.
    """
    if not user:
        return RedirectResponse("/login")

    if not user.linkedin_profile_url and not user.linkedin_id:
         return RedirectResponse("/logout")

    if not user.github_username and not user.github_id:
         return RedirectResponse("/onboarding/connect-github", status_code=303)

    # Zero Touch: We no longer block access for profile completion

    return None



##### INICIO DO ARQUIVO: app/services/payment.py #####
import stripe
import os
import logging
from datetime import datetime, timedelta
from typing import Optional
from app.core.config import settings

logger = logging.getLogger(__name__)

# Configuration
stripe.api_key = settings.STRIPE_SECRET_KEY

class PaymentService:
    @staticmethod
    def create_customer(email: str, name: str) -> Optional[str]:
        """
        Creates a Stripe customer.
        Returns the Customer ID.
        """
        # Strictly enable mock only in dev AND if keys are missing
        if not stripe.api_key:
             if settings.ENVIRONMENT == "development":
                 logger.warning("Stripe Key missing. Using MOCK CUSTOMER in Development.")
                 return "cus_mock_12345"
             else:
                 logger.error("CRITICAL: Stripe Key missing in PRODUCTION. Payment failed.")
                 return None

        try:
            customer = stripe.Customer.create(
                email=email,
                name=name
            )
            return customer.id
        except Exception as e:
            logger.error(f"Stripe Error (create_customer): {e}")
            if settings.ENVIRONMENT == "development":
                 return "cus_mock_fallback"
            return None

    @staticmethod
    def process_payment(
        amount_cents: int,
        currency: str,
        source_token: str,
        customer_id: str,
        description: str = "Payment"
    ) -> bool:
        """
        Process a one-time charge or setup for subscription.
        """
        if not stripe.api_key:
            if settings.ENVIRONMENT == "development":
                 logger.warning("Stripe Key missing. Using MOCK PAYMENT in Development.")
                 return True
            return False

        try:
            # Attach the source (card token) to the customer
            # (In modern Stripe, we use PaymentMethods, but for simplicity with tokens:)
            stripe.Customer.create_source(
                customer_id,
                source=source_token
            )

            stripe.Charge.create(
                amount=amount_cents,
                currency=currency,
                customer=customer_id,
                description=description
            )
            return True
        except Exception as e:
            logger.error(f"Stripe Error (process_payment): {e}")
            # Only mock in dev
            if settings.ENVIRONMENT == "development":
                if "Authentication failed" in str(e) or "Invalid API Key" in str(e):
                    logger.info("Mocking success due to invalid keys in sandbox.")
                    return True
            return False

    @staticmethod
    def create_subscription(customer_id: str, price_id: str) -> bool:
        """
        Creates a recurring subscription.
        """
        if not stripe.api_key:
             if settings.ENVIRONMENT == "development":
                 return True
             return False

        try:
            stripe.Subscription.create(
                customer=customer_id,
                items=[{"price": price_id}]
            )
            return True
        except Exception as e:
            logger.error(f"Stripe Error (create_subscription): {e}")
            if "Authentication failed" in str(e):
                 return True
            return False



##### INICIO DO ARQUIVO: app/services/pdf_generator.py #####
from reportlab.lib import colors
from reportlab.lib.pagesizes import letter
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, Image
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.graphics.shapes import Drawing
from reportlab.graphics.charts.piecharts import Pie
from reportlab.graphics.charts.legends import Legend
from io import BytesIO
from sqlalchemy.orm import Session
from app.db.models.user import User
from app.db.models.career import LearningPlan

def generate_career_passport(user: User, db: Session) -> BytesIO:
    buffer = BytesIO()
    doc = SimpleDocTemplate(buffer, pagesize=letter)
    styles = getSampleStyleSheet()

    # Custom Styles
    title_style = styles['Title']
    title_style.textColor = colors.HexColor("#0f172a")

    h2_style = ParagraphStyle(
        'Heading2Custom',
        parent=styles['Heading2'],
        textColor=colors.HexColor("#008080"),
        spaceAfter=12
    )

    normal_style = styles['Normal']

    story = []

    # --- HEADER ---
    story.append(Paragraph(f"{user.name or 'User'}'s Career Strategy Report", title_style))
    story.append(Spacer(1, 12))
    story.append(Paragraph("Generated by CareerDev AI", normal_style))
    story.append(Spacer(1, 24))

    # --- SECTION 1: REALITY CHECK (CHART) ---
    story.append(Paragraph("1. Reality Check: Skill Distribution", h2_style))

    profile = user.career_profile
    if profile and profile.skills_graph_data:
        data_json = profile.skills_graph_data
        # Parse Chart.js format
        # {"labels": ["A", "B"], "datasets": [{"data": [10, 20]}]}
        labels = data_json.get("labels", [])
        datasets = data_json.get("datasets", [])

        if datasets:
            data_values = datasets[0].get("data", [])

            if data_values and sum(data_values) > 0:
                # Create Drawing
                d = Drawing(400, 200)

                # Pie/Doughnut
                pc = Pie()
                pc.x = 100
                pc.y = 50
                pc.width = 100
                pc.height = 100
                pc.data = data_values
                pc.labels = labels

                # Make it a doughnut (Simulated by Pie for now as innerRadius is not standard)
                # pc.innerRadius = 30

                # Colors
                # Default Chart.js colors mapped to Hex
                chart_colors = [
                    colors.HexColor("#00f3ff"), # Cyan
                    colors.HexColor("#bd00ff"), # Purple
                    colors.HexColor("#00ff88"), # Green
                    colors.HexColor("#ffff00"), # Yellow
                    colors.HexColor("#ff0055"), # Red
                    colors.gray
                ]

                for i, val in enumerate(data_values):
                    # Cycle through colors
                    pc.slices[i].fillColor = chart_colors[i % len(chart_colors)]
                    pc.slices[i].strokeColor = colors.white
                    pc.slices[i].strokeWidth = 1

                d.add(pc)

                # Legend
                legend = Legend()
                legend.alignment = 'right'
                legend.x = 250
                legend.y = 100
                legend.colorNamePairs = [(pc.slices[i].fillColor, labels[i]) for i in range(len(labels))]
                d.add(legend)

                story.append(d)
            else:
                 story.append(Paragraph("No skill data available to generate chart.", normal_style))
        else:
            story.append(Paragraph("No dataset found.", normal_style))
    else:
        story.append(Paragraph("Profile analysis not yet complete.", normal_style))

    story.append(Spacer(1, 24))

    # --- SECTION 2: MARKET SCORE ---
    story.append(Paragraph("2. Market Alignment Score", h2_style))

    score = profile.market_relevance_score if profile else 0
    score_text = f"<b>{score}/100</b>"

    # Simple Color Logic for Text
    color = "red"
    if score > 80: color = "green"
    elif score > 50: color = "orange"

    story.append(Paragraph(f"Current Relevance Score: <font color={color}>{score_text}</font>", normal_style))
    story.append(Paragraph("This score reflects the intersection between your GitHub activity and current market demand for high-value skills.", normal_style))
    story.append(Spacer(1, 24))

    # --- SECTION 3: ACTION PLAN ---
    story.append(Paragraph("3. Action Plan", h2_style))

    plan_items = []

    # 1. Primary: Pending Micro Projects
    if profile and profile.pending_micro_projects:
        for task in profile.pending_micro_projects:
            # Assuming task is dict: {'title': '...', 'status': '...'}
             if isinstance(task, dict) and task.get('status') != 'completed':
                 plan_items.append([task.get('title', 'Unknown Task'), "Pending"])

    # 2. Secondary: Learning Plan
    if not plan_items:
        learning_plans = db.query(LearningPlan).filter(
            LearningPlan.user_id == user.id,
            LearningPlan.status != 'completed'
        ).all()
        for lp in learning_plans:
            plan_items.append([lp.title, "Learning Goal"])

    # 3. Tertiary: Default
    if not plan_items:
        plan_items.append(["System Task: Complete your Profile Setup", "Critical"])

    # Render Table
    table_data = [["Task / Goal", "Type"]] + plan_items
    t = Table(table_data, colWidths=[4*inch, 2*inch])
    t.setStyle(TableStyle([
        ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor("#0f172a")),
        ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),
        ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
        ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
        ('BACKGROUND', (0, 1), (-1, -1), colors.whitesmoke),
        ('GRID', (0, 0), (-1, -1), 1, colors.black),
    ]))

    story.append(t)

    # Build
    doc.build(story)
    buffer.seek(0)
    return buffer



##### INICIO DO ARQUIVO: app/services/resume.py #####
import asyncio
import json
import logging
import openai
from typing import List, Dict, Optional
from sqlalchemy.orm import Session
from app.core.config import settings
from app.db.models.career import CareerProfile, LearningPlan

logger = logging.getLogger(__name__)

def _mock_analyze(text: str, target_role: str):
    """
    Fallback/Mock logic when AI is unavailable.
    """
    text_lower = text.lower()
    keywords = {
        "python": 10, "rust": 15, "docker": 10, "kubernetes": 15,
        "aws": 10, "react": 5, "sql": 10, "fastapi": 10
    }

    found = []
    missing = []
    score = 20

    for kw, points in keywords.items():
        if kw in text_lower:
            found.append(kw.title())
            score += points
        else:
            missing.append(kw.title())

    score = min(score, 100)

    feedback = "Seu currÃ­culo Ã© sÃ³lido, mas pode melhorar (Modo Offline/Mock)."
    if score > 80:
        feedback = "Excelente perfil! VocÃª estÃ¡ muito bem posicionado."
    elif score < 40:
        feedback = "Precisa de mais palavras-chave tÃ©cnicas relevantes."

    return {
        "score": score,
        "found_skills": found,
        "missing_skills": missing[:3],
        "feedback": feedback
    }

def analyze_resume_text(text: str, target_role: str):
    """
    Uses OpenAI to analyze the resume against the target role.
    Falls back to mock logic on error or missing key.
    """
    if not settings.OPENAI_API_KEY:
        return _mock_analyze(text, target_role)

    try:
        openai.api_key = settings.OPENAI_API_KEY

        system_prompt = """
        You are an expert Senior Technical Recruiter and Career Coach.
        Analyze the provided resume text for the specific Target Role.

        You must return a valid JSON object with the following structure:
        {
            "score": <integer 0-100>,
            "found_skills": [<list of strings (skills found)>],
            "missing_skills": [<list of strings (critical skills missing for the role)>],
            "feedback": "<string (brief, constructive feedback in Portuguese)>"
        }

        Be strict but encouraging. Prioritize hard skills for the score.
        """

        response = openai.chat.completions.create(
            model=settings.OPENAI_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Target Role: {target_role}\n\nResume Text:\n{text}"}
            ],
            response_format={"type": "json_object"},
            temperature=0.7
        )

        content = response.choices[0].message.content
        data = json.loads(content)

        # Ensure fallback for partial JSON
        return {
            "score": data.get("score", 50),
            "found_skills": data.get("found_skills", []),
            "missing_skills": data.get("missing_skills", []),
            "feedback": data.get("feedback", "AnÃ¡lise concluÃ­da.")
        }

    except Exception as e:
        logger.error(f"Error in AI Resume Analysis: {e}")
        return _mock_analyze(text, target_role)

def process_resume_upload(db: Session, user_id: int, resume_text: str):
    user_profile = db.query(CareerProfile).filter(CareerProfile.user_id == user_id).first()
    target = user_profile.target_role if user_profile else "Developer"

    analysis = analyze_resume_text(resume_text, target)

    # Auto-link: Add missing skills to Learning Plan
    added_plans = []
    # Ensure missing_skills is a list
    missing = analysis.get("missing_skills", [])
    if not isinstance(missing, list):
        missing = []

    # N+1 fix: Fetch existing plans in one query
    existing_plans = db.query(LearningPlan.technology).filter(
        LearningPlan.user_id == user_id,
        LearningPlan.technology.in_(missing)
    ).all()
    existing_skills = {plan.technology for plan in existing_plans}

    for skill in missing:
        if skill not in existing_skills:
            new_plan = LearningPlan(
                user_id=user_id,
                title=f"Dominar {skill}",
                description=f"Identificado pela IA como gap para {target}.",
                technology=skill,
                status="pending"
            )
            db.add(new_plan)
            added_plans.append(skill)

    db.commit()

    analysis["added_plans"] = added_plans
    return analysis

async def process_resume_upload_async(db: Session, user_id: int, resume_text: str, github_evidence: Optional[Dict[str, List[str]]] = None):
    user_profile = await asyncio.to_thread(db.query(CareerProfile).filter(CareerProfile.user_id == user_id).first)
    target = user_profile.target_role if user_profile else "Developer"

    analysis = await asyncio.to_thread(analyze_resume_text, resume_text, target)

    # --- Cross-Validation Logic ---
    verification_results = []
    github_evidence = github_evidence or {}

    # Create a case-insensitive lookup for GitHub evidence
    # Key: Lowercase Skill, Value: (Original Skill Name, List of Repos)
    gh_lookup = {k.lower(): (k, v) for k, v in github_evidence.items()}

    resume_skills = analysis.get("found_skills", [])
    # Track which GH skills were matched to avoid duplicates in Inferred
    matched_gh_skills = set()

    for skill in resume_skills:
        skill_lower = skill.lower()
        if skill_lower in gh_lookup:
            # VERIFIED
            original_gh_name, repos = gh_lookup[skill_lower]
            repo_str = ", ".join(repos[:3]) # Limit to 3
            if len(repos) > 3: repo_str += f" +{len(repos)-3} others"

            verification_results.append({
                "skill": skill,
                "status": "VERIFIED",
                "evidence": f"Found in: {repo_str}"
            })
            matched_gh_skills.add(skill_lower)
        else:
            # DECLARED
            verification_results.append({
                "skill": skill,
                "status": "DECLARED",
                "evidence": "Not found in public repositories"
            })

    # INFERRED (Bonus - skills in GitHub but not in Resume)
    for skill_lower, (original_name, repos) in gh_lookup.items():
        if skill_lower not in matched_gh_skills:
             repo_str = ", ".join(repos[:3])
             verification_results.append({
                "skill": original_name,
                "status": "INFERRED",
                "evidence": f"Found in: {repo_str}"
            })

    # Sort results: Verified first, then Declared, then Inferred
    status_order = {"VERIFIED": 0, "DECLARED": 1, "INFERRED": 2}
    verification_results.sort(key=lambda x: status_order.get(x["status"], 99))

    analysis["verification_results"] = verification_results
    # ------------------------------

    # Auto-link: Add missing skills to Learning Plan
    added_plans = []
    # Ensure missing_skills is a list
    missing = analysis.get("missing_skills", [])
    if not isinstance(missing, list):
        missing = []

    # N+1 fix: Fetch existing plans in one query
    existing_plans = await asyncio.to_thread(db.query(LearningPlan.technology).filter(
        LearningPlan.user_id == user_id,
        LearningPlan.technology.in_(missing)
    ).all)
    existing_skills = {plan.technology for plan in existing_plans}

    for skill in missing:
        if skill not in existing_skills:
            new_plan = LearningPlan(
                user_id=user_id,
                title=f"Dominar {skill}",
                description=f"Identificado pela IA como gap para {target}.",
                technology=skill,
                status="pending"
            )
            db.add(new_plan)
            added_plans.append(skill)

    await asyncio.to_thread(db.commit)

    analysis["added_plans"] = added_plans
    return analysis



##### INICIO DO ARQUIVO: app/services/security_service.py #####
from sqlalchemy.orm import Session
from app.db.models.security import AuditLog, UserSession
from datetime import datetime
import json
import logging

logger = logging.getLogger(__name__)

def create_user_session(db: Session, user_id: int, ip_address: str, user_agent: str) -> str:
    """Creates a persistent session and returns the session ID."""
    try:
        session = UserSession(
            user_id=user_id,
            ip_address=ip_address,
            user_agent=user_agent,
            last_active_at=datetime.utcnow(),
            is_active=True
        )
        db.add(session)
        db.commit()
        db.refresh(session)
        logger.info(f"âœ… SESSION CREATED: ID={session.id} User={user_id} IP={ip_address}")
        return session.id
    except Exception as e:
        logger.error(f"âŒ SESSION CREATION FAILED: {e}")
        db.rollback()
        raise e

def revoke_session(db: Session, session_id: str):
    """Revokes a session by ID."""
    session = db.query(UserSession).filter(UserSession.id == session_id).first()
    if session:
        session.is_active = False
        db.commit()

def log_audit(
    db: Session,
    user_id: int | None,
    action: str,
    ip_address: str,
    details: str | dict = None,
    device_type: str = None,
    browser: str = None,
    os: str = None,
    user_agent_raw: str = None
):
    """Logs a critical action."""
    try:
        if isinstance(details, dict):
            details = json.dumps(details, default=str)

        audit = AuditLog(
            user_id=user_id,
            action=action,
            ip_address=ip_address,
            details=details,
            device_type=device_type,
            browser=browser,
            os=os,
            user_agent_raw=user_agent_raw
        )
        db.add(audit)
        db.commit()
    except Exception as e:
        logger.error(f"Failed to write audit log: {e}")

def get_active_sessions(db: Session, user_id: int):
    """Returns active sessions for a user."""
    return db.query(UserSession).filter(
        UserSession.user_id == user_id,
        UserSession.is_active == True
    ).order_by(UserSession.last_active_at.desc()).all()

def get_all_user_sessions(db: Session, user_id: int):
    """Returns all sessions for a user, ordered by creation date descending."""
    return db.query(UserSession).filter(
        UserSession.user_id == user_id
    ).order_by(UserSession.created_at.desc()).all()

def update_session_activity(db: Session, session_id: str):
    """Updates last_active_at for a session."""
    try:
        session = db.query(UserSession).filter(UserSession.id == session_id).first()
        if session:
            session.last_active_at = datetime.utcnow()
            db.commit()
    except Exception:
        pass # Don't crash on session update failure



##### INICIO DO ARQUIVO: app/services/social_harvester.py #####
import httpx
import logging
import asyncio
import random
from typing import Dict, List, Optional, Any
from sqlalchemy.orm import Session
from datetime import datetime, timedelta
from app.db.models.user import User
from app.db.models.career import CareerProfile
from app.db.session import SessionLocal

logger = logging.getLogger(__name__)

class SocialHarvester:
    """
    Service responsible for fetching raw data from Social APIs (GitHub, LinkedIn)
    and transforming it into actionable Dashboard metrics.
    """

    def __init__(self):
        # System's "High Demand" List
        self.market_high_demand_skills = ["Rust", "Go", "Python", "AI/ML", "React", "System Design", "Cloud Architecture", "TypeScript", "Kubernetes"]

    # --- Sync Helpers for DB Operations (to be run in thread) ---

    def _get_user_sync(self, user_id: int) -> bool:
        """Check if user exists."""
        with SessionLocal() as db:
            user = db.query(User).filter(User.id == user_id).first()
            return bool(user)

    def _save_linkedin_data_sync(self, user_id: int, alignment_data: dict, score_bump: int):
        """Update User with LinkedIn Data."""
        with SessionLocal() as db:
            try:
                user = db.query(User).filter(User.id == user_id).first()
                if not user:
                    logger.error(f"âŒ User {user_id} not found during LinkedIn save.")
                    return

                if not user.career_profile:
                    user.career_profile = CareerProfile(user_id=user.id)

                user.career_profile.linkedin_alignment_data = alignment_data

                # Bump score
                current_score = user.career_profile.market_relevance_score or 0
                user.career_profile.market_relevance_score = min(current_score + score_bump, 100)

                db.commit()
                logger.info(f"âœ… [SocialHarvester] LinkedIn data saved for {user.full_name}")
            except Exception as e:
                logger.error(f"ðŸ”¥ Error saving LinkedIn data: {e}")
                db.rollback()

    def _ensure_profile_exists_sync(self, user_id: int) -> tuple[Optional[str], Optional[str]]:
        """Ensures profile exists and returns (target_role, None) for calculation context."""
        with SessionLocal() as db:
            user = db.query(User).get(user_id)
            if not user:
                return None, None

            if not user.career_profile:
                # Create profile if missing
                profile = CareerProfile(user_id=user_id)
                db.add(profile)
                db.commit()
                db.refresh(profile)

            return user.career_profile.target_role, None # Can return more if needed

    def _save_github_data_sync(self, user_id: int, skills_graph_data: dict, market_score: int, commit_metrics: dict, linkedin_alignment_data: dict, ai_summary: str):
        """Updates User Profile with calculated GitHub Stats."""
        with SessionLocal() as db:
            try:
                user = db.query(User).get(user_id)
                if not user or not user.career_profile:
                    return

                profile = user.career_profile
                profile.skills_graph_data = skills_graph_data
                profile.market_relevance_score = market_score
                profile.github_activity_metrics = commit_metrics
                profile.linkedin_alignment_data = linkedin_alignment_data
                profile.ai_insights_summary = ai_summary

                db.commit()
                logger.info(f"âœ… Data Fusion Complete for User {user_id}. Score: {market_score}")
            except Exception as e:
                logger.error(f"ðŸ”¥ Error saving GitHub data: {e}")
                db.rollback()

    # --- Async Main Methods ---

    async def harvest_linkedin_data(self, user_id: int, token: str):
        """
        Background Task: Fetches LinkedIn data using non-blocking DB operations.
        """
        try:
            logger.info(f"âš¡ [SocialHarvester] Starting LinkedIn sync for user_id {user_id}...")

            # 1. Check User Existence (Sync -> Thread)
            exists = await asyncio.to_thread(self._get_user_sync, user_id)
            if not exists:
                logger.error(f"âŒ User {user_id} not found during background harvest.")
                return

            # 2. Fetch Profile Data (Async I/O)
            async with httpx.AsyncClient() as client:
                response = await client.get(
                    "https://api.linkedin.com/v2/userinfo",
                    headers={"Authorization": f"Bearer {token}"}
                )

            if response.status_code != 200:
                logger.error(f"âŒ LinkedIn API Error: {response.text}")
                return

            data = response.json()

            # 3. Process Data (CPU)
            first_name = data.get("given_name", "")
            last_name = data.get("family_name", "")
            picture = data.get("picture", "")

            alignment_data = {
                "source": "linkedin_oauth",
                "connected": True,
                "first_name": first_name,
                "last_name": last_name,
                "picture": picture,
                "status": "Active",
                "detected_role": "Lite Profile (Update via Dashboard)",
                "industry": "Tech"
            }

            # 4. Save to DB (Sync -> Thread)
            await asyncio.to_thread(self._save_linkedin_data_sync, user_id, alignment_data, 10)

        except Exception as e:
            logger.exception(f"ðŸ”¥ Critical Harvester Crash (LinkedIn): {e}")

    async def harvest_github_data(self, user_id: int, token: str):
        """Wrapper for sync_profile to match Route calls"""
        try:
            # Optimized: Calls sync_profile which handles DB in threads
            await self.sync_profile(user_id, token)
        except Exception as e:
            logger.exception(f"ðŸ”¥ Critical Harvester Crash (GitHub): {e}")

    async def scan_user_dependencies(self, user_id: int, token: str) -> Dict[str, List[str]]:
        """
        Scans user's repositories for dependency files (package.json, requirements.txt, etc.)
        and maps frameworks/languages to the repositories where they were found.

        Returns:
            Dict[str, List[str]]: A map of Skill -> List[Repo Names]
            e.g., {"react": ["portfolio", "admin-panel"], "python": ["backend"]}
        """
        headers = {
            "Authorization": f"Bearer {token}",
            "Accept": "application/vnd.github.v3+json",
            "User-Agent": "CareerDev-AI-Harvester"
        }

        skill_evidence_map: Dict[str, List[str]] = {}

        async with httpx.AsyncClient() as client:
            # 1. Fetch Repos (Top 30 recently updated to cover more ground)
            repos_url = f"https://api.github.com/user/repos?sort=updated&per_page=30&type=owner"
            repos_resp = await client.get(repos_url, headers=headers)

            if repos_resp.status_code != 200:
                logger.error(f"GitHub API Error during dependency scan: {repos_resp.status_code}")
                return {}

            repos = repos_resp.json()

            # 2. Parallel Scan
            sem = asyncio.Semaphore(10) # Higher concurrency for file checks

            async def check_repo(repo):
                async with sem:
                    repo_name = repo.get("name")
                    files_to_check = {
                        "requirements.txt": ["fastapi", "django", "flask", "numpy", "pandas", "torch", "scikit-learn", "requests"],
                        "package.json": ["react", "next", "vue", "express", "nestjs", "typescript", "angular", "tailwindcss"],
                        "go.mod": ["gin", "gorm", "fiber", "echo"],
                        "Cargo.toml": ["tokio", "serde", "actix", "axum", "rocket", "diesel"],
                        "pom.xml": ["spring", "hibernate", "jakarta", "junit"],
                        "build.gradle": ["spring", "hibernate", "kotlin"],
                        "Gemfile": ["rails", "sinatra", "rspec", "jekyll"]
                    }

                    # Scan contents (List root files first to avoid 404 spam)
                    # Note: contents_url usually ends with /{+path}
                    contents_url = repo.get("contents_url", "").split("{")[0]
                    c_resp = await client.get(contents_url, headers=headers)

                    if c_resp.status_code == 200:
                        root_files = {f["name"]: f for f in c_resp.json()}

                        for filename, keywords in files_to_check.items():
                            if filename in root_files:
                                # Fetch Content
                                file_url = root_files[filename].get("download_url")
                                if file_url:
                                    f_resp = await client.get(file_url)
                                    if f_resp.status_code == 200:
                                        content = f_resp.text.lower()
                                        for kw in keywords:
                                            if kw in content:
                                                # Use title case for consistency
                                                skill_name = kw.title()
                                                if skill_name == "Next": skill_name = "Next.js"
                                                if skill_name == "Vue": skill_name = "Vue.js"

                                                # Atomic update (since we are in async gather, dict set is not thread safe if not careful,
                                                # but we are in single threaded event loop so it is fine actually)
                                                if skill_name not in skill_evidence_map:
                                                    skill_evidence_map[skill_name] = []
                                                if repo_name not in skill_evidence_map[skill_name]:
                                                    skill_evidence_map[skill_name].append(repo_name)

            tasks = [check_repo(repo) for repo in repos]
            await asyncio.gather(*tasks)

        return skill_evidence_map

    async def sync_profile(self, user_id: int, github_token: str, db: Optional[Session] = None) -> bool:
        """
        Orchestrates the data fusion:
        1. Harvest GitHub (Byte Distribution)
        2. Process for Chart.js
        3. Calculate Market Score
        4. Update DB

        Args:
            user_id: User ID
            github_token: GitHub Token
            db: Optional Session (Deprecated/Ignored for thread safety in optimized mode,
                but kept in signature if strictly needed by legacy callers - though we plan to migrate them)
        """
        try:
            # 1. Ensure Profile Exists (Sync -> Thread)
            # Returns target_role to use in calculation
            target_role, _ = await asyncio.to_thread(self._ensure_profile_exists_sync, user_id)
            target_role = target_role or "Senior Developer"

            # 2. Harvest GitHub (Logic Requirement 1: Real Skill Calculator) (Async I/O)
            raw_langs, commit_metrics = await self._harvest_github_raw(github_token)

            # 3. Process Logic (CPU Bound - fast enough to run on loop, or could be threaded)

            # Chart Data
            total_bytes = sum(raw_langs.values())
            sorted_langs = sorted(raw_langs.items(), key=lambda x: x[1], reverse=True)[:6]
            chart_labels = [l[0] for l in sorted_langs]
            chart_values = []
            for l in sorted_langs:
                percentage = int((l[1] / total_bytes) * 100) if total_bytes > 0 else 0
                chart_values.append(percentage)

            skills_graph_data = {
                "labels": chart_labels,
                "datasets": [{
                    "data": chart_values,
                    "backgroundColor": ["#00f3ff", "#bd00ff", "#00ff88", "#ffff00", "#ff0055", "#ffffff"]
                }]
            }

            # Market Score
            market_score = self.calculate_market_overlap(raw_langs, self.market_high_demand_skills)

            # Metrics
            commit_metrics["raw_languages"] = raw_langs

            # Simulated LinkedIn Data
            mock_claimed = list(raw_langs.keys())[:2]
            if len(mock_claimed) > 1 and random.random() > 0.5:
                mock_claimed.pop()
            mock_claimed.append("AWS" if "AWS" not in raw_langs else "Kubernetes")

            linkedin_alignment_data = {
                "role": target_role or "Software Engineer",
                "industry": "Tech",
                "missing_keywords": [s for s in self.market_high_demand_skills if s not in chart_labels][:3],
                "claimed_skills": mock_claimed
            }

            # AI Summary
            ai_summary = self.generate_gap_analysis(raw_langs, mock_claimed, target_role)

            # 4. Save to DB (Sync -> Thread)
            await asyncio.to_thread(
                self._save_github_data_sync,
                user_id,
                skills_graph_data,
                market_score,
                commit_metrics,
                linkedin_alignment_data,
                ai_summary
            )

            return True

        except Exception as e:
            logger.error(f"Sync Profile Error: {e}", exc_info=True)
            return False

    async def _harvest_github_raw(self, token: str) -> tuple[Dict[str, int], Dict[str, Any]]:
        """
        Internal helper to fetch raw byte counts, commit metrics, AND Deep Scan Frameworks.
        Returns: (language_bytes_map, commit_metrics_json)
        """
        headers = {
            "Authorization": f"Bearer {token}",
            "Accept": "application/vnd.github.v3+json",
            "User-Agent": "CareerDev-AI-Harvester"
        }

        language_bytes = {}
        detected_frameworks = set()
        commit_metrics = {
            "commits_last_30_days": 0,
            "top_repo": "N/A",
            "velocity_score": "Low",
            "detected_frameworks": [] # Added to metrics
        }

        async with httpx.AsyncClient() as client:
            # Fetch User Info
            user_resp = await client.get("https://api.github.com/user", headers=headers)
            if user_resp.status_code != 200:
                logger.error(f"GitHub API Error: {user_resp.status_code}")
                return {}, commit_metrics

            gh_user = user_resp.json()
            username = gh_user.get("login")

            # Fetch Repos (Top 20 recently updated)
            repos_url = f"https://api.github.com/user/repos?sort=updated&per_page=20&type=owner"
            repos_resp = await client.get(repos_url, headers=headers)
            repos = repos_resp.json() if repos_resp.status_code == 200 else []

            # 1. Byte Calculation & Deep Scan
            sem = asyncio.Semaphore(5)

            async def scan_repo(repo):
                async with sem:
                    repo_name = repo.get("name")

                    # A. Language Stats
                    lang_url = repo.get("languages_url")
                    lang_data = {}
                    if lang_url:
                        r = await client.get(lang_url, headers=headers)
                        if r.status_code == 200:
                            lang_data = r.json()

                    # B. Deep File Scan (Dependency Check)
                    # We check root files for specific patterns
                    found_frameworks = []
                    files_to_check = {
                        "requirements.txt": ["fastapi", "django", "flask", "numpy", "pandas"],
                        "package.json": ["react", "next", "vue", "express", "nestjs", "typescript"],
                        "Cargo.toml": ["tokio", "serde", "actix", "axum", "rocket"]
                    }

                    # Scan contents (List root files first to avoid 404 spam)
                    contents_url = repo.get("contents_url", "").replace("{+path}", "")
                    c_resp = await client.get(contents_url, headers=headers)
                    if c_resp.status_code == 200:
                        files = {f["name"]: f for f in c_resp.json()}

                        for filename, keywords in files_to_check.items():
                            if filename in files:
                                # Fetch Content
                                file_url = files[filename].get("download_url")
                                if file_url:
                                    f_resp = await client.get(file_url)
                                    if f_resp.status_code == 200:
                                        content = f_resp.text.lower()
                                        for kw in keywords:
                                            if kw in content:
                                                found_frameworks.append(kw)

                    return lang_data, repo_name, found_frameworks

            tasks = [scan_repo(repo) for repo in repos]
            results = await asyncio.gather(*tasks)

            max_repo_bytes = 0
            top_repo_name = "N/A"

            for lang_map, repo_name, frameworks in results:
                # Aggregate Frameworks
                for f in frameworks:
                    detected_frameworks.add(f)

                repo_total = 0
                for lang, bytes_count in lang_map.items():
                    language_bytes[lang] = language_bytes.get(lang, 0) + bytes_count
                    repo_total += bytes_count

                if repo_total > max_repo_bytes:
                    max_repo_bytes = repo_total
                    top_repo_name = repo_name

            # 2. Commit Velocity (Events)
            events_url = f"https://api.github.com/users/{username}/events?per_page=100"
            events_resp = await client.get(events_url, headers=headers)
            commit_count = 0
            if events_resp.status_code == 200:
                events = events_resp.json()
                cutoff = datetime.utcnow() - timedelta(days=30)
                for e in events:
                    if e.get("type") == "PushEvent":
                        created_at = datetime.strptime(e["created_at"], "%Y-%m-%dT%H:%M:%SZ")
                        if created_at > cutoff:
                            payload = e.get("payload", {})
                            commit_count += payload.get("size", 1)

            velocity = "Low"
            if commit_count > 50: velocity = "High"
            elif commit_count > 20: velocity = "Medium"

            commit_metrics = {
                "commits_last_30_days": commit_count,
                "top_repo": top_repo_name,
                "velocity_score": velocity,
                "detected_frameworks": list(detected_frameworks)
            }

        return language_bytes, commit_metrics

    def calculate_market_overlap(self, user_langs: Dict[str, int], market_trends: List[str]) -> int:
        """
        Compare User's Top 3 Languages vs. System's "High Demand" List.
        Algorithm:
        1. Identify User's Top 3 Languages by bytes.
        2. Count how many are in Market Trends.
        3. Score = (Matches / 3) * 100.
        """
        if not user_langs:
            return 0

        # Sort user langs by bytes
        sorted_user_langs = sorted(user_langs.items(), key=lambda x: x[1], reverse=True)
        top_3 = [l[0].lower() for l in sorted_user_langs[:3]]

        market_lower = {m.lower() for m in market_trends}

        matches = 0
        for lang in top_3:
            if lang in market_lower:
                matches += 1

        # Calculate Score based on Top 3 intersection
        # If user has 0 matches in top 3, score is low.
        # But maybe we should also look at overall?
        # Prompt says: "Compare User's Top 3 Languages vs. System's 'High Demand' List"

        if len(top_3) == 0: return 0

        # Simple ratio
        score = int((matches / 3) * 100)

        # Bonus: If they have Rust or Go (hardcoded high value), give a boost?
        # Keeping it simple as per prompt instructions.

        return score

    def generate_gap_analysis(self, github_reality: Dict[str, int], linkedin_perception: List[str], target_role: str) -> str:
        """
        Phase 1 Logic: The 'Intelligence Engine'
        Compares Reality (GitHub Bytes) vs Perception (LinkedIn Claims).
        Returns a string summary for Zone C.
        """
        insights = []

        # Normalize keys
        gh_skills = {k.lower(): v for k, v in github_reality.items()}
        li_skills = {k.lower() for k in linkedin_perception}

        # Logic 1: Imposter Syndrome (Claimed but not Coded)
        imposter_risks = []
        for skill in li_skills:
            # If claimed but < 1000 bytes (arbitrary low threshold)
            if skill not in gh_skills or gh_skills[skill] < 1000:
                imposter_risks.append(skill)

        if imposter_risks:
            sk = imposter_risks[0].title()
            insights.append(f"âš ï¸ <strong>Imposter Alert:</strong> You list '{sk}' on LinkedIn but have little code to back it up. Build a project.")

        # Logic 2: Invisible Gold (Coded but not Claimed)
        hidden_gems = []
        for skill, bytes_count in gh_skills.items():
            if bytes_count > 5000 and skill not in li_skills:
                hidden_gems.append(skill)

        if hidden_gems:
            gem = hidden_gems[0].title()
            insights.append(f"ðŸ’Ž <strong>Hidden Asset:</strong> You have significant {gem} code. Add it to your profile immediately.")

        # Logic 3: Consistency Check
        if not insights:
            insights.append("âœ… <strong>Profile Synced:</strong> Your code and claims are perfectly aligned. Great consistency.")

        # Add Market Context
        if "rust" in gh_skills or "go" in gh_skills:
             insights.append("ðŸš€ <strong>Market Ready:</strong> High-value systems languages detected.")

        return "<br>".join(insights)

    def _scan_github_sync(self, user_id: int):
        """Sync helper for legacy scan simulation."""
        with SessionLocal() as db:
            user = db.query(User).get(user_id)
            if not user or not user.career_profile:
                return

            profile = user.career_profile
            # Boost score slightly to show 'activity' effect
            current = profile.market_relevance_score or 0
            profile.market_relevance_score = min(current + 2, 100)

            # Update velocity
            metrics = profile.github_activity_metrics or {}
            metrics["velocity_score"] = "High (Verified)"
            metrics["commits_last_30_days"] = (metrics.get("commits_last_30_days", 0) + 1)
            profile.github_activity_metrics = metrics

            db.commit()

    # Legacy / Simulation Support (Optional - kept if needed for fallback)
    async def scan_github(self, db: Session, user: User):
        """
        Public Trigger for 'Done' button.
        Since we might not have a token in session here, we simulate or use stored token if available.
        For now, this will just trigger a re-score simulation if real token is missing.
        """
        # In a real app, we'd retrieve the encrypted token from DB or Vault.
        # Assuming we don't have it, we'll simulate the update for the 'Done' interaction.
        logger.info(f"Scanning GitHub for user {user.id}...")

        # Simulate delay and update
        await asyncio.sleep(1)

        # Offload DB update to thread to avoid blocking main loop
        await asyncio.to_thread(self._scan_github_sync, user.id)

social_harvester = SocialHarvester()



##### INICIO DO ARQUIVO: app/static/__init__.py #####



##### INICIO DO ARQUIVO: app/static/css/__init__.py #####



##### INICIO DO ARQUIVO: app/static/js/__init__.py #####



##### INICIO DO ARQUIVO: Dockerfile #####
FROM python:3.12-slim

# Install system dependencies
# netcat-openbsd for the wait-for-db check
# gcc and libpq-dev for potential build requirements (e.g. psycopg2)
RUN apt-get update && apt-get install -y \
    netcat-openbsd \
    gcc \
    libpq-dev \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Install python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Expose port
EXPOSE 8000

CMD alembic upgrade head && uvicorn app.main:app --host 0.0.0.0 --port ${PORT:-8000} --workers 1



##### INICIO DO ARQUIVO: docker-compose.yml #####
version: '3.8'

services:
  app:
    build: .
    ports:
      - "8000:8000"
    depends_on:
      - db
    env_file:
      - .env
    volumes:
      - .:/app
    command: sh -c "while ! nc -z db 5432; do sleep 1; echo 'Waiting for DB'; done; uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload"

  db:
    image: postgres:16-alpine
    restart: always
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: changeme
      POSTGRES_DB: app_db
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data:



##### INICIO DO ARQUIVO: requirements.txt #####
# Web Framework & Server
fastapi==0.111.0
uvicorn==0.30.1
slowapi==0.1.9
starlette==0.37.2
python-multipart==0.0.9

# Database
sqlalchemy==2.0.30
psycopg2-binary==2.9.9
alembic

# Security & Auth
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
bcrypt==4.0.1
authlib==1.3.1
itsdangerous==2.2.0
oauthlib==3.2.2
python-dotenv==1.0.1

# APIs & External Services
openai==1.30.1
stripe==9.0.0
sentry-sdk[fastapi]==2.50.0
google-auth-oauthlib==1.2.0
requests==2.32.3
httpx==0.27.0
user-agents==2.2.0

# Utilities
pydantic==2.7.1
pydantic-settings==2.2.1
jinja2==3.1.4
asyncpg==0.29.0
reportlab==4.4.9

# Machine Learning
pandas>=2.2.0
scikit-learn>=1.4.0
joblib>=1.3.2
tensorflow>=2.16.1
mlflow
